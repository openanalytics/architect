
\chapter{The \texttt{stats} package}
\HeaderA{stats-package}{The R Stats Package}{stats.Rdash.package}
\aliasA{stats}{stats-package}{stats}
\keyword{package}{stats-package}
%
\begin{Description}\relax
R statistical functions
\end{Description}
%
\begin{Details}\relax
This package contains functions for statistical calculations
and random number generation.

For a complete
list of functions, use \code{library(help="stats")}.

\end{Details}
%
\begin{Author}\relax
R Core Team and contributors worldwide

Maintainer: R Core Team \email{R-core@r-project.org}
\end{Author}
\HeaderA{.checkMFClasses}{Functions to Check the Type of Variables passed to Model Frames}{.checkMFClasses}
\aliasA{.getXlevels}{.checkMFClasses}{.getXlevels}
\aliasA{.MFclass}{.checkMFClasses}{.MFclass}
\keyword{utilities}{.checkMFClasses}
%
\begin{Description}\relax
\code{.checkMFClasses} checks if the variables used in a predict
method agree in type with those used for fitting.

\code{.MFclass} categorizes variables for this purpose.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
.checkMFClasses(cl, m, ordNotOK = FALSE)
.MFclass(x)
.getXlevels(Terms, m)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{cl}] a character vector of class descriptions to match.
\item[\code{m}] a model frame.
\item[\code{x}] any \R{} object.
\item[\code{ordNotOK}] logical: are ordered factors different?
\item[\code{Terms}] a \code{terms} object.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For applications involving \code{model.matrix} such as linear models
we do not need to differentiate between ordered factors and factors as
although these affect the coding, the coding used in the fit is
already recorded and imposed during prediction.  However, other
applications may treat ordered factors differently:
\code{\LinkA{rpart}{rpart}} does, for example.
\end{Details}
%
\begin{Value}
\code{.MFclass} returns a character string, one of \code{"logical"},
\code{"ordered"}, \code{"factor"}, \code{"numeric"}, \code{"nmatrix.*"}
(a numeric matrix with a number of columns appended) or \code{"other"}.

\code{.getXlevels} returns a named character vector, or \code{NULL}.
\end{Value}
\HeaderA{acf}{Auto- and Cross- Covariance and -Correlation Function Estimation}{acf}
\aliasA{ccf}{acf}{ccf}
\aliasA{pacf}{acf}{pacf}
\methaliasA{pacf.default}{acf}{pacf.default}
\aliasA{[.acf}{acf}{[.acf}
\keyword{ts}{acf}
%
\begin{Description}\relax
The function \code{acf} computes (and by default plots) estimates of
the autocovariance or autocorrelation function.  Function \code{pacf}
is the function used for the partial autocorrelations.  Function
\code{ccf} computes the cross-correlation or cross-covariance of two
univariate series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
acf(x, lag.max = NULL,
    type = c("correlation", "covariance", "partial"),
    plot = TRUE, na.action = na.fail, demean = TRUE, ...)

pacf(x, lag.max, plot, na.action, ...)

## Default S3 method:
pacf(x, lag.max = NULL, plot = TRUE, na.action = na.fail,
    ...)

ccf(x, y, lag.max = NULL, type = c("correlation", "covariance"),
    plot = TRUE, na.action = na.fail, ...)

## S3 method for class 'acf'
x[i, j]
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] a univariate or multivariate (not \code{ccf}) numeric time
series object or a numeric vector or matrix, or an \code{"acf"} object.
\item[\code{lag.max}] maximum lag at which to calculate the acf.
Default is \eqn{10\log_{10}(N/m)}{} where \eqn{N}{} is the
number of observations and \eqn{m}{} the number of series.  Will
be automatically limited to one less than the number of observations
in the series.
\item[\code{type}] character string giving the type of acf to be computed.
Allowed values are
\code{"correlation"} (the default), \code{"covariance"} or
\code{"partial"}.
\item[\code{plot}] logical. If \code{TRUE} (the default) the acf is plotted.
\item[\code{na.action}] function to be called to handle missing
values. \code{na.pass} can be used.
\item[\code{demean}] logical.  Should the covariances be about the sample
means?
\item[\code{...}] further arguments to be passed to \code{plot.acf}.

\item[\code{i}] a set of lags (time differences) to retain.
\item[\code{j}] a set of series (names or numbers) to retain.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For \code{type} = \code{"correlation"} and \code{"covariance"}, the
estimates are based on the sample covariance. (The lag 0 autocorrelation
is fixed at 1 by convention.)

By default, no missing values are allowed.  If the \code{na.action}
function passes through missing values (as \code{na.pass} does), the
covariances are computed from the complete cases.  This means that the
estimate computed may well not be a valid autocorrelation sequence,
and may contain missing values.  Missing values are not allowed when
computing the PACF of a multivariate time series.

The partial correlation coefficient is estimated by fitting
autoregressive models of successively higher orders up to
\code{lag.max}.

The generic function \code{plot} has a method for objects of class
\code{"acf"}.

The lag is returned and plotted in units of time, and not numbers of
observations.

There are \code{print} and subsetting methods for objects of class
\code{"acf"}.
\end{Details}
%
\begin{Value}
An object of class \code{"acf"}, which is a list with the following
elements:

\begin{ldescription}
\item[\code{lag}] A three dimensional array containing the lags at which
the acf is estimated.
\item[\code{acf}] An array with the same dimensions as \code{lag} containing
the estimated acf.
\item[\code{type}] The type of correlation (same as the \code{type}
argument).
\item[\code{n.used}] The number of observations in the time series.
\item[\code{series}] The name of the series \code{x}.
\item[\code{snames}] The series names for a multivariate time series.

\end{ldescription}
The lag \code{k} value returned by \code{ccf(x,y)} estimates the
correlation between \code{x[t+k]} and \code{y[t]}.

The result is returned invisibly if \code{plot} is \code{TRUE}.
\end{Value}
%
\begin{Author}\relax
Original: Paul Gilbert, Martyn Plummer.
Extensive modifications and univariate case of \code{pacf} by
B. D. Ripley.
\end{Author}
%
\begin{References}\relax
Venables, W. N. and Ripley, B. D. (2002)
\emph{Modern Applied Statistics with S}.  Fourth Edition.
Springer-Verlag.

(This contains the exact definitions used.)
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{plot.acf}{plot.acf}}, \code{\LinkA{ARMAacf}{ARMAacf}} for the exact
autocorrelations of a given ARMA process.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Examples from Venables & Ripley
acf(lh)
acf(lh, type = "covariance")
pacf(lh)

acf(ldeaths)
acf(ldeaths, ci.type = "ma")
acf(ts.union(mdeaths, fdeaths))
ccf(mdeaths, fdeaths, ylab = "cross-correlation")
# (just the cross-correlations)

presidents # contains missing values
acf(presidents, na.action = na.pass)
pacf(presidents, na.action = na.pass)
\end{ExampleCode}
\end{Examples}
\HeaderA{acf2AR}{Compute an AR Process Exactly Fitting an ACF}{acf2AR}
\keyword{ts}{acf2AR}
%
\begin{Description}\relax
Compute an AR process exactly fitting an autocorrelation function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
acf2AR(acf)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{acf}] An autocorrelation or autocovariance sequence.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A matrix, with one row for the computed AR(p) coefficients for
\code{1 <= p <= length(acf)}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{ARMAacf}{ARMAacf}}, \code{\LinkA{ar.yw}{ar.yw}} which does this from an
empirical ACF.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
(Acf <- ARMAacf(c(0.6, 0.3, -0.2)))
acf2AR(Acf)
\end{ExampleCode}
\end{Examples}
\HeaderA{add1}{Add or Drop All Possible Single Terms to a Model}{add1}
\methaliasA{add1.default}{add1}{add1.default}
\methaliasA{add1.glm}{add1}{add1.glm}
\methaliasA{add1.lm}{add1}{add1.lm}
\aliasA{drop1}{add1}{drop1}
\methaliasA{drop1.default}{add1}{drop1.default}
\methaliasA{drop1.glm}{add1}{drop1.glm}
\methaliasA{drop1.lm}{add1}{drop1.lm}
\keyword{models}{add1}
%
\begin{Description}\relax
Compute all the single terms in the \code{scope} argument that can be
added to or dropped from the model, fit those models and compute a
table of the changes in fit.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
add1(object, scope, ...)

## Default S3 method:
add1(object, scope, scale = 0, test = c("none", "Chisq"),
     k = 2, trace = FALSE, ...)

## S3 method for class 'lm'
add1(object, scope, scale = 0, test = c("none", "Chisq", "F"),
     x = NULL, k = 2, ...)

## S3 method for class 'glm'
add1(object, scope, scale = 0, test = c("none", "Rao", "LRT", "Chisq", "F"),
     x = NULL, k = 2, ...)

drop1(object, scope, ...)

## Default S3 method:
drop1(object, scope, scale = 0, test = c("none", "Chisq"),
      k = 2, trace = FALSE, ...)

## S3 method for class 'lm'
drop1(object, scope, scale = 0, all.cols = TRUE,
      test = c("none", "Chisq", "F"), k = 2, ...)

## S3 method for class 'glm'
drop1(object, scope, scale = 0, test = c("none", "Rao", "LRT", "Chisq", "F"),
      k = 2, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a fitted model object.
\item[\code{scope}] a formula giving the terms to be considered for adding or
dropping.
\item[\code{scale}] an estimate of the residual mean square to be
used in computing \eqn{C_p}{}. Ignored if \code{0} or \code{NULL}.
\item[\code{test}] should the results include a test statistic relative to the
original model?  The F test is only appropriate for \code{\LinkA{lm}{lm}} and
\code{\LinkA{aov}{aov}} models or perhaps for \code{\LinkA{glm}{glm}} fits with
estimated dispersion.
The \eqn{\chi^2}{} test can be an exact test
(\code{lm} models with known scale) or a likelihood-ratio test or a
test of the reduction in scaled deviance depending on the method.
For \code{\LinkA{glm}{glm}} fits, you can also choose \code{"LRT"} and 
\code{"Rao"} for likelihood ratio tests and Rao's efficient score test. 
The former is synonymous with \code{"Chisq"} (although both have 
an asymptotic chi-square distribution).

\item[\code{k}] the penalty constant in AIC / \eqn{C_p}{}.
\item[\code{trace}] if \code{TRUE}, print out progress reports.
\item[\code{x}] a model matrix containing columns for the fitted model and all
terms in the upper scope.  Useful if \code{add1} is to be called
repeatedly.  \bold{Warning:} no checks are done on its validity.
\item[\code{all.cols}] (Provided for compatibility with S.)  Logical to specify
whether all columns of the design matrix should be used.  If
\code{FALSE} then non-estimable columns are dropped, but the result
is not usually statistically meaningful.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For \code{drop1} methods, a missing \code{scope} is taken to be all
terms in the model. The hierarchy is respected when considering terms
to be added or dropped: all main effects contained in a second-order
interaction must remain, and so on.

In a \code{scope} formula \code{.} means `what is already there'.

The methods for \code{\LinkA{lm}{lm}} and \code{\LinkA{glm}{glm}} are more
efficient in that they do not recompute the model matrix and call the
\code{fit} methods directly.

The default output table gives AIC, defined as minus twice log
likelihood plus \eqn{2p}{} where \eqn{p}{} is the rank of the model (the
number of effective parameters).  This is only defined up to an
additive constant (like log-likelihoods).  For linear Gaussian models
with fixed scale, the constant is chosen to give Mallows' \eqn{C_p}{},
\eqn{RSS/scale + 2p - n}{}.  Where \eqn{C_p}{} is used,
the column is labelled as \code{Cp} rather than \code{AIC}.

The F tests for the \code{"glm"} methods are based on analysis of
deviance tests, so if the dispersion is estimated it is based on the
residual deviance, unlike the F tests of \code{\LinkA{anova.glm}{anova.glm}}.
\end{Details}
%
\begin{Value}
An object of class \code{"anova"} summarizing the differences in fit
between the models.
\end{Value}
%
\begin{Section}{Warning}
The model fitting must apply the models to the same dataset. Most
methods will attempt to use a subset of the data with no missing
values for any of the variables if \code{na.action=na.omit}, but
this may give biased results.  Only use these functions with data
containing missing values with great care.

The default methods make calls to the function \code{\LinkA{nobs}{nobs}} to
check that the number of observations involved in the fitting process
remained unchanged. 
\end{Section}
%
\begin{Note}\relax
These are not fully equivalent to the functions in S.  There is no
\code{keep} argument, and the methods used are not quite so
computationally efficient.

Their authors' definitions of Mallows' \eqn{C_p}{} and Akaike's AIC
are used, not those of the authors of the models chapter of S.
\end{Note}
%
\begin{Author}\relax
The design was inspired by the S functions of the same names described
in Chambers (1992).
\end{Author}
%
\begin{References}\relax
Chambers, J. M. (1992)
\emph{Linear models.}
Chapter 4 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{step}{step}}, \code{\LinkA{aov}{aov}}, \code{\LinkA{lm}{lm}},
\code{\LinkA{extractAIC}{extractAIC}}, \code{\LinkA{anova}{anova}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(graphics); require(utils)
## following example(swiss)
lm1 <- lm(Fertility ~ ., data = swiss)
add1(lm1, ~ I(Education^2) + .^2)
drop1(lm1, test="F")  # So called 'type II' anova

## following example(glm)

drop1(glm.D93, test="Chisq")
drop1(glm.D93, test="F")
add1(glm.D93, scope=~outcome*treatment, test="Rao") ## Pearson Chi-square 

\end{ExampleCode}
\end{Examples}
\HeaderA{addmargins}{Puts Arbitrary Margins on Multidimensional Tables or Arrays}{addmargins}
\keyword{manip}{addmargins}
\keyword{array}{addmargins}
%
\begin{Description}\relax
For a given table one can specify which of the classifying factors to
expand by one or more levels to hold margins to be calculated.  One may for
example form sums and means over the first dimension and medians over the
second.  The resulting table will then have two extra levels for the first
dimension and one extra level for the second.  The default is to sum over
all margins in the table.  Other possibilities may give results that
depend on the order in which the margins are computed.  This is flagged
in the printed output from the function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
addmargins(A, margin = seq_along(dim(A)), FUN = sum, quiet = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{A}] table or array.  The function uses the presence of the
\code{"dim"} and \code{"dimnames"} attributes of \code{A}.
\item[\code{margin}] vector of dimensions over which to form margins.  Margins
are formed in the order in which dimensions are specified in
\code{margin}.
\item[\code{FUN}] list of the same length as \code{margin}, each element of
the list being either a function or a list of functions.  Names of
the list elements will appear as levels in dimnames of the result.
Unnamed list elements will have names constructed:  the name
of a function or a constructed name based on the position in the table.
\item[\code{quiet}] logical which suppresses the message telling the order in
which the margins were computed.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If the functions used to form margins are not commutative the result
depends on the order in which margins are computed.  Annotation
of margins is done via naming the \code{FUN} list.
\end{Details}
%
\begin{Value}
A table or array with the same number of dimensions as \code{A}, but
with extra levels of the dimensions mentioned in \code{margin}.  The
number of levels added to each dimension is the length of the entries
in \code{FUN}.  A message with the order of computation of margins is
printed.
\end{Value}
%
\begin{Author}\relax
Bendix Carstensen, Steno Diabetes Center \& Department of
Biostatistics, University of Copenhagen,
\url{http://www.biostat.ku.dk/~bxc}, autumn 2003.
Margin naming enhanced by Duncan Murdoch.
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{table}{table}}, \code{\LinkA{ftable}{ftable}}, \code{\LinkA{margin.table}{margin.table}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
Aye <- sample(c("Yes", "Si", "Oui"), 177, replace = TRUE)
Bee <- sample(c("Hum", "Buzz"), 177, replace = TRUE)
Sea <- sample(c("White", "Black", "Red", "Dead"), 177, replace = TRUE)
(A <- table(Aye, Bee, Sea))
addmargins(A)

ftable(A)
ftable(addmargins(A))

# Non-commutative functions - note differences between resulting tables:
ftable(addmargins(A, c(1,3),
       FUN = list(Sum = sum, list(Min = min, Max = max))))
ftable(addmargins(A, c(3,1),
       FUN = list(list(Min = min, Max = max), Sum = sum)))

# Weird function needed to return the N when computing percentages
sqsm <- function(x) sum(x)^2/100
B <- table(Sea, Bee)
round(sweep(addmargins(B, 1, list(list(All = sum, N = sqsm))), 2,
            apply(B, 2, sum)/100, "/"), 1)
round(sweep(addmargins(B, 2, list(list(All = sum, N = sqsm))), 1,
            apply(B, 1, sum)/100, "/"), 1)

# A total over Bee requires formation of the Bee-margin first:
mB <-  addmargins(B, 2, FUN = list(list(Total = sum)))
round(ftable(sweep(addmargins(mB, 1, list(list(All = sum, N = sqsm))), 2,
                   apply(mB,2,sum)/100, "/")), 1)

## Zero.Printing table+margins:
set.seed(1)
x <- sample( 1:7, 20, replace=TRUE)
y <- sample( 1:7, 20, replace=TRUE)
tx <- addmargins( table(x, y) )
print(tx, zero.print = ".")
\end{ExampleCode}
\end{Examples}
\HeaderA{aggregate}{Compute Summary Statistics of Data Subsets}{aggregate}
\methaliasA{aggregate.data.frame}{aggregate}{aggregate.data.frame}
\methaliasA{aggregate.default}{aggregate}{aggregate.default}
\methaliasA{aggregate.formula}{aggregate}{aggregate.formula}
\methaliasA{aggregate.ts}{aggregate}{aggregate.ts}
\keyword{category}{aggregate}
\keyword{array}{aggregate}
%
\begin{Description}\relax
Splits the data into subsets, computes summary statistics for each,
and returns the result in a convenient form.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
aggregate(x, ...)

## Default S3 method:
aggregate(x, ...)

## S3 method for class 'data.frame'
aggregate(x, by, FUN, ..., simplify = TRUE)

## S3 method for class 'formula'
aggregate(formula, data, FUN, ...,
          subset, na.action = na.omit)

## S3 method for class 'ts'
aggregate(x, nfrequency = 1, FUN = sum, ndeltat = 1,
          ts.eps = getOption("ts.eps"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an R object.
\item[\code{by}] a list of grouping elements, each as long as the variables
in \code{x}.
\item[\code{FUN}] a function to compute the summary statistics which can be
applied to all data subsets.
\item[\code{simplify}] a logical indicating whether results should be
simplified to a vector or matrix if possible.
\item[\code{formula}] a \LinkA{formula}{formula}, such as \code{y \textasciitilde{} x} or
\code{cbind(y1, y2) \textasciitilde{} x1 + x2}, where the \code{y} variables are
numeric data to be split into groups according to the grouping
\code{x} variables (usually factors).
\item[\code{data}] a data frame (or list) from which the variables in formula
should be taken.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA} values. The default is to ignore missing
values in the given variables.
\item[\code{nfrequency}] new number of observations per unit of time; must
be a divisor of the frequency of \code{x}.
\item[\code{ndeltat}] new fraction of the sampling period between
successive observations; must be a divisor of the sampling
interval of \code{x}.
\item[\code{ts.eps}] tolerance used to decide if \code{nfrequency} is a
sub-multiple of the original frequency.
\item[\code{...}] further arguments passed to or used by methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{aggregate} is a generic function with methods for data frames
and time series.

The default method \code{aggregate.default} uses the time series
method if \code{x} is a time series, and otherwise coerces \code{x}
to a data frame and calls the data frame method.

\code{aggregate.data.frame} is the data frame method.  If \code{x} is
not a data frame, it is coerced to one, which must have a non-zero
number of rows.  Then, each of the variables (columns) in \code{x} is
split into subsets of cases (rows) of identical combinations of the
components of \code{by}, and \code{FUN} is applied to each such subset
with further arguments in \code{...} passed to it.  The result is
reformatted into a data frame containing the variables in \code{by}
and \code{x}.  The ones arising from \code{by} contain the unique
combinations of grouping values used for determining the subsets, and
the ones arising from \code{x} the corresponding summaries for the
subset of the respective variables in \code{x}.  If \code{simplify} is
true, summaries are simplified to vectors or matrices if they have a
common length of one or greater than one, respectively; otherwise,
lists of summary results according to subsets are obtained.  Rows with
missing values in any of the \code{by} variables will be omitted from
the result.  (Note that versions of \R{} prior to 2.11.0 required
\code{FUN} to be a scalar function.)

\code{aggregate.formula} is a standard formula interface to
\code{aggregate.data.frame}.

\code{aggregate.ts} is the time series method, and requires \code{FUN}
to be a scalar function.  If \code{x} is not a time series, it is
coerced to one.  Then, the variables in \code{x} are split into
appropriate blocks of length \code{frequency(x) / nfrequency}, and
\code{FUN} is applied to each such block, with further (named)
arguments in \code{...} passed to it.  The result returned is a time
series with frequency \code{nfrequency} holding the aggregated values.
Note that this make most sense for a quarterly or yearly result when
the original series covers a whole number of quarters or years: in
particular aggregating a monthly series to quarters starting in
February does not give a conventional quarterly series.

\code{FUN} is passed to \code{\LinkA{match.fun}{match.fun}}, and hence it can be a
function or a symbol or character string naming a function.
\end{Details}
%
\begin{Value}
For the time series method, a time series of class \code{"ts"} or
class \code{c("mts", "ts")}.

For the data frame method, a data frame with columns
corresponding to the grouping variables in \code{by} followed by
aggregated columns from \code{x}.  If the \code{by} has names, the
non-empty times are used to label the columns in the results, with
unnamed grouping variables being named \code{Group.\var{i}} for
\code{by[[\var{i}]]}.
\end{Value}
%
\begin{Author}\relax
Kurt Hornik, with contributions by Arni Magnusson.
\end{Author}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{apply}{apply}}, \code{\LinkA{lapply}{lapply}}, \code{\LinkA{tapply}{tapply}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Compute the averages for the variables in 'state.x77', grouped
## according to the region (Northeast, South, North Central, West) that
## each state belongs to.
aggregate(state.x77, list(Region = state.region), mean)

## Compute the averages according to region and the occurrence of more
## than 130 days of frost.
aggregate(state.x77,
          list(Region = state.region,
               Cold = state.x77[,"Frost"] > 130),
          mean)
## (Note that no state in 'South' is THAT cold.)


## example with character variables and NAs
testDF <- data.frame(v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),
                     v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99) )
by1 <- c("red","blue",1,2,NA,"big",1,2,"red",1,NA,12)
by2 <- c("wet","dry",99,95,NA,"damp",95,99,"red",99,NA,NA)
aggregate(x = testDF, by = list(by1, by2), FUN = "mean")

# and if you want to treat NAs as a group
fby1 <- factor(by1, exclude = "")
fby2 <- factor(by2, exclude = "")
aggregate(x = testDF, by = list(fby1, fby2), FUN = "mean")


## Formulas, one ~ one, one ~ many, many ~ one, and many ~ many:
aggregate(weight ~ feed, data = chickwts, mean)
aggregate(breaks ~ wool + tension, data = warpbreaks, mean)
aggregate(cbind(Ozone, Temp) ~ Month, data = airquality, mean)
aggregate(cbind(ncases, ncontrols) ~ alcgp + tobgp, data = esoph, sum)

## Dot notation:
aggregate(. ~ Species, data = iris, mean)
aggregate(len ~ ., data = ToothGrowth, mean)

## Often followed by xtabs():
ag <- aggregate(len ~ ., data = ToothGrowth, mean)
xtabs(len ~ ., data = ag)


## Compute the average annual approval ratings for American presidents.
aggregate(presidents, nfrequency = 1, FUN = mean)
## Give the summer less weight.
aggregate(presidents, nfrequency = 1,
          FUN = weighted.mean, w = c(1, 1, 0.5, 1))
\end{ExampleCode}
\end{Examples}
\HeaderA{AIC}{Akaike's An Information Criterion}{AIC}
\aliasA{BIC}{AIC}{BIC}
\keyword{models}{AIC}
%
\begin{Description}\relax
Generic function calculating Akaike's `An Information Criterion' for
one or several fitted model objects for which a log-likelihood value
can be obtained, according to the formula
\eqn{-2 \mbox{log-likelihood} + k n_{par}}{},
where \eqn{n_{par}}{} represents the number of parameters in the
fitted model, and \eqn{k = 2}{} for the usual AIC, or
\eqn{k = \log(n)}{}
(\eqn{n}{} being the number of observations) for the so-called BIC or SBC
(Schwarz's Bayesian criterion).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
AIC(object, ..., k = 2)

BIC(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a fitted model object for which there exists a
\code{logLik} method to extract the corresponding log-likelihood, or
an object inheriting from class \code{logLik}.
\item[\code{...}] optionally more fitted model objects.
\item[\code{k}] numeric, the \emph{penalty} per parameter to be used; the
default \code{k = 2} is the classical AIC.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These are generic functions (with S4 generics defined in package
\pkg{stats4}): however methods should be defined for the
log-likelihood function \code{\LinkA{logLik}{logLik}} rather than these
functions: the action of their default methods is to call \code{logLik}
on all the supplied objects and assemble the results.

When comparing fitted objects, the smaller the AIC or BIC, the better
the fit.

The log-likelihood and hence the AIC/BIC is only defined up to an
additive constant.  Different constants have conventionally be used
for different purposes and so \code{\LinkA{extractAIC}{extractAIC}} and \code{AIC}
may give different values (and do for models of class \code{"lm"}: see
the help for \code{\LinkA{extractAIC}{extractAIC}}).  Particular care is needed
when comparing fits of different classes (with, for example, a
comparison of a Poisson and gamma GLM being meaningless since one has
a discrete response, the other continuous).

\code{BIC} is defined as
\code{AIC(object, ..., k = log(nobs(object)))}.
This needs the number of observations to be known: the default method
looks first for a \code{"nobs"} attribute on the return value from the
\code{\LinkA{logLik}{logLik}} method, then tries the \code{\LinkA{nobs}{nobs}}
generic, and if neither succeed returns BIC as \code{NA}.
\end{Details}
%
\begin{Value}
If just one object is provided, a numeric value with the corresponding
AIC (or BIC, or \dots, depending on \code{k}).

If multiple objects are provided, a \code{data.frame} with rows
corresponding to the objects and columns representing the number of
parameters in the model (\code{df}) and the AIC or BIC.
\end{Value}
%
\begin{Author}\relax
Originally by JosÃ© Pinheiro and Douglas Bates,
more recent revisions by R-core.
\end{Author}
%
\begin{References}\relax
Sakamoto, Y., Ishiguro, M., and Kitagawa G. (1986).
\emph{Akaike Information Criterion Statistics}.
D. Reidel Publishing Company.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{extractAIC}{extractAIC}}, \code{\LinkA{logLik}{logLik}}, \code{\LinkA{nobs}{nobs}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
lm1 <- lm(Fertility ~ . , data = swiss)
AIC(lm1)
stopifnot(all.equal(AIC(lm1),
                    AIC(logLik(lm1))))
BIC(lm1)

lm2 <- update(lm1, . ~ . -Examination)
AIC(lm1, lm2)
BIC(lm1, lm2)
\end{ExampleCode}
\end{Examples}
\HeaderA{alias}{Find Aliases (Dependencies) in a Model}{alias}
\methaliasA{alias.formula}{alias}{alias.formula}
\methaliasA{alias.lm}{alias}{alias.lm}
\keyword{models}{alias}
%
\begin{Description}\relax
Find aliases (linearly dependent terms) in a linear model specified by
a formula.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
alias(object, ...)

## S3 method for class 'formula'
alias(object, data, ...)

## S3 method for class 'lm'
alias(object, complete = TRUE, partial = FALSE,
      partial.pattern = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] A fitted model object, for example from \code{lm} or
\code{aov}, or a formula for \code{alias.formula}.
\item[\code{data}] Optionally, a data frame to search for the objects
in the formula.
\item[\code{complete}] Should information on complete aliasing be included?
\item[\code{partial}] Should information on partial aliasing be included?
\item[\code{partial.pattern}] Should partial aliasing be presented in a
schematic way? If this is done, the results are presented in a
more compact way, usually giving the deciles of the coefficients.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Although the main method is for class \code{"lm"}, \code{alias} is
most useful for experimental designs and so is used with fits from
\code{aov}.
Complete aliasing refers to effects in linear models that cannot be estimated
independently of the terms which occur earlier in the model and so
have their coefficients omitted from the fit. Partial aliasing refers
to effects that can be estimated less precisely because of
correlations induced by the design.
\end{Details}
%
\begin{Value}
A list (of \code{\LinkA{class}{class}} \code{"listof"}) containing components
\begin{ldescription}
\item[\code{Model}] Description of the model; usually the formula.
\item[\code{Complete}] A matrix with columns corresponding to effects that
are linearly dependent on the rows.
\item[\code{Partial}] The correlations of the estimable effects, with a zero
diagonal. An object of class \code{"mtable"} which has its own
\code{\LinkA{print}{print}} method.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The aliasing pattern may depend on the contrasts in use: Helmert
contrasts are probably most useful.

The defaults are different from those in S.
\end{Note}
%
\begin{Author}\relax
The design was inspired by the S function of the same name described
in Chambers \emph{et al.} (1992).
\end{Author}
%
\begin{References}\relax
Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
\emph{Analysis of variance; designed experiments.}
Chapter 5 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
## From Venables and Ripley (2002) p.165.
utils::data(npk, package="MASS")

op <- options(contrasts=c("contr.helmert", "contr.poly"))
npk.aov <- aov(yield ~ block + N*P*K, npk)
alias(npk.aov)
options(op)# reset
\end{ExampleCode}
\end{Examples}
\HeaderA{anova}{Anova Tables}{anova}
\aliasA{print.anova}{anova}{print.anova}
\keyword{regression}{anova}
\keyword{models}{anova}
%
\begin{Description}\relax
Compute analysis of variance (or deviance) tables for one or more
fitted model objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
anova(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object containing the results returned by a model
fitting function (e.g., \code{lm} or \code{glm}).
\item[\code{...}] additional objects of the same type.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
This (generic) function returns an object of class \code{anova}.
These objects represent analysis-of-variance and analysis-of-deviance tables.
When given a single argument it produces a table which
tests whether the model terms are significant.

When given a sequence of objects, \code{anova} tests
the models against one another in the order specified.

The print method for \code{anova} objects prints
tables in a `pretty' form.
\end{Value}
%
\begin{Section}{Warning}
The comparison between two or more models will only be valid if they
are fitted to the same dataset. This may be a problem if there are
missing values and \R{}'s default of \code{na.action = na.omit} is used.
\end{Section}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{coefficients}{coefficients}}, \code{\LinkA{effects}{effects}},
\code{\LinkA{fitted.values}{fitted.values}}, \code{\LinkA{residuals}{residuals}},
\code{\LinkA{summary}{summary}}, \code{\LinkA{drop1}{drop1}}, \code{\LinkA{add1}{add1}}.
\end{SeeAlso}
\HeaderA{anova.glm}{Analysis of Deviance for Generalized Linear Model Fits}{anova.glm}
\aliasA{anova.glmlist}{anova.glm}{anova.glmlist}
\keyword{models}{anova.glm}
\keyword{regression}{anova.glm}
%
\begin{Description}\relax
Compute an analysis of deviance table for one or more generalized
linear model fits.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'glm'
anova(object, ..., dispersion = NULL, test = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object, ...}] objects of class \code{glm}, typically
the result of a call to \code{\LinkA{glm}{glm}}, or a list of
\code{objects} for the \code{"glmlist"} method.
\item[\code{dispersion}] the dispersion parameter for the fitting family.
By default it is obtained from the object(s).
\item[\code{test}] a character string, (partially) matching one of \code{"Chisq"}, 
\code{"LRT"}, \code{"Rao"},
\code{"F"} or \code{"Cp"}. See \code{\LinkA{stat.anova}{stat.anova}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Specifying a single object gives a sequential analysis of deviance
table for that fit.  That is, the reductions in the residual deviance
as each term of the formula is added in turn are given in as
the rows of a table, plus the residual deviances themselves.

If more than one object is specified, the table has a row for the
residual degrees of freedom and deviance for each model.
For all but the first model, the change in degrees of freedom and
deviance is also given. (This only makes statistical sense if the
models are nested.)  It is conventional to list the models from
smallest to largest, but this is up to the user.

The table will optionally contain test statistics (and P values)
comparing the reduction in deviance for the row to the residuals.
For models with known dispersion (e.g., binomial and Poisson fits)
the chi-squared test is most appropriate, and for those with
dispersion estimated by moments (e.g., \code{gaussian},
\code{quasibinomial} and \code{quasipoisson} fits) the F test is
most appropriate.  Mallows' \eqn{C_p}{} statistic is the residual
deviance plus twice the estimate of \eqn{\sigma^2}{} times
the residual degrees of freedom, which is closely related to AIC (and
a multiple of it if the dispersion is known). 
You can also choose \code{"LRT"} and 
\code{"Rao"} for likelihood ratio tests and Rao's efficient score test. 
The former is synonymous with \code{"Chisq"} (although both have 
an asymptotic chi-square distribution).

The dispersion estimate will be taken from the largest model, using
the value returned by \code{\LinkA{summary.glm}{summary.glm}}.  As this will in most
cases use a Chisquared-based estimate, the F tests are not based on
the residual deviance in the analysis of deviance table shown.
\end{Details}
%
\begin{Value}
An object of class \code{"anova"} inheriting from class \code{"data.frame"}.
\end{Value}
%
\begin{Section}{Warning}
The comparison between two or more models by \code{anova} or
\code{anova.glmlist} will only be valid if they
are fitted to the same dataset. This may be a problem if there are
missing values and \R{}'s default of \code{na.action = na.omit} is used,
and \code{anova.glmlist} will detect this with an error.
\end{Section}
%
\begin{References}\relax
Hastie, T. J. and Pregibon, D. (1992)
\emph{Generalized linear models.}
Chapter 6 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{glm}{glm}}, \code{\LinkA{anova}{anova}}.

\code{\LinkA{drop1}{drop1}} for
so-called `type II' anova where each term is dropped one at a
time respecting their hierarchy.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## --- Continuing the Example from  '?glm':

anova(glm.D93)
anova(glm.D93, test = "Cp")
anova(glm.D93, test = "Chisq")
glm.D93a <- update(glm.D93, ~treatment*outcome) ## equivalent to Pearson Chi-square
anova(glm.D93, glm.D93a, test = "Rao")
\end{ExampleCode}
\end{Examples}
\HeaderA{anova.lm}{ANOVA for Linear Model Fits}{anova.lm}
\aliasA{anova.lmlist}{anova.lm}{anova.lmlist}
\keyword{regression}{anova.lm}
\keyword{models}{anova.lm}
%
\begin{Description}\relax
Compute an analysis of variance table for one or more linear model fits.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'lm'
anova(object, ...)

anova.lmlist(object, ..., scale = 0, test = "F")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object, ...}] objects of class \code{lm}, usually, a result of a
call to \code{\LinkA{lm}{lm}}.
\item[\code{test}] a character string specifying the test statistic to be
used. Can be one of \code{"F"}, \code{"Chisq"} or \code{"Cp"},
with partial matching allowed, or \code{NULL} for no test.
\item[\code{scale}] numeric. An estimate of the noise variance
\eqn{\sigma^2}{}. If zero this will be estimated from the
largest model considered.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Specifying a single object gives a sequential analysis of variance
table for that fit.  That is, the reductions in the residual sum of
squares as each term of the formula is added in turn are given in as
the rows of a table, plus the residual sum of squares.

The table will contain F statistics (and P values) comparing the
mean square for the row to the residual mean square.

If more than one object is specified, the table has a row for the
residual degrees of freedom and sum of squares for each model.
For all but the first model, the change in degrees of freedom and sum
of squares is also given. (This only make statistical sense if the
models are nested.)  It is conventional to list the models from
smallest to largest, but this is up to the user.

Optionally the table can include test statistics.  Normally the
F statistic is most appropriate, which compares the mean square for a
row to the residual sum of squares for the largest model considered.
If \code{scale} is specified chi-squared tests can be used. Mallows'
\eqn{C_p}{} statistic is the residual sum of squares plus twice the
estimate of \eqn{\sigma^2}{} times the residual degrees of freedom.
\end{Details}
%
\begin{Value}
An object of class \code{"anova"} inheriting from class \code{"data.frame"}.
\end{Value}
%
\begin{Section}{Warning}
The comparison between two or more models will only be valid if they
are fitted to the same dataset. This may be a problem if there are
missing values and \R{}'s default of \code{na.action = na.omit} is used,
and \code{anova.lmlist} will detect this with an error.
\end{Section}
%
\begin{Note}\relax
Versions of \R{} prior to 1.2.0 based F tests on pairwise comparisons,
and this behaviour can still be obtained by a direct call to
\code{anovalist.lm}.
\end{Note}
%
\begin{References}\relax
Chambers, J. M. (1992)
\emph{Linear models.}
Chapter 4 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
The model fitting function \code{\LinkA{lm}{lm}}, \code{\LinkA{anova}{anova}}.

\code{\LinkA{drop1}{drop1}} for
so-called `type II' anova where each term is dropped one at a
time respecting their hierarchy.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## sequential table
fit <- lm(sr ~ ., data = LifeCycleSavings)
anova(fit)

## same effect via separate models
fit0 <- lm(sr ~ 1, data = LifeCycleSavings)
fit1 <- update(fit0, . ~ . + pop15)
fit2 <- update(fit1, . ~ . + pop75)
fit3 <- update(fit2, . ~ . + dpi)
fit4 <- update(fit3, . ~ . + ddpi)
anova(fit0, fit1, fit2, fit3, fit4, test="F")

anova(fit4, fit2, fit0, test="F") # unconventional order
\end{ExampleCode}
\end{Examples}
\HeaderA{anova.mlm}{Comparisons between Multivariate Linear Models}{anova.mlm}
\aliasA{anova.mlmlist}{anova.mlm}{anova.mlmlist}
\keyword{regression}{anova.mlm}
\keyword{models}{anova.mlm}
\keyword{multivariate}{anova.mlm}
%
\begin{Description}\relax
Compute a (generalized) analysis of variance table for one or more 
multivariate linear models. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'mlm'
anova(object, ...,
      test = c("Pillai", "Wilks", "Hotelling-Lawley", "Roy", "Spherical"),
      Sigma = diag(nrow = p), T = Thin.row(proj(M) - proj(X)),
       M = diag(nrow = p), X = ~0,
      idata = data.frame(index = seq_len(p)), tol = 1e-7)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object of class \code{"mlm"}.
\item[\code{...}] further objects of class \code{"mlm"}.
\item[\code{test}] choice of test statistic (see below).
\item[\code{Sigma}] (only relevant if  \code{test == "Spherical"}).  Covariance
matrix assumed proportional to \code{Sigma}.
\item[\code{T}] transformation matrix.  By default computed from \code{M} and
\code{X}.
\item[\code{M}] formula or matrix describing the outer projection (see below).
\item[\code{X}] formula or matrix describing the inner projection (see below).
\item[\code{idata}] data frame describing intra-block design.

\item[\code{tol}] tolerance to be used in deciding if the residuals are
rank-deficient: see \code{\LinkA{qr}{qr}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \code{anova.mlm} method uses either a multivariate test statistic for
the summary table, or a test based on sphericity assumptions (i.e.
that the covariance is proportional to a given matrix).

For the multivariate test, Wilks' statistic is most popular in the
literature, but the default Pillai--Bartlett statistic is
recommended by Hand and Taylor (1987).  See
\code{\LinkA{summary.manova}{summary.manova}} for further details.

For the \code{"Spherical"} test, proportionality is usually with the
identity matrix but a different matrix can be specified using \code{Sigma}).
Corrections for asphericity known as the Greenhouse--Geisser,
respectively Huynh--Feldt, epsilons are given and adjusted \eqn{F}{} tests are
performed.

It is common to transform the observations prior to testing. This
typically involves 
transformation to intra-block differences, but more complicated
within-block designs can be encountered,
making more elaborate transformations necessary.  A
transformation matrix \code{T} can be given directly or specified as
the difference between two projections onto the spaces spanned by
\code{M} and \code{X}, which in turn can be given as matrices or as
model formulas with respect to \code{idata} (the tests will be
invariant to parametrization of the quotient space \code{M/X}).

As with \code{anova.lm}, all test statistics use the SSD matrix from
the largest model considered as the (generalized) denominator.

Contrary to other \code{anova} methods, the intercept is not excluded
from the display in the single-model case.  When contrast
transformations are involved, it often makes good sense to test for a
zero intercept.
\end{Details}
%
\begin{Value}
An object of class \code{"anova"} inheriting from class \code{"data.frame"}
\end{Value}
%
\begin{Note}\relax
The Huynh--Feldt epsilon differs from that calculated by SAS (as of
v. 8.2) except when the DF is equal to the number of observations
minus one.  This is believed to be a bug in SAS, not in \R{}.
\end{Note}
%
\begin{References}\relax
Hand, D. J. and Taylor, C. C.  (1987)
\emph{Multivariate Analysis of Variance and Repeated Measures.}
Chapman and Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{summary.manova}{summary.manova}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)
utils::example(SSD) # Brings in the mlmfit and reacttime objects

mlmfit0 <- update(mlmfit, ~0)

### Traditional tests of intrasubj. contrasts
## Using MANOVA techniques on contrasts:
anova(mlmfit, mlmfit0, X=~1)

## Assuming sphericity
anova(mlmfit, mlmfit0, X=~1, test="Spherical") 


### tests using intra-subject 3x2 design
idata <- data.frame(deg=gl(3,1,6,labels=c(0,4,8)),
                    noise=gl(2,3,6,labels=c("A","P")))

anova(mlmfit, mlmfit0, X = ~ deg + noise,
      idata = idata, test = "Spherical")
anova(mlmfit, mlmfit0, M = ~ deg + noise, X = ~ noise,
      idata = idata, test="Spherical" )
anova(mlmfit, mlmfit0, M = ~ deg + noise, X = ~ deg,
      idata = idata, test="Spherical" )

f <- factor(rep(1:2,5)) # bogus, just for illustration
mlmfit2 <- update(mlmfit, ~f)
anova(mlmfit2, mlmfit, mlmfit0, X = ~1, test = "Spherical")
anova(mlmfit2, X = ~1, test = "Spherical")
# one-model form, eqiv. to previous 

### There seems to be a strong interaction in these data
plot(colMeans(reacttime))
\end{ExampleCode}
\end{Examples}
\HeaderA{ansari.test}{Ansari-Bradley Test}{ansari.test}
\methaliasA{ansari.test.default}{ansari.test}{ansari.test.default}
\methaliasA{ansari.test.formula}{ansari.test}{ansari.test.formula}
\keyword{htest}{ansari.test}
%
\begin{Description}\relax
Performs the Ansari-Bradley two-sample test for a difference in scale
parameters.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ansari.test(x, ...)

## Default S3 method:
ansari.test(x, y,
            alternative = c("two.sided", "less", "greater"),
            exact = NULL, conf.int = FALSE, conf.level = 0.95,
            ...)

## S3 method for class 'formula'
ansari.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric vector of data values.
\item[\code{y}] numeric vector of data values.
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"}, \code{"greater"} or \code{"less"}.  You
can specify just the initial letter.
\item[\code{exact}] a logical indicating whether an exact p-value
should be computed.
\item[\code{conf.int}] a logical,indicating whether a confidence interval
should be computed.
\item[\code{conf.level}] confidence level of the interval.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
is a numeric variable giving the data values and \code{rhs} a factor
with two levels giving the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Suppose that \code{x} and \code{y} are independent samples from
distributions with densities \eqn{f((t-m)/s)/s}{} and \eqn{f(t-m)}{},
respectively, where \eqn{m}{} is an unknown nuisance parameter and
\eqn{s}{}, the ratio of scales, is the parameter of interest.  The
Ansari-Bradley test is used for testing the null that \eqn{s}{} equals
1, the two-sided alternative being that \eqn{s \ne 1}{} (the
distributions differ only in variance), and the one-sided alternatives
being \eqn{s > 1}{} (the distribution underlying \code{x} has a larger
variance, \code{"greater"}) or \eqn{s < 1}{} (\code{"less"}).

By default (if \code{exact} is not specified), an exact p-value
is computed if both samples contain less than 50 finite values and
there are no ties.  Otherwise, a normal approximation is used.

Optionally, a nonparametric confidence interval and an estimator for
\eqn{s}{} are computed.  If exact p-values are available, an exact
confidence interval is obtained by the algorithm described in Bauer
(1972), and the Hodges-Lehmann estimator is employed.  Otherwise, the
returned confidence interval and point estimate are based on normal
approximations.

Note that mid-ranks are used in the case of ties rather than average
scores as employed in Hollander \& Wolfe (1973).  See, e.g., Hajek,
Sidak and Sen (1999), pages 131ff, for more information.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the Ansari-Bradley test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{null.value}] the ratio of scales \eqn{s}{} under the null, 1.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] the string \code{"Ansari-Bradley test"}.
\item[\code{data.name}] a character string giving the names of the data.
\item[\code{conf.int}] a confidence interval for the scale parameter.
(Only present if argument \code{conf.int = TRUE}.)
\item[\code{estimate}] an estimate of the ratio of scales.
(Only present if argument \code{conf.int = TRUE}.)
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
To compare results of the Ansari-Bradley test to those of the F test
to compare two variances (under the assumption of normality), observe
that \eqn{s}{} is the ratio of scales and hence \eqn{s^2}{} is the ratio
of variances (provided they exist), whereas for the F test the ratio
of variances itself is the parameter of interest.  In particular,
confidence intervals are for \eqn{s}{} in the Ansari-Bradley test but
for \eqn{s^2}{} in the F test.
\end{Note}
%
\begin{References}\relax
David F. Bauer (1972),
Constructing confidence sets using rank statistics.
\emph{Journal of the American Statistical Association}
\bold{67}, 687--690.

Jaroslav Hajek, Zbynek Sidak and Pranab K. Sen (1999),
\emph{Theory of Rank Tests}.
San Diego, London: Academic Press.

Myles Hollander and Douglas A. Wolfe (1973),
\emph{Nonparametric Statistical Methods.}
New York: John Wiley \& Sons.
Pages 83--92.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{fligner.test}{fligner.test}} for a rank-based (nonparametric)
\eqn{k}{}-sample test for homogeneity of variances;
\code{\LinkA{mood.test}{mood.test}} for another rank-based two-sample test for a
difference in scale parameters;
\code{\LinkA{var.test}{var.test}} and \code{\LinkA{bartlett.test}{bartlett.test}} for parametric
tests for the homogeneity in variance.

\code{\LinkA{ansari\_test}{ansari.Rul.test}} in package \Rhref{http://CRAN.R-project.org/package=coin}{\pkg{coin}}
for exact and approximate \emph{conditional} p-values for the
Ansari-Bradley test, as well as different methods for handling ties.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Hollander & Wolfe (1973, p. 86f):
## Serum iron determination using Hyland control sera
ramsay <- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,
            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)
jung.parekh <- c(107, 108, 106, 98, 105, 103, 110, 105, 104,
            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)
ansari.test(ramsay, jung.parekh)

ansari.test(rnorm(10), rnorm(10, 0, 2), conf.int = TRUE)

## try more points - failed in 2.4.1
ansari.test(rnorm(100), rnorm(100, 0, 2), conf.int = TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{aov}{Fit an Analysis of Variance Model}{aov}
\aliasA{Error}{aov}{Error}
\aliasA{print.aov}{aov}{print.aov}
\aliasA{print.aovlist}{aov}{print.aovlist}
\keyword{models}{aov}
\keyword{regression}{aov}
%
\begin{Description}\relax
Fit an analysis of variance model by a call to \code{lm} for each stratum.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
aov(formula, data = NULL, projections = FALSE, qr = TRUE,
    contrasts = NULL, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] A formula specifying the model.
\item[\code{data}] A data frame in which the variables specified in the
formula will be found. If missing, the variables are searched for in
the standard way.
\item[\code{projections}] Logical flag: should the projections be returned?
\item[\code{qr}] Logical flag: should the QR decomposition be returned?
\item[\code{contrasts}] A list of contrasts to be used for some of the factors
in the formula. These are not used for any \code{Error} term, and
supplying contrasts for factors only in the \code{Error} term will give
a warning.
\item[\code{...}] Arguments to be passed to \code{lm}, such as \code{subset}
or \code{na.action}.  See `Details' about \code{weights}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This provides a wrapper to \code{lm} for fitting linear models to
balanced or unbalanced experimental designs.

The main difference from \code{lm} is in the way \code{print},
\code{summary} and so on handle the fit: this is expressed in the
traditional language of the analysis of variance rather than that of
linear models.

If the formula contains a single \code{Error} term, this is used to
specify error strata, and appropriate models are fitted within each
error stratum.

The formula can specify multiple responses.

Weights can be specified by a \code{weights} argument, but should not
be used with an \code{Error} term, and are incompletely supported
(e.g., not by \code{\LinkA{model.tables}{model.tables}}).
\end{Details}
%
\begin{Value}
An object of class \code{c("aov", "lm")} or for multiple responses
of class \code{c("maov", "aov", "mlm", "lm")} or for multiple error
strata of class \code{"aovlist"}.  There are
\code{\LinkA{print}{print}} and \code{\LinkA{summary}{summary}} methods available for these.
\end{Value}
%
\begin{Note}\relax
\code{aov} is designed for balanced designs, and the results can be
hard to interpret without balance: beware that missing values in the
response(s) will likely lose the balance.  If there are two or more
error strata, the methods used are statistically inefficient without
balance, and it may be better to use \code{\LinkA{lme}{lme}} in
package \Rhref{http://CRAN.R-project.org/package=nlme}{\pkg{nlme}}.

Balance can be checked with the \code{\LinkA{replications}{replications}} function.

The default `contrasts' in \R{} are not orthogonal contrasts, and
\code{aov} and its helper functions will work better with such
contrasts: see the examples for how to select these.
\end{Note}
%
\begin{Author}\relax
The design was inspired by the S function of the same name described
in Chambers \emph{et al.} (1992).
\end{Author}
%
\begin{References}\relax
Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
\emph{Analysis of variance; designed experiments.}
Chapter 5 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{lm}{lm}}, \code{\LinkA{summary.aov}{summary.aov}},
\code{\LinkA{replications}{replications}}, \code{\LinkA{alias}{alias}},
\code{\LinkA{proj}{proj}}, \code{\LinkA{model.tables}{model.tables}}, \code{\LinkA{TukeyHSD}{TukeyHSD}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## From Venables and Ripley (2002) p.165.
utils::data(npk, package="MASS")

## Set orthogonal contrasts.
op <- options(contrasts=c("contr.helmert", "contr.poly"))
( npk.aov <- aov(yield ~ block + N*P*K, npk) )
summary(npk.aov)
coefficients(npk.aov)

## to show the effects of re-ordering terms contrast the two fits
aov(yield ~ block + N * P + K, npk)
aov(terms(yield ~ block + N * P + K, keep.order=TRUE), npk)


## as a test, not particularly sensible statistically
npk.aovE <- aov(yield ~  N*P*K + Error(block), npk)
npk.aovE
summary(npk.aovE)
options(op)# reset to previous
\end{ExampleCode}
\end{Examples}
\HeaderA{approxfun}{Interpolation Functions}{approxfun}
\aliasA{approx}{approxfun}{approx}
\keyword{arith}{approxfun}
\keyword{dplot}{approxfun}
%
\begin{Description}\relax
Return a list of points which linearly interpolate given data points,
or a function performing the linear (or constant) interpolation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
approx   (x, y = NULL, xout, method = "linear", n=50,
          yleft, yright, rule = 1, f = 0, ties = mean)

approxfun(x, y = NULL,       method  ="linear",
          yleft, yright, rule = 1, f = 0, ties = mean)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] numeric vectors giving the coordinates of the points to be
interpolated.  Alternatively a single plotting structure can be
specified: see \code{\LinkA{xy.coords}{xy.coords}}.
\item[\code{xout}] an optional set of numeric values specifying where
interpolation is to take place.
\item[\code{method}] specifies the interpolation method to be used.  Choices
are \code{"linear"} or \code{"constant"}.
\item[\code{n}] If \code{xout} is not specified, interpolation takes place at
\code{n} equally spaced points spanning the interval [\code{min(x)},
\code{max(x)}].
\item[\code{yleft}] the value to be returned when input \code{x} values are
less than \code{min(x)}. The default is defined by the value
of \code{rule} given below.
\item[\code{yright}] the value to be returned when input \code{x} values are
greater than \code{max(x)}. The default is defined by the value
of \code{rule} given below.
\item[\code{rule}] an integer (of length 1 or 2) describing how interpolation
is to take place outside the interval [\code{min(x)}, \code{max(x)}].
If \code{rule} is \code{1} then \code{NA}s are returned for such
points and if it is \code{2}, the value at the closest data extreme
is used.  Use, e.g., \code{rule = 2:1}, if the left and right side
extrapolation should differ.
\item[\code{f}] for \code{method = "constant"} a number between 0 and 1
inclusive, indicating a compromise between left- and
right-continuous step functions. If \code{y0} and \code{y1} are the
values to the left and right of the point then the value is
\code{y0*(1-f)+y1*f} so that \code{f = 0} is right-continuous and
\code{f = 1} is left-continuous.
\item[\code{ties}] Handling of tied \code{x} values.  Either a function
with a single vector argument returning a single number result or
the string \code{"ordered"}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The inputs can contain missing values which are deleted, so at least
two complete \code{(x, y)} pairs are required (for \code{method =
  "linear"}, one otherwise).  If there are duplicated (tied) \code{x}
values and \code{ties} is a function it is applied to the \code{y}
values for each distinct \code{x} value.
Useful functions in this context include \code{\LinkA{mean}{mean}},
\code{\LinkA{min}{min}}, and \code{\LinkA{max}{max}}.  If \code{ties = "ordered"}
the \code{x} values are assumed to be already ordered.  The first
\code{y} value will be used for interpolation to the left and the last
one for interpolation to the right.
\end{Details}
%
\begin{Value}
\code{approx} returns a list with components \code{x} and \code{y},
containing \code{n} coordinates which interpolate the given data
points according to the \code{method} (and \code{rule}) desired.

The function \code{approxfun} returns a function performing (linear or
constant) interpolation of the given data points.  For a given set of
\code{x} values, this function will return the corresponding
interpolated values.  This is often more useful than \code{approx}.
\end{Value}
%
\begin{Section}{Warning}
The value returned by \code{approxfun} contains references to the code
in the current version of \R{}: it is not intended to be saved and
loaded into a different \R{} session.
\end{Section}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{spline}{spline}} and \code{\LinkA{splinefun}{splinefun}} for spline
interpolation.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

x <- 1:10
y <- rnorm(10)
par(mfrow = c(2,1))
plot(x, y, main = "approx(.) and approxfun(.)")
points(approx(x, y), col = 2, pch = "*")
points(approx(x, y, method = "constant"), col = 4, pch = "*")

f <- approxfun(x, y)
curve(f(x), 0, 11, col = "green2")
points(x, y)
is.function(fc <- approxfun(x, y, method = "const")) # TRUE
curve(fc(x), 0, 10, col = "darkblue", add = TRUE)
## different extrapolation on left and right side :
plot(approxfun(x, y, rule = 2:1), 0, 11,
     col = "tomato", add = TRUE, lty = 3, lwd = 2)

## Show treatment of 'ties' :

x <- c(2,2:4,4,4,5,5,7,7,7)
y <- c(1:6, 5:4, 3:1)
approx(x,y, xout=x)$y # warning
(ay <- approx(x,y, xout=x, ties = "ordered")$y)
stopifnot(ay == c(2,2,3,6,6,6,4,4,1,1,1))
approx(x,y, xout=x, ties = min)$y
approx(x,y, xout=x, ties = max)$y


\end{ExampleCode}
\end{Examples}
\HeaderA{ar}{Fit Autoregressive Models to Time Series}{ar}
\methaliasA{ar.burg}{ar}{ar.burg}
\methaliasA{ar.burg.default}{ar}{ar.burg.default}
\methaliasA{ar.burg.mts}{ar}{ar.burg.mts}
\methaliasA{ar.mle}{ar}{ar.mle}
\methaliasA{ar.yw}{ar}{ar.yw}
\methaliasA{ar.yw.default}{ar}{ar.yw.default}
\methaliasA{ar.yw.mts}{ar}{ar.yw.mts}
\aliasA{predict.ar}{ar}{predict.ar}
\aliasA{print.ar}{ar}{print.ar}
\keyword{ts}{ar}
%
\begin{Description}\relax
Fit an autoregressive time series model to the data, by default
selecting the complexity by AIC.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ar(x, aic = TRUE, order.max = NULL,
   method=c("yule-walker", "burg", "ols", "mle", "yw"),
   na.action, series, ...)

ar.burg(x, ...)
## Default S3 method:
ar.burg(x, aic = TRUE, order.max = NULL,
        na.action = na.fail, demean = TRUE, series,
        var.method = 1, ...)
## S3 method for class 'mts'
ar.burg(x, aic = TRUE, order.max = NULL,
        na.action = na.fail, demean = TRUE, series,
        var.method = 1, ...)

ar.yw(x, ...)
## Default S3 method:
ar.yw(x, aic = TRUE, order.max = NULL,
      na.action = na.fail, demean = TRUE, series, ...)
## S3 method for class 'mts'
ar.yw(x, aic = TRUE, order.max = NULL,
      na.action = na.fail, demean = TRUE, series,
      var.method = 1, ...)

ar.mle(x, aic = TRUE, order.max = NULL, na.action = na.fail,
       demean = TRUE, series, ...)

## S3 method for class 'ar'
predict(object, newdata, n.ahead = 1, se.fit = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A univariate or multivariate time series.

\item[\code{aic}] Logical flag.  If \code{TRUE} then the Akaike Information
Criterion is used to choose the order of the autoregressive
model. If \code{FALSE}, the model of order \code{order.max} is
fitted.

\item[\code{order.max}] Maximum order (or order) of model to fit. Defaults
to the smaller of \eqn{N-1}{} and \eqn{10\log_{10}(N)}{}
where \eqn{N}{} is the number of observations
except for \code{method="mle"} where it is the minimum of this
quantity and 12.

\item[\code{method}] Character string giving the method used to fit the
model.  Must be one of the strings in the default argument
(the first few characters are sufficient).  Defaults to
\code{"yule-walker"}.

\item[\code{na.action}] function to be called to handle missing values.

\item[\code{demean}] should a mean be estimated during fitting?

\item[\code{series}] names for the series.  Defaults to
\code{deparse(substitute(x))}.

\item[\code{var.method}] the method to estimate the innovations variance
(see `Details').

\item[\code{...}] additional arguments for specific methods.

\item[\code{object}] a fit from \code{ar}.

\item[\code{newdata}] data to which to apply the prediction.

\item[\code{n.ahead}] number of steps ahead at which to predict.

\item[\code{se.fit}] logical: return estimated standard errors of the
prediction error?
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For definiteness, note that the AR coefficients have the sign in

\deqn{x_t - \mu = a_1(x_{t-1} - \mu) + \cdots +  a_p(x_{t-p} - \mu) + e_t}{}

\code{ar} is just a wrapper for the functions \code{ar.yw},
\code{ar.burg}, \code{\LinkA{ar.ols}{ar.ols}} and \code{ar.mle}.

Order selection is done by AIC if \code{aic} is true. This is
problematic, as of the methods here only \code{ar.mle} performs
true maximum likelihood estimation. The AIC is computed as if the variance
estimate were the MLE, omitting the determinant term from the
likelihood. Note that this is not the same as the Gaussian likelihood
evaluated at the estimated parameter values. In \code{ar.yw} the
variance matrix of the innovations is computed from the fitted
coefficients and the autocovariance of \code{x}.

\code{ar.burg} allows two methods to estimate the innovations
variance and hence AIC. Method 1 is to use the update given by
the Levinson-Durbin recursion (Brockwell and Davis, 1991, (8.2.6)
on page 242), and follows S-PLUS. Method 2 is the mean of the sum
of squares of the forward and backward prediction errors
(as in Brockwell and Davis, 1996, page 145). Percival and Walden
(1998) discuss both. In the multivariate case the estimated
coefficients will depend (slightly) on the variance estimation method.

Remember that \code{ar} includes by default a constant in the model, by
removing the overall mean of \code{x} before fitting the AR model,
or (\code{ar.mle}) estimating a constant to subtract.
\end{Details}
%
\begin{Value}
For \code{ar} and its methods a list of class \code{"ar"} with
the following elements:
\begin{ldescription}
\item[\code{order}] The order of the fitted model.  This is chosen by
minimizing the AIC if \code{aic=TRUE}, otherwise it is \code{order.max}.
\item[\code{ar}] Estimated autoregression coefficients for the fitted model.
\item[\code{var.pred}] The prediction variance: an estimate of the portion of the
variance of the time series that is not explained by the
autoregressive model.
\item[\code{x.mean}] The estimated mean of the series used in fitting and for
use in prediction.
\item[\code{x.intercept}] (\code{ar.ols} only.) The intercept in the model for
\code{x - x.mean}.
\item[\code{aic}] The differences in AIC between each model and the
best-fitting model.  Note that the latter can have an AIC of \code{-Inf}.
\item[\code{n.used}] The number of observations in the time series.
\item[\code{order.max}] The value of the \code{order.max} argument.
\item[\code{partialacf}] The estimate of the partial autocorrelation function
up to lag \code{order.max}.
\item[\code{resid}] residuals from the fitted model, conditioning on the
first \code{order} observations. The first \code{order} residuals
are set to \code{NA}. If \code{x} is a time series, so is \code{resid}.
\item[\code{method}] The value of the \code{method} argument.
\item[\code{series}] The name(s) of the time series.
\item[\code{frequency}] The frequency of the time series.
\item[\code{call}] The matched call.
\item[\code{asy.var.coef}] (univariate case, \code{order > 0}.)
The asymptotic-theory variance matrix of the coefficient estimates.

\end{ldescription}
For \code{predict.ar}, a time series of predictions, or if
\code{se.fit = TRUE}, a list with components \code{pred}, the
predictions, and \code{se}, the estimated standard errors. Both
components are time series.
\end{Value}
%
\begin{Note}\relax
Only the univariate case of \code{ar.mle} is implemented.

Fitting by \code{method="mle"} to long series can be very slow.
\end{Note}
%
\begin{Author}\relax
Martyn Plummer. Univariate case of \code{ar.yw}, \code{ar.mle}
and C code for univariate case of \code{ar.burg} by B. D. Ripley.
\end{Author}
%
\begin{References}\relax
Brockwell, P. J. and Davis, R. A. (1991) \emph{Time
Series and Forecasting Methods.}  Second edition. Springer, New
York. Section 11.4.

Brockwell, P. J. and Davis, R. A. (1996) \emph{Introduction to Time
Series and Forecasting.} Springer, New York. Sections 5.1 and 7.6.

Percival, D. P. and Walden, A. T. (1998) \emph{Spectral Analysis
for Physical Applications.} Cambridge University Press.

Whittle, P. (1963) On the fitting of multivariate autoregressions
and the approximate canonical factorization of a spectral density
matrix. \emph{Biometrika} \bold{40}, 129--134.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ar.ols}{ar.ols}}, \code{\LinkA{arima}{arima}} for ARMA models;
\code{\LinkA{acf2AR}{acf2AR}}, for AR construction from the ACF.

\code{\LinkA{arima.sim}{arima.sim}} for simulation of AR processes.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ar(lh)
ar(lh, method="burg")
ar(lh, method="ols")
ar(lh, FALSE, 4) # fit ar(4)

(sunspot.ar <- ar(sunspot.year))
predict(sunspot.ar, n.ahead=25)
## try the other methods too

ar(ts.union(BJsales, BJsales.lead))
## Burg is quite different here, as is OLS (see ar.ols)
ar(ts.union(BJsales, BJsales.lead), method="burg")
\end{ExampleCode}
\end{Examples}
\HeaderA{ar.ols}{Fit Autoregressive Models to Time Series by OLS}{ar.ols}
\keyword{ts}{ar.ols}
%
\begin{Description}\relax
Fit an autoregressive time series model to the data by ordinary
least squares, by default selecting the complexity by AIC.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ar.ols(x, aic = TRUE, order.max = NULL, na.action = na.fail,
       demean = TRUE, intercept = demean, series, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A univariate or multivariate time series.

\item[\code{aic}] Logical flag.  If \code{TRUE} then the Akaike Information
Criterion is used to choose the order of the autoregressive
model. If \code{FALSE}, the model of order \code{order.max} is
fitted.

\item[\code{order.max}] Maximum order (or order) of model to fit. Defaults
to \eqn{10\log_{10}(N)}{} where \eqn{N}{} is the number
of observations.

\item[\code{na.action}] function to be called to handle missing values.

\item[\code{demean}] should the AR model be for \code{x} minus its mean?

\item[\code{intercept}] should a separate intercept term be fitted?

\item[\code{series}] names for the series.  Defaults to
\code{deparse(substitute(x))}.

\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{ar.ols} fits the general AR model to a possibly non-stationary
and/or multivariate system of series \code{x}. The resulting
unconstrained least squares estimates are consistent, even if
some of the series are non-stationary and/or co-integrated.
For definiteness, note that the AR coefficients have the sign in

\deqn{x_t - \mu = a_0 + a_1(x_{t-1} - \mu) + \cdots + a_p(x_{t-p} - \mu) + e_t}{}

where \eqn{a_0}{} is zero unless \code{intercept} is true, and
\eqn{\mu}{} is the sample mean if \code{demean} is true, zero
otherwise.

Order selection is done by AIC if \code{aic} is true. This is
problematic, as \code{ar.ols} does not perform
true maximum likelihood estimation. The AIC is computed as if
the variance estimate (computed from the variance matrix of the
residuals) were the MLE, omitting the determinant term from the
likelihood. Note that this is not the same as the Gaussian
likelihood evaluated at the estimated parameter values.

Some care is needed if \code{intercept} is true and \code{demean} is
false. Only use this is the series are roughly centred on
zero. Otherwise the computations may be inaccurate or fail entirely.
\end{Details}
%
\begin{Value}
A list of class \code{"ar"} with the following elements:
\begin{ldescription}
\item[\code{order}] The order of the fitted model.  This is chosen by
minimizing the AIC if \code{aic=TRUE}, otherwise it is
\code{order.max}.
\item[\code{ar}] Estimated autoregression coefficients for the fitted
model.
\item[\code{var.pred}] The prediction variance: an estimate of the portion of
the variance of the time series that is not explained by the
autoregressive model.
\item[\code{x.mean}] The estimated mean (or zero if \code{demean} is false)
of the series used in fitting and for use in prediction.
\item[\code{x.intercept}] The intercept in the model for
\code{x - x.mean}, or zero if \code{intercept} is false.
\item[\code{aic}] The differences in AIC between each model and the
best-fitting model.  Note that the latter can have an AIC of \code{-Inf}.
\item[\code{n.used}] The number of observations in the time series.
\item[\code{order.max}] The value of the \code{order.max} argument.
\item[\code{partialacf}] \code{NULL}.  For compatibility with \code{ar}.
\item[\code{resid}] residuals from the fitted model, conditioning on the
first \code{order} observations.  The first \code{order} residuals
are set to \code{NA}. If \code{x} is a time series, so is
\code{resid}.
\item[\code{method}] The character string \code{"Unconstrained LS"}.
\item[\code{series}] The name(s) of the time series.
\item[\code{frequency}] The frequency of the time series.
\item[\code{call}] The matched call.
\item[\code{asy.se.coef}] The asymptotic-theory standard errors of the
coefficient estimates.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Adrian Trapletti, Brian Ripley.
\end{Author}
%
\begin{References}\relax
Luetkepohl, H. (1991): \emph{Introduction to Multiple Time Series
Analysis.} Springer Verlag, NY, pp. 368--370.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ar}{ar}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ar(lh, method="burg")
ar.ols(lh)
ar.ols(lh, FALSE, 4) # fit ar(4)

ar.ols(ts.union(BJsales, BJsales.lead))

x <- diff(log(EuStockMarkets))
ar.ols(x, order.max=6, demean=FALSE, intercept=TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{arima}{ARIMA Modelling of Time Series}{arima}
\keyword{ts}{arima}
%
\begin{Description}\relax
Fit an ARIMA model to a univariate time series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
arima(x, order = c(0, 0, 0),
      seasonal = list(order = c(0, 0, 0), period = NA),
      xreg = NULL, include.mean = TRUE,
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c("CSS-ML", "ML", "CSS"),
      n.cond, optim.method = "BFGS",
      optim.control = list(), kappa = 1e6)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a univariate time series

\item[\code{order}] A specification of the non-seasonal part of the ARIMA
model: the three components \eqn{(p, d, q)}{} are the AR order, the
degree of differencing, and the MA order.

\item[\code{seasonal}] A specification of the seasonal part of the ARIMA
model, plus the period (which defaults to \code{frequency(x)}).
This should be a list with components \code{order} and
\code{period}, but a specification of just a numeric vector of
length 3 will be turned into a suitable list with the specification
as the \code{order}.

\item[\code{xreg}] Optionally, a vector or matrix of external regressors,
which must have the same number of rows as \code{x}.

\item[\code{include.mean}] Should the ARMA model include a mean/intercept term?  The
default is \code{TRUE} for undifferenced series, and it is ignored
for ARIMA models with differencing.

\item[\code{transform.pars}] Logical.  If true, the AR parameters are
transformed to ensure that they remain in the region of
stationarity.  Not used for \code{method = "CSS"}.

\item[\code{fixed}] optional numeric vector of the same length as the total
number of parameters.  If supplied, only \code{NA} entries in
\code{fixed} will be varied.  \code{transform.pars = TRUE}
will be overridden (with a warning) if any AR parameters are fixed.
It may be wise to set \code{transform.pars = FALSE} when fixing
MA parameters, especially near non-invertibility.


\item[\code{init}] optional numeric vector of initial parameter
values.  Missing values will be filled in, by zeroes except for
regression coefficients.  Values already specified in \code{fixed}
will be ignored.

\item[\code{method}] Fitting method: maximum likelihood or minimize
conditional sum-of-squares.  The default (unless there are missing
values) is to use conditional-sum-of-squares to find starting
values, then maximum likelihood.

\item[\code{n.cond}] Only used if fitting by conditional-sum-of-squares: the
number of initial observations to ignore.  It will be ignored if
less than the maximum lag of an AR term.

\item[\code{optim.method}] The value passed as the \code{method} argument to
\code{\LinkA{optim}{optim}}.

\item[\code{optim.control}] List of control parameters for \code{\LinkA{optim}{optim}}.

\item[\code{kappa}] the prior variance (as a multiple of the innovations
variance) for the past observations in a differenced model.  Do not
reduce this.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Different definitions of ARMA models have different signs for the
AR and/or MA coefficients.  The definition used here has

\deqn{X_t = a_1X_{t-1} + \cdots + a_pX_{t-p} + e_t + b_1e_{t-1} + \dots + b_qe_{t-q}}{}

and so the MA coefficients differ in sign from those of S-PLUS.
Further, if \code{include.mean} is true (the default for an ARMA
model), this formula applies to \eqn{X - m}{} rather than \eqn{X}{}.  For
ARIMA models with differencing, the differenced series follows a
zero-mean ARMA model. If am \code{xreg} term is included, a linear
regression (with a constant term if \code{include.mean} is true and
there is no differencing) is fitted with an ARMA model for the error
term.

The variance matrix of the estimates is found from the Hessian of
the log-likelihood, and so may only be a rough guide.

Optimization is done by \code{\LinkA{optim}{optim}}.  It will work
best if the columns in \code{xreg} are roughly scaled to zero mean
and unit variance, but does attempt to estimate suitable scalings.
\end{Details}
%
\begin{Value}
A list of class \code{"Arima"} with components:

\begin{ldescription}
\item[\code{coef}] a vector of AR, MA and regression coefficients, which can
be extracted by the \code{\LinkA{coef}{coef}} method.

\item[\code{sigma2}] the MLE of the innovations variance.

\item[\code{var.coef}] the estimated variance matrix of the coefficients
\code{coef}, which can be extracted by the \code{\LinkA{vcov}{vcov}} method.

\item[\code{loglik}] the maximized log-likelihood (of the differenced data),
or the approximation to it used.

\item[\code{arma}] A compact form of the specification, as a vector giving
the number of AR, MA, seasonal AR and seasonal MA coefficients,
plus the period and the number of non-seasonal and seasonal
differences.

\item[\code{aic}] the AIC value corresponding to the log-likelihood. Only
valid for \code{method = "ML"} fits.

\item[\code{residuals}] the fitted innovations.

\item[\code{call}] the matched call.

\item[\code{series}] the name of the series \code{x}.

\item[\code{code}] the convergence value returned by \code{\LinkA{optim}{optim}}.

\item[\code{n.cond}] the number of initial observations not used in the fitting.

\item[\code{model}] A list representing the Kalman Filter used in the
fitting.  See \code{\LinkA{KalmanLike}{KalmanLike}}.
\end{ldescription}
\end{Value}
%
\begin{Section}{Fitting methods}
The exact likelihood is computed via a state-space representation of
the ARIMA process, and the innovations and their variance found by a
Kalman filter.  The initialization of the differenced ARMA process uses
stationarity and is based on Gardner \emph{et al.} (1980).  For a
differenced process the non-stationary components are given a diffuse
prior (controlled by \code{kappa}).  Observations which are still
controlled by the diffuse prior (determined by having a Kalman gain of
at least \code{1e4}) are excluded from the likelihood calculations.
(This gives comparable results to \code{\LinkA{arima0}{arima0}} in the absence
of missing values, when the observations excluded are precisely those
dropped by the differencing.)

Missing values are allowed, and are handled exactly in method \code{"ML"}.

If \code{transform.pars} is true, the optimization is done using an
alternative parametrization which is a variation on that suggested by
Jones (1980) and ensures that the model is stationary.  For an AR(p)
model the parametrization is via the inverse tanh of the partial
autocorrelations: the same procedure is applied (separately) to the
AR and seasonal AR terms.  The MA terms are not constrained to be
invertible during optimization, but they will be converted to
invertible form after optimization if \code{transform.pars} is true.

Conditional sum-of-squares is provided mainly for expositional
purposes.  This computes the sum of squares of the fitted innovations
from observation \code{n.cond} on, (where \code{n.cond} is at least
the maximum lag of an AR term), treating all earlier innovations to
be zero.  Argument \code{n.cond} can be used to allow comparability
between different fits.  The `part log-likelihood' is the first
term, half the log of the estimated mean square.  Missing values
are allowed, but will cause many of the innovations to be missing.

When regressors are specified, they are orthogonalized prior to
fitting unless any of the coefficients is fixed.  It can be helpful to
roughly scale the regressors to zero mean and unit variance.
\end{Section}
%
\begin{Note}\relax
The results are likely to be different from S-PLUS's
\code{arima.mle}, which computes a conditional likelihood and does
not include a mean in the model.  Further, the convention used by
\code{arima.mle} reverses the signs of the MA coefficients.

\code{arima} is very similar to \code{\LinkA{arima0}{arima0}} for
ARMA models or for differenced models without missing values,
but handles differenced models with missing values exactly.
It is somewhat slower than \code{arima0}, particularly for seasonally
differenced models.
\end{Note}
%
\begin{References}\relax
Brockwell, P. J. and Davis, R. A. (1996) \emph{Introduction to Time
Series and Forecasting.} Springer, New York. Sections 3.3 and 8.3.

Durbin, J. and Koopman, S. J. (2001) \emph{Time Series Analysis by
State Space Methods.}  Oxford University Press.

Gardner, G, Harvey, A. C. and Phillips, G. D. A. (1980) Algorithm
AS154. An algorithm for exact maximum likelihood estimation of
autoregressive-moving average models by means of Kalman filtering.
\emph{Applied Statistics} \bold{29}, 311--322.

Harvey, A. C. (1993) \emph{Time Series Models},
2nd Edition, Harvester Wheatsheaf, sections 3.3 and 4.4.

Jones, R. H. (1980) Maximum likelihood fitting of ARMA models to time
series with missing observations. \emph{Technometrics} \bold{20} 389--395.

Ripley, B. D. (2002) Time series in \R{} 1.5.0.
\emph{R News}, \bold{2/2}, 2--7.
\url{http://www.r-project.org/doc/Rnews/Rnews_2002-2.pdf}
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{predict.Arima}{predict.Arima}}, \code{\LinkA{arima.sim}{arima.sim}} for simulating
from an ARIMA model, \code{\LinkA{tsdiag}{tsdiag}}, \code{\LinkA{arima0}{arima0}},
\code{\LinkA{ar}{ar}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
arima(lh, order = c(1,0,0))
arima(lh, order = c(3,0,0))
arima(lh, order = c(1,0,1))

arima(lh, order = c(3,0,0), method = "CSS")

arima(USAccDeaths, order = c(0,1,1), seasonal = list(order=c(0,1,1)))
arima(USAccDeaths, order = c(0,1,1), seasonal = list(order=c(0,1,1)),
      method = "CSS") # drops first 13 observations.
# for a model with as few years as this, we want full ML

arima(LakeHuron, order = c(2,0,0), xreg = time(LakeHuron)-1920)

## presidents contains NAs
## graphs in example(acf) suggest order 1 or 3
require(graphics)
(fit1 <- arima(presidents, c(1, 0, 0)))
tsdiag(fit1)
(fit3 <- arima(presidents, c(3, 0, 0)))  # smaller AIC
tsdiag(fit3)
\end{ExampleCode}
\end{Examples}
\HeaderA{arima.sim}{Simulate from an ARIMA Model}{arima.sim}
\keyword{ts}{arima.sim}
%
\begin{Description}\relax
Simulate from an ARIMA model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
arima.sim(model, n, rand.gen = rnorm, innov = rand.gen(n, ...),
          n.start = NA, start.innov = rand.gen(n.start, ...),
          ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] A list with component \code{ar} and/or \code{ma} giving
the AR and MA coefficients respectively.  Optionally a component
\code{order} can be used.  An empty list gives an ARIMA(0, 0, 0)
model, that is white noise.
\item[\code{n}] length of output series, before un-differencing.  A strictly
positive integer.
\item[\code{rand.gen}] optional: a function to generate the innovations.
\item[\code{innov}] an optional times series of innovations.  If not
provided, \code{rand.gen} is used.
\item[\code{n.start}] length of `burn-in' period.  If \code{NA}, the
default, a reasonable value is computed.
\item[\code{start.innov}] an optional times series of innovations to be used
for the burn-in period.  If supplied there must be at least
\code{n.start} values (and \code{n.start} is by default computed
inside the function).
\item[\code{...}] additional arguments for \code{rand.gen}.  Most usefully,
the standard deviation of the innovations generated by \code{rnorm}
can be specified by \code{sd}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
See \code{\LinkA{arima}{arima}} for the precise definition of an ARIMA model.

The ARMA model is checked for stationarity.

ARIMA models are specified via the \code{order} component of
\code{model}, in the same way as for \code{\LinkA{arima}{arima}}.  Other
aspects of the \code{order} component are ignored, but inconsistent
specifications of the MA and AR orders are detected.  The
un-differencing assumes previous values of zero, and to remind the
user of this, those values are returned.

Random inputs for the `burn-in' period are generated by calling
\code{rand.gen}.
\end{Details}
%
\begin{Value}
A time-series object of class \code{"ts"}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{arima}{arima}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

arima.sim(n = 63, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),
          sd = sqrt(0.1796))
# mildly long-tailed
arima.sim(n = 63, list(ar=c(0.8897, -0.4858), ma=c(-0.2279, 0.2488)),
          rand.gen = function(n, ...) sqrt(0.1796) * rt(n, df = 5))

# An ARIMA simulation
ts.sim <- arima.sim(list(order = c(1,1,0), ar = 0.7), n = 200)
ts.plot(ts.sim)
\end{ExampleCode}
\end{Examples}
\HeaderA{arima0}{ARIMA Modelling of Time Series -- Preliminary Version}{arima0}
\aliasA{predict.arima0}{arima0}{predict.arima0}
\aliasA{print.arima0}{arima0}{print.arima0}
\keyword{ts}{arima0}
%
\begin{Description}\relax
Fit an ARIMA model to a univariate time series, and forecast from
the fitted model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
arima0(x, order = c(0, 0, 0),
       seasonal = list(order = c(0, 0, 0), period = NA),
       xreg = NULL, include.mean = TRUE, delta = 0.01,
       transform.pars = TRUE, fixed = NULL, init = NULL,
       method = c("ML", "CSS"), n.cond, optim.control = list())

## S3 method for class 'arima0'
predict(object, n.ahead = 1, newxreg, se.fit = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a univariate time series

\item[\code{order}] A specification of the non-seasonal part of the ARIMA
model: the three components \eqn{(p, d, q)}{} are the AR order, the
degree of differencing, and the MA order.

\item[\code{seasonal}] A specification of the seasonal part of the ARIMA
model, plus the period (which defaults to \code{frequency(x)}).
This should be a list with components \code{order} and
\code{period}, but a specification of just a numeric vector of
length 3 will be turned into a suitable list with the specification
as the \code{order}.

\item[\code{xreg}] Optionally, a vector or matrix of external regressors,
which must have the same number of rows as \code{x}.

\item[\code{include.mean}] Should the ARIMA model include
a mean term? The default is \code{TRUE} for undifferenced series,
\code{FALSE} for differenced ones (where a mean would not affect
the fit nor predictions).

\item[\code{delta}] A value to indicate at which point `fast
recursions' should be used.  See the `Details' section.

\item[\code{transform.pars}] Logical.  If true, the AR parameters are
transformed to ensure that they remain in the region of
stationarity.  Not used for \code{method = "CSS"}.

\item[\code{fixed}] optional numeric vector of the same length as the total
number of parameters.  If supplied, only \code{NA} entries in
\code{fixed} will be varied.  \code{transform.pars = TRUE}
will be overridden (with a warning) if any ARMA parameters are
fixed.

\item[\code{init}] optional numeric vector of initial parameter
values.  Missing values will be filled in, by zeroes except for
regression coefficients.  Values already specified in \code{fixed}
will be ignored.

\item[\code{method}] Fitting method: maximum likelihood or minimize
conditional sum-of-squares.

\item[\code{n.cond}] Only used if fitting by conditional-sum-of-squares: the
number of initial observations to ignore.  It will be ignored if
less than the maximum lag of an AR term.

\item[\code{optim.control}] List of control parameters for \code{\LinkA{optim}{optim}}.

\item[\code{object}] The result of an \code{arima0} fit.

\item[\code{newxreg}] New values of \code{xreg} to be used for
prediction. Must have at least \code{n.ahead} rows.

\item[\code{n.ahead}] The number of steps ahead for which prediction is required.

\item[\code{se.fit}] Logical: should standard errors of prediction be returned?

\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Different definitions of ARMA models have different signs for the
AR and/or MA coefficients. The definition here has

\deqn{X_t = a_1X_{t-1} + \cdots + a_pX_{t-p} + e_t + b_1e_{t-1} + \dots + b_qe_{t-q}}{}

and so the MA coefficients differ in sign from those of
S-PLUS.  Further, if \code{include.mean} is true, this formula
applies to \eqn{X-m}{} rather than \eqn{X}{}.  For ARIMA models with
differencing, the differenced series follows a zero-mean ARMA model.

The variance matrix of the estimates is found from the Hessian of
the log-likelihood, and so may only be a rough guide, especially for
fits close to the boundary of invertibility.

Optimization is done by \code{\LinkA{optim}{optim}}. It will work
best if the columns in \code{xreg} are roughly scaled to zero mean
and unit variance, but does attempt to estimate suitable scalings.

Finite-history prediction is used. This is only statistically
efficient if the MA part of the fit is invertible, so
\code{predict.arima0} will give a warning for non-invertible MA
models.
\end{Details}
%
\begin{Value}
For \code{arima0}, a list of class \code{"arima0"} with components:

\begin{ldescription}
\item[\code{coef}] a vector of AR, MA and regression coefficients,

\item[\code{sigma2}] the MLE of the innovations variance.

\item[\code{var.coef}] the estimated variance matrix of the coefficients
\code{coef}.

\item[\code{loglik}] the maximized log-likelihood (of the differenced data),
or the approximation to it used.

\item[\code{arma}] A compact form of the specification, as a vector giving
the number of AR, MA, seasonal AR and seasonal MA coefficients,
plus the period and the number of non-seasonal and seasonal
differences.

\item[\code{aic}] the AIC value corresponding to the log-likelihood. Only
valid for \code{method = "ML"} fits.

\item[\code{residuals}] the fitted innovations.

\item[\code{call}] the matched call.

\item[\code{series}] the name of the series \code{x}.

\item[\code{convergence}] the value returned by \code{\LinkA{optim}{optim}}.

\item[\code{n.cond}] the number of initial observations not used in the fitting.

\end{ldescription}
For \code{predict.arima0}, a time series of predictions, or if
\code{se.fit = TRUE}, a list with components \code{pred}, the
predictions, and \code{se}, the estimated standard errors. Both
components are time series.
\end{Value}
%
\begin{Section}{Fitting methods}
The exact likelihood is computed via a state-space representation of
the ARMA process, and the innovations and their variance found by a
Kalman filter based on Gardner \emph{et al.} (1980).  This has
the option to switch to `fast recursions' (assume an
effectively infinite past) if the innovations variance is close
enough to its asymptotic bound. The argument \code{delta} sets the
tolerance: at its default value the approximation is normally
negligible and the speed-up considerable.  Exact computations can be
ensured by setting \code{delta} to a negative value.

If \code{transform.pars} is true, the optimization is done using an
alternative parametrization which is a variation on that suggested by
Jones (1980) and ensures that the model is stationary.  For an AR(p)
model the parametrization is via the inverse tanh of the partial
autocorrelations: the same procedure is applied (separately) to the
AR and seasonal AR terms.  The MA terms are also constrained to be
invertible during optimization by the same transformation if
\code{transform.pars} is true.  Note that the MLE for MA terms does
sometimes occur for MA polynomials with unit roots: such models can be
fitted by using \code{transform.pars = FALSE} and specifying a good
set of initial values (often obtainable from a fit with
\code{transform.pars = TRUE}).

Missing values are allowed, but any missing values
will force \code{delta} to be ignored and full recursions used.
Note that missing values will be propagated by differencing, so the
procedure used in this function is not fully efficient in that case.

Conditional sum-of-squares is provided mainly for expositional
purposes.  This computes the sum of squares of the fitted innovations
from observation
\code{n.cond} on, (where \code{n.cond} is at least the maximum lag of
an AR term), treating all earlier innovations to be zero.  Argument
\code{n.cond} can be used to allow comparability between different
fits.  The `part log-likelihood' is the first term, half the
log of the estimated mean square.  Missing values are allowed, but
will cause many of the innovations to be missing.

When regressors are specified, they are orthogonalized prior to
fitting unless any of the coefficients is fixed.  It can be helpful to
roughly scale the regressors to zero mean and unit variance.
\end{Section}
%
\begin{Note}\relax
This is a preliminary version, and will be replaced by \code{\LinkA{arima}{arima}}.

The standard errors of prediction exclude the uncertainty in the
estimation of the ARMA model and the regression coefficients.

The results are likely to be different from S-PLUS's
\code{arima.mle}, which computes a conditional likelihood and does
not include a mean in the model.  Further, the convention used by
\code{arima.mle} reverses the signs of the MA coefficients.
\end{Note}
%
\begin{References}\relax
Brockwell, P. J. and Davis, R. A. (1996) \emph{Introduction to Time
Series and Forecasting.} Springer, New York. Sections 3.3 and 8.3.

Gardner, G, Harvey, A. C. and Phillips, G. D. A. (1980) Algorithm
AS154. An algorithm for exact maximum likelihood estimation of
autoregressive-moving average models by means of Kalman filtering.
\emph{Applied Statistics} \bold{29}, 311--322.

Harvey, A. C. (1993) \emph{Time Series Models},
2nd Edition, Harvester Wheatsheaf, sections 3.3 and  4.4.

Harvey, A. C. and McKenzie, C. R. (1982) Algorithm AS182.
An algorithm for finite sample prediction from ARIMA processes.
\emph{Applied Statistics} \bold{31}, 180--187.

Jones, R. H. (1980) Maximum likelihood fitting of ARMA models to time
series with missing observations. \emph{Technometrics} \bold{20} 389--395.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{arima}{arima}}, \code{\LinkA{ar}{ar}}, \code{\LinkA{tsdiag}{tsdiag}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: arima0(lh, order = c(1,0,0))
arima0(lh, order = c(3,0,0))
arima0(lh, order = c(1,0,1))
predict(arima0(lh, order = c(3,0,0)), n.ahead = 12)

arima0(lh, order = c(3,0,0), method = "CSS")

# for a model with as few years as this, we want full ML
(fit <- arima0(USAccDeaths, order = c(0,1,1),
               seasonal = list(order=c(0,1,1)), delta = -1))
predict(fit, n.ahead = 6)

arima0(LakeHuron, order = c(2,0,0), xreg = time(LakeHuron)-1920)
## Not run: 
## presidents contains NAs
## graphs in example(acf) suggest order 1 or 3
(fit1 <- arima0(presidents, c(1, 0, 0), delta = -1))  # avoid warning
tsdiag(fit1)
(fit3 <- arima0(presidents, c(3, 0, 0), delta = -1))  # smaller AIC
tsdiag(fit3)
## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{ARMAacf}{Compute Theoretical ACF for an ARMA Process}{ARMAacf}
\keyword{ts}{ARMAacf}
%
\begin{Description}\relax
Compute the theoretical autocorrelation function or partial
autocorrelation function for an ARMA process.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ARMAacf(ar = numeric(), ma = numeric(), lag.max = r, pacf = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{ar}] numeric vector of AR coefficients
\item[\code{ma}] numeric vector of MA coefficients
\item[\code{lag.max}] integer.  Maximum lag required.  Defaults to
\code{max(p, q+1)}, where \code{p, q} are the numbers of AR and MA
terms respectively.
\item[\code{pacf}] logical.  Should the partial autocorrelations be returned?
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The methods used follow Brockwell \& Davis (1991, section 3.3).  Their
equations (3.3.8) are solved for the autocovariances at lags
\eqn{0, \dots, \max(p, q+1)}{},
and the remaining autocorrelations are given by a recursive filter.
\end{Details}
%
\begin{Value}
A vector of (partial) autocorrelations, named by the lags.
\end{Value}
%
\begin{References}\relax
Brockwell, P. J. and Davis, R. A. (1991) \emph{Time Series: Theory and
Methods}, Second Edition.  Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{arima}{arima}}, \code{\LinkA{ARMAtoMA}{ARMAtoMA}},
\code{\LinkA{acf2AR}{acf2AR}} for inverting part of \code{ARMAacf}; further
\code{\LinkA{filter}{filter}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ARMAacf(c(1.0, -0.25), 1.0, lag.max = 10)

## Example from Brockwell & Davis (1991, pp.92-4)
## answer 2^(-n) * (32/3 + 8 * n) /(32/3)
n <- 1:10; 2^(-n) * (32/3 + 8 * n) /(32/3)
ARMAacf(c(1.0, -0.25), 1.0, lag.max = 10, pacf = TRUE)
zapsmall(ARMAacf(c(1.0, -0.25), lag.max = 10, pacf = TRUE))

## Cov-Matrix of length-7 sub-sample of AR(1) example:
toeplitz(ARMAacf(0.8, lag.max = 7))
\end{ExampleCode}
\end{Examples}
\HeaderA{ARMAtoMA}{Convert ARMA Process to Infinite MA Process}{ARMAtoMA}
\keyword{ts}{ARMAtoMA}
%
\begin{Description}\relax
Convert ARMA process to infinite MA process.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ARMAtoMA(ar = numeric(), ma = numeric(), lag.max)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{ar}] numeric vector of AR coefficients
\item[\code{ma}] numeric vector of MA coefficients
\item[\code{lag.max}] Largest MA(Inf) coefficient required.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A vector of coefficients.
\end{Value}
%
\begin{References}\relax
Brockwell, P. J. and Davis, R. A. (1991) \emph{Time Series: Theory and
Methods}, Second Edition.  Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{arima}{arima}}, \code{\LinkA{ARMAacf}{ARMAacf}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ARMAtoMA(c(1.0, -0.25), 1.0, 10)
## Example from Brockwell & Davis (1991, p.92)
## answer (1 + 3*n)*2^(-n)
n <- 1:10; (1 + 3*n)*2^(-n)
\end{ExampleCode}
\end{Examples}
\HeaderA{as.hclust}{Convert Objects to Class hclust}{as.hclust}
\methaliasA{as.hclust.default}{as.hclust}{as.hclust.default}
\methaliasA{as.hclust.twins}{as.hclust}{as.hclust.twins}
\keyword{multivariate}{as.hclust}
\keyword{cluster}{as.hclust}
%
\begin{Description}\relax
Converts objects from other hierarchical clustering functions to
class \code{"hclust"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
as.hclust(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] Hierarchical clustering object
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Currently there is only support for converting objects of
class \code{"twins"} as produced by the functions \code{diana} and
\code{agnes} from the package \Rhref{http://CRAN.R-project.org/package=cluster}{\pkg{cluster}}.  The default method
throws an error unless passed an \code{"hclust"} object.
\end{Details}
%
\begin{Value}
An object of class \code{"hclust"}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{hclust}{hclust}}, and from package \Rhref{http://CRAN.R-project.org/package=cluster}{\pkg{cluster}},
\code{\LinkA{diana}{diana}} and \code{\LinkA{agnes}{agnes}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- matrix(rnorm(30), ncol=3)
hc <- hclust(dist(x), method="complete")

if(require("cluster", quietly=TRUE)) {# is a recommended package
  ag <- agnes(x, method="complete")
  hcag <- as.hclust(ag)
  ## The dendrograms order slightly differently:
  op <- par(mfrow=c(1,2))
  plot(hc) ;  mtext("hclust", side=1)
  plot(hcag); mtext("agnes",  side=1)
}
\end{ExampleCode}
\end{Examples}
\HeaderA{asOneSidedFormula}{Convert to One-Sided Formula}{asOneSidedFormula}
\keyword{models}{asOneSidedFormula}
%
\begin{Description}\relax
Names, expressions, numeric values, and character strings are converted to
one-sided formulae. If \code{object} is a formula, it must be
one-sided, in which case it is returned unaltered.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
asOneSidedFormula(object)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a one-sided formula, an expression, a numeric value, or a
character string.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a one-sided formula representing \code{object}
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{formula}{formula}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
asOneSidedFormula("age")
asOneSidedFormula(~ age)
\end{ExampleCode}
\end{Examples}
\HeaderA{ave}{Group Averages Over Level Combinations of Factors}{ave}
\keyword{univar}{ave}
%
\begin{Description}\relax
Subsets of \code{x[]} are averaged, where each subset consist of those
observations with the same factor levels.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ave(x, ..., FUN = mean)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A numeric.
\item[\code{...}] Grouping variables, typically factors, all of the same
\code{length} as \code{x}.
\item[\code{FUN}] Function to apply for each factor level combination.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric vector, say \code{y} of length \code{length(x)}.
If \code{...} is \code{g1,g2}, e.g.,
\code{y[i]} is equal to \code{FUN(x[j]}, for all \code{j} with
\code{g1[j] == g1[i]} and \code{g2[j] == g2[i])}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{mean}{mean}}, \code{\LinkA{median}{median}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

ave(1:3)# no grouping -> grand mean

attach(warpbreaks)
ave(breaks, wool)
ave(breaks, tension)
ave(breaks, tension, FUN = function(x)mean(x, trim=.1))
plot(breaks, main =
     "ave( Warpbreaks )  for   wool  x  tension  combinations")
lines(ave(breaks, wool, tension            ), type='s', col = "blue")
lines(ave(breaks, wool, tension, FUN=median), type='s', col = "green")
legend(40,70, c("mean","median"), lty=1,col=c("blue","green"), bg="gray90")
detach()
\end{ExampleCode}
\end{Examples}
\HeaderA{bandwidth}{Bandwidth Selectors for Kernel Density Estimation}{bandwidth}
\aliasA{bw.bcv}{bandwidth}{bw.bcv}
\aliasA{bw.nrd}{bandwidth}{bw.nrd}
\aliasA{bw.nrd0}{bandwidth}{bw.nrd0}
\aliasA{bw.SJ}{bandwidth}{bw.SJ}
\aliasA{bw.ucv}{bandwidth}{bw.ucv}
\keyword{distribution}{bandwidth}
\keyword{smooth}{bandwidth}
%
\begin{Description}\relax
Bandwidth selectors for Gaussian kernels in \code{\LinkA{density}{density}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
bw.nrd0(x)

bw.nrd(x)

bw.ucv(x, nb = 1000, lower = 0.1 * hmax, upper = hmax, tol = 0.1 * lower)

bw.bcv(x, nb = 1000, lower = 0.1 * hmax, upper = hmax, tol = 0.1 * lower)

bw.SJ(x, nb = 1000, lower = 0.1 * hmax, upper = hmax,
      method = c("ste", "dpi"), tol = 0.1 * lower)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric vector.
\item[\code{nb}] number of bins to use.
\item[\code{lower, upper}] range over which to minimize.  The default is
almost always satisfactory.  \code{hmax} is calculated internally
from a normal reference bandwidth.
\item[\code{method}] either \code{"ste"} ("solve-the-equation") or
\code{"dpi"} ("direct plug-in").
\item[\code{tol}] for method \code{"ste"}, the convergence tolerance for
\code{\LinkA{uniroot}{uniroot}}.  The default leads to bandwidth estimates
with only slightly more than one digit accuracy, which is sufficient
for practical density estimation, but possibly not for theoretical
simulation studies.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{bw.nrd0} implements a rule-of-thumb for
choosing the bandwidth of a Gaussian kernel density estimator.
It defaults to 0.9 times the
minimum of the standard deviation and the interquartile range divided by
1.34 times the sample size to the negative one-fifth power
(= Silverman's `rule of thumb', Silverman (1986, page 48, eqn (3.31))
\emph{unless} the quartiles coincide when a positive result
will be guaranteed.

\code{bw.nrd} is the more common variation given by Scott (1992),
using factor 1.06.

\code{bw.ucv} and \code{bw.bcv} implement unbiased and
biased cross-validation respectively.

\code{bw.SJ} implements the methods of Sheather \& Jones (1991)
to select the bandwidth using pilot estimation of derivatives.\\{}
The algorithm for method \code{"ste"} solves an equation (via
\code{\LinkA{uniroot}{uniroot}}) and because of that, enlarges the interval
\code{c(lower,upper)} when the boundaries were not user-specified and
do not bracket the root.
\end{Details}
%
\begin{Value}
A bandwidth on a scale suitable for the \code{bw} argument
of \code{density}.
\end{Value}
%
\begin{References}\relax
Scott, D. W. (1992)
\emph{Multivariate Density Estimation: Theory, Practice, and
Visualization.}
Wiley.

Sheather, S. J. and Jones, M. C. (1991)
A reliable data-based bandwidth selection method for kernel density
estimation.
\emph{Journal of the Royal Statistical Society series B},
\bold{53}, 683--690.

Silverman, B. W. (1986)
\emph{Density Estimation}.
London: Chapman and Hall.

Venables, W. N. and Ripley, B. D. (2002)
\emph{Modern Applied Statistics with S}.
Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{density}{density}}.

\code{\LinkA{bandwidth.nrd}{bandwidth.nrd}}, \code{\LinkA{ucv}{ucv}},
\code{\LinkA{bcv}{bcv}} and \code{\LinkA{width.SJ}{width.SJ}} in
package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}, which are all scaled to the \code{width} argument
of \code{density} and so give answers four times as large.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

plot(density(precip, n = 1000))
rug(precip)
lines(density(precip, bw="nrd"), col = 2)
lines(density(precip, bw="ucv"), col = 3)
lines(density(precip, bw="bcv"), col = 4)
lines(density(precip, bw="SJ-ste"), col = 5)
lines(density(precip, bw="SJ-dpi"), col = 6)
legend(55, 0.035,
       legend = c("nrd0", "nrd", "ucv", "bcv", "SJ-ste", "SJ-dpi"),
       col = 1:6, lty = 1)
\end{ExampleCode}
\end{Examples}
\HeaderA{bartlett.test}{Bartlett Test of Homogeneity of Variances}{bartlett.test}
\methaliasA{bartlett.test.default}{bartlett.test}{bartlett.test.default}
\methaliasA{bartlett.test.formula}{bartlett.test}{bartlett.test.formula}
\keyword{htest}{bartlett.test}
%
\begin{Description}\relax
Performs Bartlett's test of the null that the variances in each of the
groups (samples) are the same.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
bartlett.test(x, ...)

## Default S3 method:
bartlett.test(x, g, ...)

## S3 method for class 'formula'
bartlett.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector of data values, or a list of numeric data
vectors representing the respective samples, or fitted linear model
objects (inheriting from class \code{"lm"}).
\item[\code{g}] a vector or factor object giving the group for the
corresponding elements of \code{x}.
Ignored if \code{x} is a list.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
gives the data values and \code{rhs} the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{x} is a list, its elements are taken as the samples or fitted
linear models to be compared for homogeneity of variances.  In this
case, the elements must either all be numeric data vectors or fitted
linear model objects, \code{g} is ignored, and one can simply use
\code{bartlett.test(x)} to perform the test.  If the samples are not
yet contained in a list, use \code{bartlett.test(list(x, ...))}.

Otherwise, \code{x} must be a numeric data vector, and \code{g} must
be a vector or factor object of the same length as \code{x} giving the
group for the corresponding elements of \code{x}.
\end{Details}
%
\begin{Value}
A list of class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] Bartlett's K-squared test statistic.
\item[\code{parameter}] the degrees of freedom of the approximate chi-squared
distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] the character string
\code{"Bartlett test of homogeneity of variances"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Bartlett, M. S. (1937).
Properties of sufficiency and statistical tests.
\emph{Proceedings of the Royal Society of London Series A}
\bold{160}, 268--282.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{var.test}{var.test}} for the special case of comparing variances in
two samples from normal distributions;
\code{\LinkA{fligner.test}{fligner.test}} for a rank-based (nonparametric)
\eqn{k}{}-sample test for homogeneity of variances;
\code{\LinkA{ansari.test}{ansari.test}} and \code{\LinkA{mood.test}{mood.test}} for two rank
based two-sample tests for difference in scale.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

plot(count ~ spray, data = InsectSprays)
bartlett.test(InsectSprays$count, InsectSprays$spray)
bartlett.test(count ~ spray, data = InsectSprays)
\end{ExampleCode}
\end{Examples}
\HeaderA{Beta}{The Beta Distribution}{Beta}
\aliasA{dbeta}{Beta}{dbeta}
\aliasA{pbeta}{Beta}{pbeta}
\aliasA{qbeta}{Beta}{qbeta}
\aliasA{rbeta}{Beta}{rbeta}
\keyword{distribution}{Beta}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the Beta distribution with parameters \code{shape1} and
\code{shape2} (and optional non-centrality parameter \code{ncp}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dbeta(x, shape1, shape2, ncp = 0, log = FALSE)
pbeta(q, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE)
qbeta(p, shape1, shape2, ncp = 0, lower.tail = TRUE, log.p = FALSE)
rbeta(n, shape1, shape2, ncp = 0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{shape1, shape2}] positive parameters of the Beta distribution.
\item[\code{ncp}] non-centrality parameter.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The Beta distribution with parameters \code{shape1} \eqn{= a}{} and
\code{shape2} \eqn{= b}{} has density
\deqn{f(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}{x}^{a-1} {(1-x)}^{b-1}%
  }{}
for \eqn{a > 0}{}, \eqn{b > 0}{} and \eqn{0 \le x \le 1}{}
where the boundary values at \eqn{x=0}{} or \eqn{x=1}{} are defined as
by continuity (as limits).
\\{}
The mean is \eqn{a/(a+b)}{} and the variance is \eqn{ab/((a+b)^2 (a+b+1))}{}.

\code{pbeta} is closely related to the incomplete beta function.  As
defined by Abramowitz and Stegun 6.6.1
\deqn{B_x(a,b) = \int_0^x t^{a-1} (1-t)^{b-1} dt,}{}
and 6.6.2 \eqn{I_x(a,b) = B_x(a,b) / B(a,b)}{} where
\eqn{B(a,b) = B_1(a,b)}{} is the Beta function (\code{\LinkA{beta}{beta}}).

\eqn{I_x(a,b)}{} is \code{pbeta(x,a,b)}.

The noncentral Beta distribution (with \code{ncp} \eqn{ = \lambda}{})
is defined (Johnson et al, 1995, pp. 502) as the distribution of
\eqn{X/(X+Y)}{} where \eqn{X \sim \chi^2_{2a}(\lambda)}{}
and \eqn{Y \sim \chi^2_{2b}}{}.
\end{Details}
%
\begin{Value}
\code{dbeta} gives the density, \code{pbeta} the distribution
function, \code{qbeta} the quantile function, and \code{rbeta}
generates random deviates.

Invalid arguments will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Note}\relax
Supplying \code{ncp = 0} uses the algorithm for the non-central
distribution, which is not the same algorithm used if \code{ncp} is
omitted.  This is to give consistent behaviour in extreme cases with
values of \code{ncp} very near zero.
\end{Note}
%
\begin{Source}\relax
The central \code{dbeta} is based on a binomial probability, using code
contributed by Catherine Loader (see \code{\LinkA{dbinom}{dbinom}}) if either
shape parameter is larger than one, otherwise directly from the definition.
The non-central case is based on the derivation as a Poisson
mixture of betas (Johnson \emph{et al}, 1995, pp. 502--3).

The central \code{pbeta} uses a C translation (and enhancement for
\code{log\_p=TRUE}) of

Didonato, A. and Morris, A., Jr, (1992)
Algorithm 708: Significant digit computation of the incomplete beta
function ratios,
\emph{ACM Transactions on Mathematical Software}, \bold{18}, 360--373.
(See also\\{}
Brown, B. and Lawrence Levy, L. (1994)
Certification of algorithm 708: Significant digit computation of the
incomplete beta,
\emph{ACM Transactions on Mathematical Software}, \bold{20}, 393--397.)

The non-central \code{pbeta} uses a C translation of

Lenth, R. V. (1987) Algorithm AS226: Computing noncentral beta
probabilities. \emph{Appl. Statist}, \bold{36}, 241--244,
incorporating\\{}
Frick, H. (1990)'s AS R84, \emph{Appl. Statist}, \bold{39}, 311--2,
and\\{}
Lam, M.L. (1995)'s AS R95, \emph{Appl. Statist}, \bold{44}, 551--2.

This computes the lower tail only, so the upper tail suffers from
cancellation and a warning will be given when this is likely to be
significant.

The central case of \code{qbeta} is based on a C translation of

Cran, G. W., K. J. Martin and G. E. Thomas (1977).
Remark AS R19 and Algorithm AS 109,
\emph{Applied Statistics},  \bold{26}, 111--114,
and subsequent remarks (AS83 and correction).

The central case of \code{rbeta} is based on a C translation of

R. C. H. Cheng (1978).
Generating beta variates with nonintegral shape parameters.
\emph{Communications of the ACM}, \bold{21}, 317--322.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Abramowitz, M. and Stegun, I. A. (1972)
\emph{Handbook of Mathematical Functions.} New York: Dover.
Chapter 6: Gamma and Related Functions.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 2, especially
chapter 25. Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions.

\code{\LinkA{beta}{beta}} for the Beta function.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- seq(0, 1, length=21)
dbeta(x, 1, 1)
pbeta(x, 1, 1)
\end{ExampleCode}
\end{Examples}
\HeaderA{binom.test}{Exact Binomial Test}{binom.test}
\keyword{htest}{binom.test}
%
\begin{Description}\relax
Performs an exact test of a simple null hypothesis about the
probability of success in a Bernoulli experiment.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
binom.test(x, n, p = 0.5,
           alternative = c("two.sided", "less", "greater"),
           conf.level = 0.95)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] number of successes, or a vector of length 2 giving the
numbers of successes and failures, respectively.
\item[\code{n}] number of trials; ignored if \code{x} has length 2.
\item[\code{p}] hypothesized probability of success.
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"}, \code{"greater"} or \code{"less"}.
You can specify just the initial letter.
\item[\code{conf.level}] confidence level for the returned confidence
interval.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Confidence intervals are obtained by a procedure first given in
Clopper and Pearson (1934).  This guarantees that the confidence level
is at least \code{conf.level}, but in general does not give the
shortest-length confidence intervals.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the number of successes.
\item[\code{parameter}] the number of trials.
\item[\code{p.value}] the p-value of the test.
\item[\code{conf.int}] a confidence interval for the probability of success.
\item[\code{estimate}] the estimated probability of success.
\item[\code{null.value}] the probability of success under the null,
\code{p}.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] the character string \code{"Exact binomial test"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Clopper, C. J. \& Pearson, E. S. (1934).
The use of confidence or fiducial limits illustrated in the case of
the binomial.
\emph{Biometrika}, \bold{26}, 404--413.

William J. Conover (1971),
\emph{Practical nonparametric statistics}.
New York: John Wiley \& Sons.
Pages 97--104.

Myles Hollander \& Douglas A. Wolfe (1973),
\emph{Nonparametric Statistical Methods.}
New York: John Wiley \& Sons.
Pages 15--22.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{prop.test}{prop.test}} for a general (approximate) test for equal or
given proportions.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Conover (1971), p. 97f.
## Under (the assumption of) simple Mendelian inheritance, a cross
##  between plants of two particular genotypes produces progeny 1/4 of
##  which are "dwarf" and 3/4 of which are "giant", respectively.
##  In an experiment to determine if this assumption is reasonable, a
##  cross results in progeny having 243 dwarf and 682 giant plants.
##  If "giant" is taken as success, the null hypothesis is that p =
##  3/4 and the alternative that p != 3/4.
binom.test(c(682, 243), p = 3/4)
binom.test(682, 682 + 243, p = 3/4)   # The same.
## => Data are in agreement with the null hypothesis.
\end{ExampleCode}
\end{Examples}
\HeaderA{Binomial}{The Binomial Distribution}{Binomial}
\aliasA{dbinom}{Binomial}{dbinom}
\aliasA{pbinom}{Binomial}{pbinom}
\aliasA{qbinom}{Binomial}{qbinom}
\aliasA{rbinom}{Binomial}{rbinom}
\keyword{distribution}{Binomial}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the binomial distribution with parameters \code{size}
and \code{prob}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dbinom(x, size, prob, log = FALSE)
pbinom(q, size, prob, lower.tail = TRUE, log.p = FALSE)
qbinom(p, size, prob, lower.tail = TRUE, log.p = FALSE)
rbinom(n, size, prob)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{size}] number of trials (zero or more).
\item[\code{prob}] probability of success on each trial.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The binomial distribution with \code{size} \eqn{= n}{} and
\code{prob} \eqn{= p}{} has density
\deqn{p(x) = {n \choose x} {p}^{x} {(1-p)}^{n-x}}{}
for \eqn{x = 0, \ldots, n}{}.
Note that binomial \emph{coefficients} can be computed by
\code{\LinkA{choose}{choose}} in \R{}.

If an element of \code{x} is not integer, the result of \code{dbinom}
is zero, with a warning.
\eqn{p(x)}{} is computed using Loader's algorithm, see the reference below.

The quantile is defined as the smallest value \eqn{x}{} such that
\eqn{F(x) \ge p}{}, where \eqn{F}{} is the distribution function.
\end{Details}
%
\begin{Value}
\code{dbinom} gives the density, \code{pbinom} gives the distribution
function, \code{qbinom} gives the quantile function and \code{rbinom}
generates random deviates.

If \code{size} is not an integer, \code{NaN} is returned.
\end{Value}
%
\begin{Source}\relax
For \code{dbinom} a saddle-point expansion is used: see

Catherine Loader (2000). \emph{Fast and Accurate Computation of
Binomial Probabilities}; available from
\url{http://www.herine.net/stat/software/dbinom.html}.

\code{pbinom} uses \code{\LinkA{pbeta}{pbeta}}.

\code{qbinom} uses the Cornish--Fisher Expansion to include a skewness
correction to a normal approximation, followed by a search.

\code{rbinom} (for \code{size < .Machine\$integer.max}) is based on

Kachitvichyanukul, V. and Schmeiser, B. W. (1988)
Binomial random variate generation.
\emph{Communications of the ACM}, \bold{31}, 216--222.

For larger values it uses inversion.
\end{Source}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dnbinom}{dnbinom}} for the negative binomial, and
\code{\LinkA{dpois}{dpois}} for the Poisson distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)
# Compute P(45 < X < 55) for X Binomial(100,0.5)
sum(dbinom(46:54, 100, 0.5))

## Using "log = TRUE" for an extended range :
n <- 2000
k <- seq(0, n, by = 20)
plot (k, dbinom(k, n, pi/10, log=TRUE), type='l', ylab="log density",
      main = "dbinom(*, log=TRUE) is better than  log(dbinom(*))")
lines(k, log(dbinom(k, n, pi/10)), col='red', lwd=2)
## extreme points are omitted since dbinom gives 0.
mtext("dbinom(k, log=TRUE)", adj=0)
mtext("extended range", adj=0, line = -1, font=4)
mtext("log(dbinom(k))", col="red", adj=1)
\end{ExampleCode}
\end{Examples}
\HeaderA{biplot}{Biplot of Multivariate Data}{biplot}
\methaliasA{biplot.default}{biplot}{biplot.default}
\keyword{hplot}{biplot}
\keyword{multivariate}{biplot}
%
\begin{Description}\relax
Plot a biplot on the current graphics device.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
biplot(x, ...)

## Default S3 method:
biplot(x, y, var.axes = TRUE, col, cex = rep(par("cex"), 2),
       xlabs = NULL, ylabs = NULL, expand = 1,
       xlim  = NULL, ylim  = NULL, arrow.len = 0.1,
       main = NULL, sub = NULL, xlab = NULL, ylab = NULL, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] The \code{biplot}, a fitted object. For \code{biplot.default},
the first set of points (a two-column matrix), usually associated
with observations.
\item[\code{y}] The second set of points (a two-column matrix), usually associated
with variables.
\item[\code{var.axes}] If \code{TRUE} the second set of points have arrows
representing them as (unscaled) axes.
\item[\code{col}] A vector of length 2 giving the colours for the first and
second set of points respectively (and the corresponding axes). If a
single colour is specified it will be used for both sets.  If
missing the default colour is looked for in the
\code{\LinkA{palette}{palette}}: if there it and the next colour as used,
otherwise the first two colours of the palette are used.
\item[\code{cex}] The character expansion factor used for labelling the
points. The labels can be of different sizes for the two sets by
supplying a vector of length two.
\item[\code{xlabs}] A vector of character strings to label the first set of
points: the default is to use the row dimname of \code{x}, or
\code{1:n} if the dimname is \code{NULL}.
\item[\code{ylabs}] A vector of character strings to label the second set of
points: the default is to use the row dimname of \code{y}, or
\code{1:n} if the dimname is \code{NULL}.
\item[\code{expand}] An expansion factor to apply when plotting the second set
of points relative to the first. This can be used to tweak the
scaling of the two sets to a physically comparable scale.
\item[\code{arrow.len}] The length of the arrow heads on the axes plotted in
\code{var.axes} is true. The arrow head can be suppressed by
\code{arrow.len = 0}.
\item[\code{xlim, ylim}] Limits for the x and y axes in the units of the
first set of variables.
\item[\code{main, sub, xlab, ylab, ...}] graphical parameters.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
A biplot is plot which aims to represent both the observations and
variables of a matrix of multivariate data on the same plot. There are
many variations on biplots (see the references) and perhaps the most
widely used one is implemented by \code{\LinkA{biplot.princomp}{biplot.princomp}}.
The function \code{biplot.default} merely provides the
underlying code to plot two sets of variables on the same figure.

Graphical parameters can also be given to \code{biplot}: the size of
\code{xlabs} and \code{ylabs} is controlled by \code{cex}.
\end{Details}
%
\begin{Section}{Side Effects}
a plot is produced on the current graphics device.
\end{Section}
%
\begin{References}\relax
K. R. Gabriel (1971).  The biplot graphical display of matrices with
application to principal component analysis. \emph{Biometrika}
\bold{58}, 453--467.

J.C. Gower and D. J. Hand (1996). \emph{Biplots}. Chapman \& Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{biplot.princomp}{biplot.princomp}}, also for examples.
\end{SeeAlso}
\HeaderA{biplot.princomp}{Biplot for Principal Components}{biplot.princomp}
\aliasA{biplot.prcomp}{biplot.princomp}{biplot.prcomp}
\keyword{multivariate}{biplot.princomp}
\keyword{hplot}{biplot.princomp}
%
\begin{Description}\relax
Produces a biplot (in the strict sense) from the output of
\code{\LinkA{princomp}{princomp}} or \code{\LinkA{prcomp}{prcomp}}
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'prcomp'
biplot(x, choices = 1:2, scale = 1, pc.biplot = FALSE, ...)

## S3 method for class 'princomp'
biplot(x, choices = 1:2, scale = 1, pc.biplot = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object of class \code{"princomp"}.
\item[\code{choices}] 
length 2 vector specifying the components to plot. Only the default
is a biplot in the strict sense.

\item[\code{scale}] 
The variables are scaled by \code{lambda \textasciicircum{} scale} and the
observations are scaled by \code{lambda \textasciicircum{} (1-scale)} where
\code{lambda} are the singular values as computed by
\code{\LinkA{princomp}{princomp}}. Normally \code{0 <= scale <= 1}, and a warning 
will be issued if the specified \code{scale} is outside this range.

\item[\code{pc.biplot}] 
If true, use what Gabriel (1971) refers to as a "principal component
biplot", with \code{lambda = 1} and observations scaled up by sqrt(n) and
variables scaled down by sqrt(n).  Then inner products between
variables approximate covariances and distances between observations
approximate Mahalanobis distance.

\item[\code{...}] optional arguments to be passed to
\code{\LinkA{biplot.default}{biplot.default}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a method for the generic function \code{biplot}.  There is
considerable confusion over the precise definitions: those of the
original paper, Gabriel (1971), are followed here.  Gabriel and
Odoroff (1990) use the same definitions, but their plots actually
correspond to \code{pc.biplot = TRUE}.
\end{Details}
%
\begin{Section}{Side Effects}
a plot is produced on the current graphics device.
\end{Section}
%
\begin{References}\relax
Gabriel, K. R. (1971).
The biplot graphical display of matrices with applications to
principal component analysis.
\emph{Biometrika}, \bold{58}, 453--467.

Gabriel, K. R. and Odoroff, C. L. (1990).
Biplots in biomedical research.
\emph{Statistics in Medicine}, \bold{9}, 469--485.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{biplot}{biplot}},
\code{\LinkA{princomp}{princomp}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)
biplot(princomp(USArrests))
\end{ExampleCode}
\end{Examples}
\HeaderA{birthday}{Probability of coincidences}{birthday}
\aliasA{pbirthday}{birthday}{pbirthday}
\aliasA{qbirthday}{birthday}{qbirthday}
\keyword{distribution}{birthday}
%
\begin{Description}\relax
Computes answers to a generalised \emph{birthday paradox} problem.
\code{pbirthday} computes the probability of a coincidence and
\code{qbirthday} computes the smallest number of observations needed
to have at least a specified probability of coincidence.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
qbirthday(prob = 0.5, classes = 365, coincident = 2)
pbirthday(n, classes = 365, coincident = 2)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{classes}] How many distinct categories the people could fall into
\item[\code{prob}] The desired probability of coincidence
\item[\code{n}] The number of people
\item[\code{coincident}] The number of people to fall in the same category
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The birthday paradox is that a very small number of people, 23,
suffices to have a 50--50 chance that two or more of them have the same
birthday.  This function generalises the calculation to probabilities
other than 0.5, numbers of coincident events other than 2, and numbers
of classes other than 365.

The formula used is approximate for \code{coincident > 2}.  The
approximation is very good for moderate values of \code{prob} but less
good for very small probabilities.
\end{Details}
%
\begin{Value}
\begin{ldescription}
\item[\code{qbirthday}] 
Minimum number of people needed for a probability of at least
\code{prob} that \code{k} or more of them have the same one out of
\code{classes} equiprobable labels.


\item[\code{pbirthday}] Probability of the specified coincidence.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Prior to \R{} 2.14.0 the approximate formula was used even for
\code{coincident = 2}.
\end{Note}
%
\begin{References}\relax
Diaconis, P. and Mosteller F. (1989)
Methods for studying coincidences.
\emph{J. American Statistical Association}, \bold{84}, 853--861.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## the standard version
qbirthday() # 23
## probability of > 2 people with the same birthday
pbirthday(23, coincident = 3)

## examples from Diaconis & Mosteller p. 858.
## 'coincidence' is that husband, wife, daughter all born on the 16th
qbirthday(classes = 30, coincident = 3) # approximately 18
qbirthday(coincident = 4)  # exact value 187
qbirthday(coincident = 10) # exact value 1181

## same 4-digit PIN number
qbirthday(classes = 10^4)

## 0.9 probability of three or more coincident birthdays
qbirthday(coincident = 3, prob = 0.9)

## Chance of 4 or more coincident birthdays in 150 people
pbirthday(150, coincident = 4)

## 100 or more coincident birthdays in 1000 people: very rare
pbirthday(1000, coincident = 100)
\end{ExampleCode}
\end{Examples}
\HeaderA{Box.test}{Box-Pierce and Ljung-Box Tests}{Box.test}
\keyword{ts}{Box.test}
%
\begin{Description}\relax
Compute the Box--Pierce or Ljung--Box test statistic for examining the
null hypothesis of independence in a given time series.  These are
sometimes known as `portmanteau' tests.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
Box.test(x, lag = 1, type = c("Box-Pierce", "Ljung-Box"), fitdf = 0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector or univariate time series.
\item[\code{lag}] the statistic will be based on \code{lag} autocorrelation
coefficients.
\item[\code{type}] test to be performed: partial matching is used.
\item[\code{fitdf}] number of degrees of freedom to be subtracted if \code{x}
is a series of residuals.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These tests are sometimes applied to the residuals from an
\code{ARMA(p, q)} fit, in which case the references suggest a better
approximation to the null-hypothesis distribution is obtained by
setting \code{fitdf = p+q}, provided of course that \code{lag > fitdf}.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the test statistic.
\item[\code{parameter}] the degrees of freedom of the approximate chi-squared
distribution of the test statistic (taking \code{fitdf} into account.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] a character string indicating which type of test was
performed.
\item[\code{data.name}] a character string giving the name of the data.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Missing values are not handled.
\end{Note}
%
\begin{Author}\relax
A. Trapletti
\end{Author}
%
\begin{References}\relax
Box, G. E. P. and Pierce, D. A. (1970),
Distribution of residual correlations in autoregressive-integrated
moving average time series models.
\emph{Journal of the American Statistical Association}, \bold{65},
1509--1526.

Ljung, G. M. and Box, G. E. P. (1978),
On a measure of lack of fit in time series models.
\emph{Biometrika} \bold{65}, 297--303.

Harvey, A. C. (1993)
\emph{Time Series Models}.
2nd Edition, Harvester Wheatsheaf, NY, pp. 44, 45.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
x <- rnorm (100)
Box.test (x, lag = 1)
Box.test (x, lag = 1, type="Ljung")
\end{ExampleCode}
\end{Examples}
\HeaderA{C}{Sets Contrasts for a Factor}{C}
\keyword{models}{C}
%
\begin{Description}\relax
Sets the \code{"contrasts"} attribute for the factor.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
C(object, contr, how.many, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a factor or ordered factor
\item[\code{contr}] which contrasts to use. Can be a matrix with one row for
each level of the factor or a suitable function like
\code{contr.poly} or a character string giving the name of the function
\item[\code{how.many}] the number of contrasts to set, by default one less
than \code{nlevels(object)}.
\item[\code{...}] additional arguments for the function \code{contr}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For compatibility with S, \code{contr} can be \code{treatment},
\code{helmert}, \code{sum} or \code{poly} (without quotes) as shorthand
for \code{contr.treatment} and so on.
\end{Details}
%
\begin{Value}
The factor \code{object} with the \code{"contrasts"} attribute set.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical models.}
Chapter 2 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{contrasts}{contrasts}}, \code{\LinkA{contr.sum}{contr.sum}}, etc.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## reset contrasts to defaults
options(contrasts=c("contr.treatment", "contr.poly"))
tens <- with(warpbreaks, C(tension, poly, 1))
attributes(tens)
## tension SHOULD be an ordered factor, but as it is not we can use
aov(breaks ~ wool + tens + tension, data=warpbreaks)

## show the use of ...  The default contrast is contr.treatment here
summary(lm(breaks ~ wool + C(tension, base=2), data=warpbreaks))


# following on from help(esoph)
model3 <- glm(cbind(ncases, ncontrols) ~ agegp + C(tobgp, , 1) +
     C(alcgp, , 1), data = esoph, family = binomial())
summary(model3)
\end{ExampleCode}
\end{Examples}
\HeaderA{cancor}{Canonical Correlations}{cancor}
\keyword{multivariate}{cancor}
%
\begin{Description}\relax
Compute the canonical correlations between two data matrices.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
cancor(x, y, xcenter = TRUE, ycenter = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric matrix (\eqn{n \times p_1}{}), containing the
x coordinates.
\item[\code{y}] numeric matrix (\eqn{n \times p_2}{}), containing the
y coordinates.
\item[\code{xcenter}] logical or numeric vector of length \eqn{p_1}{},
describing any centering to be done on the x values before the
analysis.  If \code{TRUE} (default), subtract the column means.
If \code{FALSE}, do not adjust the columns.  Otherwise, a vector
of values to be subtracted from the columns.
\item[\code{ycenter}] analogous to \code{xcenter}, but for the y values.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The canonical correlation analysis seeks linear combinations of the
\code{y} variables which are well explained by linear combinations
of the \code{x} variables. The relationship is symmetric as
`well explained' is measured by correlations.
\end{Details}
%
\begin{Value}
A list containing the following components:
\begin{ldescription}
\item[\code{cor}] correlations.
\item[\code{xcoef}] estimated coefficients for the \code{x} variables.
\item[\code{ycoef}] estimated coefficients for the \code{y} variables.
\item[\code{xcenter}] the values used to adjust the \code{x} variables.
\item[\code{ycenter}] the values used to adjust the \code{x} variables.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Hotelling H. (1936).
Relations between two sets of variables.
\emph{Biometrika}, \bold{28}, 321--327.

Seber, G. A. F. (1984).
\emph{Multivariate Observations}.
New York: Wiley, p. 506f.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{qr}{qr}}, \code{\LinkA{svd}{svd}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## signs of results are random
pop <- LifeCycleSavings[, 2:3]
oec <- LifeCycleSavings[, -(2:3)]
cancor(pop, oec)

x <- matrix(rnorm(150), 50, 3)
y <- matrix(rnorm(250), 50, 5)
(cxy <- cancor(x, y))
all(abs(cor(x %*% cxy$xcoef,
            y %*% cxy$ycoef)[,1:3] - diag(cxy $ cor)) < 1e-15)
all(abs(cor(x %*% cxy$xcoef) - diag(3)) < 1e-15)
all(abs(cor(y %*% cxy$ycoef) - diag(5)) < 1e-15)
\end{ExampleCode}
\end{Examples}
\HeaderA{case+variable.names}{Case and Variable Names of Fitted Models}{case+variable.names}
\aliasA{case.names}{case+variable.names}{case.names}
\methaliasA{case.names.lm}{case+variable.names}{case.names.lm}
\aliasA{variable.names}{case+variable.names}{variable.names}
\methaliasA{variable.names.lm}{case+variable.names}{variable.names.lm}
\keyword{regression}{case+variable.names}
\keyword{models}{case+variable.names}
%
\begin{Description}\relax
Simple utilities returning (non-missing) case names, and
(non-eliminated) variable names.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
case.names(object, ...)
## S3 method for class 'lm'
case.names(object, full = FALSE, ...)

variable.names(object, ...)
## S3 method for class 'lm'
variable.names(object, full = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an \R{} object, typically a fitted model.
\item[\code{full}] logical; if \code{TRUE}, all names (including zero weights,
\dots) are returned.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character vector.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{lm}{lm}}; further, \code{\LinkA{all.names}{all.names}},
\code{\LinkA{all.vars}{all.vars}} for functions with a similar name but only slightly
related purpose.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:20
y <-  x + (x/4 - 2)^3 + rnorm(20, sd=3)
names(y) <- paste("O",x,sep=".")
ww <- rep(1,20); ww[13] <- 0
summary(lmxy <- lm(y ~ x + I(x^2)+I(x^3) + I((x-10)^2),
                   weights = ww), cor = TRUE)
variable.names(lmxy)
variable.names(lmxy, full= TRUE)# includes the last
case.names(lmxy)
case.names(lmxy, full = TRUE)# includes the 0-weight case
\end{ExampleCode}
\end{Examples}
\HeaderA{Cauchy}{The Cauchy Distribution}{Cauchy}
\aliasA{dcauchy}{Cauchy}{dcauchy}
\aliasA{pcauchy}{Cauchy}{pcauchy}
\aliasA{qcauchy}{Cauchy}{qcauchy}
\aliasA{rcauchy}{Cauchy}{rcauchy}
\keyword{distribution}{Cauchy}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the Cauchy distribution with location parameter
\code{location} and scale parameter \code{scale}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dcauchy(x, location = 0, scale = 1, log = FALSE)
pcauchy(q, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
qcauchy(p, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
rcauchy(n, location = 0, scale = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{location, scale}] location and scale parameters.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{location} or \code{scale} are not specified, they assume
the default values of \code{0} and \code{1} respectively.

The Cauchy distribution with location \eqn{l}{} and scale \eqn{s}{} has
density
\deqn{f(x) = \frac{1}{\pi s}
    \left( 1 + \left(\frac{x - l}{s}\right)^2 \right)^{-1}%
  }{}
for all \eqn{x}{}.
\end{Details}
%
\begin{Value}
\code{dcauchy}, \code{pcauchy}, and \code{qcauchy} are respectively
the density, distribution function and quantile function of the Cauchy
distribution.  \code{rcauchy} generates random deviates from the
Cauchy.
\end{Value}
%
\begin{Source}\relax
\code{dcauchy}, \code{pcauchy} and \code{qcauchy} are all calculated
from numerically stable versions of the definitions.

\code{rcauchy} uses inversion.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 1, chapter 16.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dt}{dt}} for the t distribution which generalizes
\code{dcauchy(*, l = 0, s = 1)}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
dcauchy(-1:4)
\end{ExampleCode}
\end{Examples}
\HeaderA{chisq.test}{Pearson's Chi-squared Test for Count Data}{chisq.test}
\keyword{htest}{chisq.test}
\keyword{distribution}{chisq.test}
%
\begin{Description}\relax
\code{chisq.test} performs chi-squared contingency table tests
and goodness-of-fit tests.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
chisq.test(x, y = NULL, correct = TRUE,
           p = rep(1/length(x), length(x)), rescale.p = FALSE,
           simulate.p.value = FALSE, B = 2000)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector or matrix. \code{x} and \code{y} can also
both be factors.
\item[\code{y}] a numeric vector; ignored if \code{x} is a matrix.  If
\code{x} is a factor, \code{y} should be a factor of the same length.
\item[\code{correct}] a logical indicating whether to apply continuity
correction when computing the test statistic for 2 by 2 tables: one
half is subtracted from all \eqn{|O - E|}{} differences; however, the 
correction will not be bigger than the differences themselves.  No correction
is done if \code{simulate.p.value = TRUE}.
\item[\code{p}] a vector of probabilities of the same length of \code{x}.
An error is given if any entry of \code{p} is negative.
\item[\code{rescale.p}] a logical scalar; if TRUE then \code{p} is rescaled
(if necessary) to sum to 1.  If \code{rescale.p} is FALSE, and
\code{p} does not sum to 1, an error is given.
\item[\code{simulate.p.value}] a logical indicating whether to compute
p-values by Monte Carlo simulation.
\item[\code{B}] an integer specifying the number of replicates used in the
Monte Carlo test.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{x} is a matrix with one row or column, or if \code{x} is a
vector and \code{y} is not given, then a \emph{goodness-of-fit test}
is performed (\code{x} is treated as a one-dimensional
contingency table).  The entries of \code{x} must be non-negative
integers.  In this case, the hypothesis tested is whether the
population probabilities equal those in \code{p}, or are all equal if
\code{p} is not given.

If \code{x} is a matrix with at least two rows and columns, it is
taken as a two-dimensional contingency table: the entries of \code{x}
must be non-negative integers.  Otherwise, \code{x} and \code{y} must
be vectors or factors of the same length; cases with missing values
are removed, the objects are coerced to factors, and the contingency
table is computed from these.  Then Pearson's chi-squared test is
performed of the null hypothesis that the joint distribution of the
cell counts in a 2-dimensional contingency table is the product of the
row and column marginals.

If \code{simulate.p.value} is \code{FALSE}, the p-value is computed
from the asymptotic chi-squared distribution of the test statistic;
continuity correction is only used in the 2-by-2 case (if \code{correct}
is \code{TRUE}, the default).  Otherwise the p-value is computed for a
Monte Carlo test (Hope, 1968) with \code{B} replicates.

In the contingency table case simulation is done by random sampling
from the set of all contingency tables with given marginals, and works
only if the marginals are strictly positive.  Continuity correction is
never used, and the statistic is quoted without it.  Note that this is
not the usual sampling situation assumed for the chi-squared test but
rather that for Fisher's exact test.

In the goodness-of-fit case simulation is done by random sampling from
the discrete distribution specified by \code{p}, each sample being
of size \code{n = sum(x)}.  This simulation is done in \R{} and may be
slow.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following
components:
\begin{ldescription}
\item[\code{statistic}] the value the chi-squared test statistic.
\item[\code{parameter}] the degrees of freedom of the approximate
chi-squared distribution of the test statistic, \code{NA} if the
p-value is computed by Monte Carlo simulation.
\item[\code{p.value}] the p-value for the test.
\item[\code{method}] a character string indicating the type of test
performed, and whether Monte Carlo simulation or continuity
correction was used.
\item[\code{data.name}] a character string giving the name(s) of the data.
\item[\code{observed}] the observed counts.
\item[\code{expected}] the expected counts under the null hypothesis.
\item[\code{residuals}] the Pearson residuals,
\code{(observed - expected) / sqrt(expected)}.
\item[\code{stdres}] standardized residuals,
\code{(observed - expected) / sqrt(V)}, where \code{V} is the residual cell variance (Agresti, 2007, 
section 2.4.5 for the case where \code{x} is a matrix, \code{n * p * (1 - p)} otherwise).
\end{ldescription}
\end{Value}
%
\begin{Source}\relax
The code for Monte Carlo simulation is a C translation of the Fortran
algorithm of Patefield (1981).
\end{Source}
%
\begin{References}\relax
Hope, A. C. A. (1968)
A simplified Monte Carlo significance test procedure.
\emph{J. Roy, Statist. Soc. B} \bold{30}, 582--598.

Patefield, W. M. (1981)
Algorithm AS159.  An efficient method of generating r x c tables
with given row and column totals.
\emph{Applied Statistics} \bold{30}, 91--97.

Agresti, A. (2007)
\emph{An Introduction to Categorical Data Analysis, 2nd ed.},
New York: John Wiley \& Sons.
Page 38.
\end{References}
%
\begin{SeeAlso}\relax
For goodness-of-fit testing, notably of continuous distributions,
\code{\LinkA{ks.test}{ks.test}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

## From Agresti(2007) p.39
M <- as.table(rbind(c(762, 327, 468), c(484,239,477)))
dimnames(M) <- list(gender=c("M","F"),
                    party=c("Democrat","Independent", "Republican"))
(Xsq <- chisq.test(M))  # Prints test summary
Xsq$observed   # observed counts (same as M) 
Xsq$expected   # expected counts under the null
Xsq$residuals  # Pearson residuals
Xsq$stdres     # standardized residuals


## Effect of simulating p-values
x <- matrix(c(12, 5, 7, 7), ncol = 2)
chisq.test(x)$p.value           # 0.4233
chisq.test(x, simulate.p.value = TRUE, B = 10000)$p.value
                                # around 0.29!

## Testing for population probabilities
## Case A. Tabulated data
x <- c(A = 20, B = 15, C = 25)
chisq.test(x)
chisq.test(as.table(x))             # the same
x <- c(89,37,30,28,2)
p <- c(40,20,20,15,5)
try(
chisq.test(x, p = p)                # gives an error
)
chisq.test(x, p = p, rescale.p = TRUE)
                                # works
p <- c(0.40,0.20,0.20,0.19,0.01)
                                # Expected count in category 5
                                # is 1.86 < 5 ==> chi square approx.
chisq.test(x, p = p)            #               maybe doubtful, but is ok!
chisq.test(x, p = p, simulate.p.value = TRUE)

## Case B. Raw data
x <- trunc(5 * runif(100))
chisq.test(table(x))            # NOT 'chisq.test(x)'!
\end{ExampleCode}
\end{Examples}
\HeaderA{Chisquare}{The (non-central) Chi-Squared Distribution}{Chisquare}
\aliasA{dchisq}{Chisquare}{dchisq}
\aliasA{pchisq}{Chisquare}{pchisq}
\aliasA{qchisq}{Chisquare}{qchisq}
\aliasA{rchisq}{Chisquare}{rchisq}
\keyword{distribution}{Chisquare}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the chi-squared (\eqn{\chi^2}{}) distribution with
\code{df} degrees of freedom and optional non-centrality parameter
\code{ncp}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dchisq(x, df, ncp=0, log = FALSE)
pchisq(q, df, ncp=0, lower.tail = TRUE, log.p = FALSE)
qchisq(p, df, ncp=0, lower.tail = TRUE, log.p = FALSE)
rchisq(n, df, ncp=0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{df}] degrees of freedom (non-negative, but can be non-integer).
\item[\code{ncp}] non-centrality parameter (non-negative).
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The chi-squared distribution with \code{df}\eqn{= n \ge 0}{}
degrees of freedom has density
\deqn{f_n(x) = \frac{1}{{2}^{n/2} \Gamma (n/2)} {x}^{n/2-1} {e}^{-x/2}}{}
for \eqn{x > 0}{}.  The mean and variance are \eqn{n}{} and \eqn{2n}{}.

The non-central chi-squared distribution with \code{df}\eqn{= n}{}
degrees of freedom and non-centrality parameter \code{ncp}
\eqn{= \lambda}{} has density
\deqn{
    f(x) = e^{-\lambda / 2}
      \sum_{r=0}^\infty \frac{(\lambda/2)^r}{r!}\, f_{n + 2r}(x)}{}
for \eqn{x \ge 0}{}.  For integer \eqn{n}{}, this is the distribution of
the sum of squares of \eqn{n}{} normals each with variance one,
\eqn{\lambda}{} being the sum of squares of the normal means; further,
\\{}
\eqn{E(X) = n + \lambda}{}, \eqn{Var(X) = 2(n + 2*\lambda)}{}, and
\eqn{E((X - E(X))^3) = 8(n + 3*\lambda)}{}.

Note that the degrees of freedom \code{df}\eqn{= n}{}, can be
non-integer, and also \eqn{n = 0}{} which is relevant for
non-centrality \eqn{\lambda > 0}{},
see Johnson et al. (1995, chapter 29).

Note that \code{ncp} values larger than about 1e5 may give inaccurate
results with many warnings for \code{pchisq} and \code{qchisq}.
\end{Details}
%
\begin{Value}
\code{dchisq} gives the density, \code{pchisq} gives the distribution
function, \code{qchisq} gives the quantile function, and \code{rchisq}
generates random deviates.

Invalid arguments will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Note}\relax
Supplying \code{ncp = 0} uses the algorithm for the non-central
distribution, which is not the same algorithm used if \code{ncp} is
omitted.  This is to give consistent behaviour in extreme cases with
values of \code{ncp} very near zero.

The code for non-zero \code{ncp} is principally intended to be used
for moderate values of \code{ncp}: it will not be highly accurate,
especially in the tails, for large values.
\end{Note}
%
\begin{Source}\relax
The central cases are computed via the gamma distribution.

The non-central \code{dchisq} and \code{rchisq} are computed as a
Poisson mixture central of chi-squares (Johnson et al, 1995, p.436).

The non-central \code{pchisq} is for \code{ncp < 80} computed from
the Poisson mixture of central chi-squares and for larger \code{ncp}
\emph{via} a C translation of

Ding, C. G. (1992)
Algorithm AS275: Computing the non-central chi-squared
distribution function. \emph{Appl.Statist.}, \bold{41} 478--482.

which computes the lower tail only (so the upper tail suffers from
cancellation and a warning will be given when this is likely to be
significant).

The non-central \code{qchisq} is based on inversion of \code{pchisq}.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, chapters 18 (volume 1)
and 29 (volume 2). Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions.

A central chi-squared distribution with \eqn{n}{} degrees of freedom
is the same as a Gamma distribution with \code{shape} \eqn{\alpha =
    n/2}{} and \code{scale} \eqn{\sigma = 2}{}.  Hence, see
\code{\LinkA{dgamma}{dgamma}} for the Gamma distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

dchisq(1, df=1:3)
pchisq(1, df= 3)
pchisq(1, df= 3, ncp = 0:4)# includes the above

x <- 1:10
## Chi-squared(df = 2) is a special exponential distribution
all.equal(dchisq(x, df=2), dexp(x, 1/2))
all.equal(pchisq(x, df=2), pexp(x, 1/2))

## non-central RNG -- df=0 with ncp > 0:  Z0 has point mass at 0!
Z0 <- rchisq(100, df = 0, ncp = 2.)
graphics::stem(Z0)

## Not run: ## visual testing
## do P-P plots for 1000 points at various degrees of freedom
L <- 1.2; n <- 1000; pp <- ppoints(n)
op <- par(mfrow = c(3,3), mar= c(3,3,1,1)+.1, mgp= c(1.5,.6,0),
          oma = c(0,0,3,0))
for(df in 2^(4*rnorm(9))) {
  plot(pp, sort(pchisq(rr <- rchisq(n,df=df, ncp=L), df=df, ncp=L)),
       ylab="pchisq(rchisq(.),.)", pch=".")
  mtext(paste("df = ",formatC(df, digits = 4)), line= -2, adj=0.05)
  abline(0,1,col=2)
}
mtext(expression("P-P plots : Noncentral  "*
                 chi^2 *"(n=1000, df=X, ncp= 1.2)"),
      cex = 1.5, font = 2, outer=TRUE)
par(op)
## End(Not run)

## "analytical" test
lam <- seq(0,100, by=.25)
p00 <- pchisq(0,      df=0, ncp=lam)
p.0 <- pchisq(1e-300, df=0, ncp=lam)
stopifnot(all.equal(p00, exp(-lam/2)),
          all.equal(p.0, exp(-lam/2)))
\end{ExampleCode}
\end{Examples}
\HeaderA{cmdscale}{Classical (Metric) Multidimensional Scaling}{cmdscale}
\keyword{multivariate}{cmdscale}
%
\begin{Description}\relax
Classical multidimensional scaling of a data matrix.
Also known as \emph{principal coordinates analysis} (Gower, 1966).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
cmdscale(d, k = 2, eig = FALSE, add = FALSE, x.ret = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{d}] a distance structure such as that returned by \code{dist}
or a full symmetric matrix containing the dissimilarities.
\item[\code{k}] the maximum dimension of the space which the data are to be
represented in; must be in \eqn{\{1, 2, \ldots, n-1\}}{}.
\item[\code{eig}] indicates whether eigenvalues should be returned.
\item[\code{add}] logical indicating if an additive constant \eqn{c*}{} should
be computed, and added to the non-diagonal dissimilarities such that
the modified dissimilarities are Euclidean.
\item[\code{x.ret}] indicates whether the doubly centred symmetric distance
matrix should be returned.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Multidimensional scaling takes a set of dissimilarities and returns a
set of points such that the distances between the points are
approximately equal to the dissimilarities.  (It is a major part of
what ecologists call `ordination'.)

A set of Euclidean distances on \eqn{n}{} points can be represented
exactly in at most \eqn{n - 1}{} dimensions.  \code{cmdscale} follows
the analysis of Mardia (1978), and returns the best-fitting
\eqn{k}{}-dimensional representation, where \eqn{k}{} may be less than the
argument \code{k}.

The representation is only determined up to location (\code{cmdscale}
takes the column means of the configuration to be at the origin),
rotations and reflections.  The configuration returned is given in
principal-component axes, so the reflection chosen may differ between
\R{} platforms (see \code{\LinkA{prcomp}{prcomp}}).

When \code{add = TRUE}, a minimal additive constant \eqn{c*}{} is
computed such that the the dissimilarities \eqn{d_{ij} + c*}{} are Euclidean and hence can be represented in \code{n - 1}
dimensions.  Whereas S (Becker \emph{et al.}, 1988) computes this
constant using an approximation suggested by Torgerson, \R{} uses the
analytical solution of Cailliez (1983), see also Cox and Cox (2001).
Note that because of numerical errors the computed eigenvalues need
not all be non-negative, and even theoretically the representation
could be in fewer than \code{n - 1} dimensions.
\end{Details}
%
\begin{Value}
If \code{eig = FALSE}, \code{add = FALSE} and \code{x.ret = FALSE}
(default), a matrix with \code{k} columns whose rows give the
coordinates of the points chosen to represent the dissimilarities.

Otherwise, a list containing the following components.
\begin{ldescription}
\item[\code{points}] a matrix with up to \code{k} columns whose rows give the
coordinates of the points chosen to represent the dissimilarities.
\item[\code{eig}] the \eqn{n}{} eigenvalues computed during the scaling process if
\code{eig} is true.  \strong{NB}: versions of \R{} before 2.12.1
returned only \code{k} but were documented to return \eqn{n - 1}{}.
\item[\code{x}] the doubly centered distance matrix if \code{x.ret} is true.
\item[\code{ac}] the additive constant \eqn{c*}{}, \code{0} if \code{add = FALSE}.
\item[\code{GOF}] a numeric vector of length 2, equal to say
\eqn{(g_1,g_2)}{}, where
\eqn{g_i = (\sum_{j=1}^k \lambda_j)/ (\sum_{j=1}^n T_i(\lambda_j))}{},
where \eqn{\lambda_j}{} are the eigenvalues (sorted in
decreasing order),
\eqn{T_1(v) = \left| v \right|}{}, and
\eqn{T_2(v) = max( v, 0 )}{}.

\end{ldescription}
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Cailliez, F. (1983)
The analytical solution of the additive constant problem.
\emph{Psychometrika} \bold{48}, 343--349.

Cox, T. F. and Cox, M. A. A. (2001)
\emph{Multidimensional Scaling}.  Second edition.
Chapman and Hall.

Gower, J. C. (1966)  
Some distance properties of latent root and vector 
methods used in multivariate analysis.  
\emph{Biometrika} \bold{53}, 325--328.

Krzanowski, W. J. and Marriott, F. H. C. (1994)
\emph{Multivariate Analysis. Part I. Distributions, Ordination and
Inference.}  London: Edward Arnold. (Especially pp. 108--111.)

Mardia, K.V. (1978)
Some properties of classical multidimensional scaling.
\emph{Communications on Statistics -- Theory and Methods}, \bold{A7},
1233--41.

Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979).  Chapter 14 of
\emph{Multivariate Analysis}, London: Academic Press.

Seber, G. A. F. (1984).
\emph{Multivariate Observations}.
New York: Wiley.

Torgerson, W. S. (1958).
\emph{Theory and Methods of Scaling}.
New York: Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{dist}{dist}}.

\code{\LinkA{isoMDS}{isoMDS}} and \code{\LinkA{sammon}{sammon}}
in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} provide alternative methods of
multidimensional scaling.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

loc <- cmdscale(eurodist)
x <- loc[, 1]
y <- -loc[, 2] # reflect so North is at the top
## note asp = 1, to ensure Euclidean distances are represented correctly
plot(x, y, type = "n", xlab = "", ylab = "", asp = 1, axes = FALSE,
     main = "cmdscale(eurodist)")
text(x, y, rownames(loc), cex = 0.6)
\end{ExampleCode}
\end{Examples}
\HeaderA{coef}{Extract Model Coefficients}{coef}
\aliasA{coefficients}{coef}{coefficients}
\keyword{regression}{coef}
\keyword{models}{coef}
%
\begin{Description}\relax
\code{coef} is a generic function which extracts model coefficients
from objects returned by modeling functions.  \code{coefficients} is
an \emph{alias} for it.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
coef(object, ...)
coefficients(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object for which the extraction of model coefficients is
meaningful.
\item[\code{...}] other arguments.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
All object classes which are returned by model fitting functions
should provide a \code{coef} method or use the default one.
(Note that the method is for \code{coef} and not \code{coefficients}.)

Class \code{"aov"} has a \code{coef} method that does not report
aliased coefficients (see \code{\LinkA{alias}{alias}}).
\end{Details}
%
\begin{Value}
Coefficients extracted from the model object \code{object}.

For standard model fitting classes this will be a named numeric vector.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{fitted.values}{fitted.values}} and \code{\LinkA{residuals}{residuals}} for related methods;
\code{\LinkA{glm}{glm}}, \code{\LinkA{lm}{lm}} for model fitting.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:5; coef(lm(c(1:3,7,6) ~ x))
\end{ExampleCode}
\end{Examples}
\HeaderA{complete.cases}{Find Complete Cases}{complete.cases}
\keyword{NA}{complete.cases}
\keyword{logic}{complete.cases}
%
\begin{Description}\relax
Return a logical vector indicating which cases are complete, i.e.,
have no missing values.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
complete.cases(...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{...}] a sequence of vectors, matrices and data frames.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A logical vector specifying which observations/rows have no missing
values across the entire sequence.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{is.na}{is.na}},
\code{\LinkA{na.omit}{na.omit}},
\code{\LinkA{na.fail}{na.fail}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- airquality[, -1] # x is a regression design matrix
y <- airquality[,  1] # y is the corresponding response

stopifnot(complete.cases(y) != is.na(y))
ok <- complete.cases(x,y)
sum(!ok) # how many are not "ok" ?
x <- x[ok,]
y <- y[ok]
\end{ExampleCode}
\end{Examples}
\HeaderA{confint}{Confidence Intervals for Model Parameters}{confint}
\methaliasA{confint.default}{confint}{confint.default}
\keyword{models}{confint}
%
\begin{Description}\relax
Computes confidence intervals for one or more parameters in a fitted
model.  There is a default and a method for objects inheriting from class
\code{"\LinkA{lm}{lm}"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
confint(object, parm, level = 0.95, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a fitted model object.
\item[\code{parm}] a specification of which parameters are to be given
confidence intervals, either a vector of numbers or a vector of
names.  If missing, all parameters are considered.
\item[\code{level}] the confidence level required.
\item[\code{...}] additional argument(s) for methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{confint} is a generic function.  The default method assumes
asymptotic normality, and needs suitable \code{\LinkA{coef}{coef}} and
\code{\LinkA{vcov}{vcov}} methods to be available.  The default method can be
called directly for comparison with other methods.

For objects of class \code{"lm"} the direct formulae based on \eqn{t}{}
values are used.

There are stub methods for classes \code{"glm"} and \code{"nls"} which
invoke those in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} which are based on profile
likelihoods.
\end{Details}
%
\begin{Value}
A matrix (or vector) with columns giving lower and upper confidence
limits for each parameter. These will be labelled as (1-level)/2 and
1 - (1-level)/2 in \% (by default 2.5\% and 97.5\%).
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{confint.glm}{confint.glm}} and
\code{\LinkA{confint.nls}{confint.nls}} in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
fit <- lm(100/mpg ~ disp + hp + wt + am, data=mtcars)
confint(fit)
confint(fit, "wt")

## from example(glm) (needs MASS to be present on the system)
counts <- c(18,17,15,20,10,20,25,13,12)
outcome <- gl(3,1,9); treatment <- gl(3,3)
glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
confint(glm.D93)
confint.default(glm.D93)  # based on asymptotic normality
\end{ExampleCode}
\end{Examples}
\HeaderA{constrOptim}{Linearly Constrained Optimization}{constrOptim}
\keyword{optimize}{constrOptim}
%
\begin{Description}\relax
Minimise a function subject to linear inequality constraints using an
adaptive barrier algorithm.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
constrOptim(theta, f, grad, ui, ci, mu = 1e-04, control = list(),
            method = if(is.null(grad)) "Nelder-Mead" else "BFGS",
            outer.iterations = 100, outer.eps = 1e-05, ...,
            hessian = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{theta}] numeric (vector) starting value (of length \eqn{p}{}): must
be in the feasible region.
\item[\code{f}] function to minimise (see below).
\item[\code{grad}] gradient of \code{f} (a \code{\LinkA{function}{function}} as well),
or \code{NULL} (see below).
\item[\code{ui}] constraint matrix (\eqn{k \times p}{}), see below.
\item[\code{ci}] constraint vector of length \eqn{k}{} (see below).
\item[\code{mu}] (Small) tuning parameter.
\item[\code{control, method, hessian}] passed to \code{\LinkA{optim}{optim}}.
\item[\code{outer.iterations}] iterations of the barrier algorithm.
\item[\code{outer.eps}] non-negative number; the relative convergence
tolerance of the barrier algorithm.
\item[\code{...}] Other named arguments to be passed to \code{f} and \code{grad}:
needs to be passed through \code{\LinkA{optim}{optim}} so should not match its
argument names.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The feasible region is defined by \code{ui \%*\% theta - ci >= 0}. The
starting value must be in the interior of the feasible region, but the
minimum may be on the boundary.

A logarithmic barrier is added to enforce the constraints and then
\code{\LinkA{optim}{optim}} is called. The barrier function is chosen so that
the objective function should decrease at each outer iteration. Minima
in the interior of the feasible region are typically found quite
quickly, but a substantial number of outer iterations may be needed
for a minimum on the boundary.

The tuning parameter \code{mu} multiplies the barrier term. Its precise
value is often relatively unimportant. As \code{mu} increases the
augmented objective function becomes closer to the original objective
function but also less smooth near the boundary of the feasible
region.

Any \code{optim} method that permits infinite values for the
objective function may be used (currently all but "L-BFGS-B").

The objective function \code{f} takes as first argument the vector
of parameters over which minimisation is to take place.  It should
return a scalar result. Optional arguments \code{...} will be
passed to \code{optim} and then (if not used by \code{optim}) to
\code{f}. As with \code{optim}, the default is to minimise, but
maximisation can be performed by setting \code{control\$fnscale} to a
negative value.

The gradient function \code{grad} must be supplied except with
\code{method="Nelder-Mead"}.  It should take arguments matching
those of \code{f} and return a vector containing the gradient.

\end{Details}
%
\begin{Value}
As for \code{\LinkA{optim}{optim}}, but with two extra components:
\code{barrier.value} giving the value of the barrier function at the
optimum and \code{outer.iterations} gives the
number of outer iterations (calls to \code{optim}).
The \code{counts} component contains the \emph{sum} of all
\code{\LinkA{optim}{optim}()\$counts}.
\end{Value}
%
\begin{References}\relax
K. Lange \emph{Numerical Analysis for Statisticians.} Springer
2001, p185ff
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{optim}{optim}}, especially \code{method="L-BFGS-B"} which
does box-constrained optimisation.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

## from optim
fr <- function(x) {   ## Rosenbrock Banana function
    x1 <- x[1]
    x2 <- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr <- function(x) { ## Gradient of 'fr'
    x1 <- x[1]
    x2 <- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}

optim(c(-1.2,1), fr, grr)
#Box-constraint, optimum on the boundary
constrOptim(c(-1.2,0.9), fr, grr, ui=rbind(c(-1,0),c(0,-1)), ci=c(-1,-1))
#  x<=0.9,  y-x>0.1
constrOptim(c(.5,0), fr, grr, ui=rbind(c(-1,0),c(1,-1)), ci=c(-0.9,0.1))


## Solves linear and quadratic programming problems
## but needs a feasible starting value
#
# from example(solve.QP) in 'quadprog'
# no derivative
fQP <- function(b) {-sum(c(0,5,0)*b)+0.5*sum(b*b)}
Amat       <- matrix(c(-4,-3,0,2,1,0,0,-2,1),3,3)
bvec       <- c(-8,2,0)
constrOptim(c(2,-1,-1), fQP, NULL, ui=t(Amat),ci=bvec)
# derivative
gQP <- function(b) {-c(0,5,0)+b}
constrOptim(c(2,-1,-1), fQP, gQP, ui=t(Amat), ci=bvec)

## Now with maximisation instead of minimisation
hQP <- function(b) {sum(c(0,5,0)*b)-0.5*sum(b*b)}
constrOptim(c(2,-1,-1), hQP, NULL, ui=t(Amat), ci=bvec,
            control=list(fnscale=-1))
\end{ExampleCode}
\end{Examples}
\HeaderA{contrast}{(Possibly Sparse) Contrast Matrices}{contrast}
\aliasA{contr.helmert}{contrast}{contr.helmert}
\aliasA{contr.poly}{contrast}{contr.poly}
\aliasA{contr.SAS}{contrast}{contr.SAS}
\aliasA{contr.sum}{contrast}{contr.sum}
\aliasA{contr.treatment}{contrast}{contr.treatment}
\keyword{design}{contrast}
\keyword{regression}{contrast}
\keyword{array}{contrast}
%
\begin{Description}\relax
Return a matrix of contrasts.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
contr.helmert(n, contrasts = TRUE, sparse = FALSE)
contr.poly(n, scores = 1:n, contrasts = TRUE, sparse = FALSE)
contr.sum(n, contrasts = TRUE, sparse = FALSE)
contr.treatment(n, base = 1, contrasts = TRUE, sparse = FALSE)
contr.SAS(n, contrasts = TRUE, sparse = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] a vector of levels for a factor, or the number of levels.
\item[\code{contrasts}] a logical indicating whether contrasts should be
computed.
\item[\code{sparse}] logical indicating if the result should be sparse
(of class \code{\LinkA{dgCMatrix}{dgCMatrix}}), using
package \Rhref{http://CRAN.R-project.org/package=Matrix}{\pkg{Matrix}}.
\item[\code{scores}] the set of values over which orthogonal polynomials are
to be computed.
\item[\code{base}] an integer specifying which group is considered the
baseline group. Ignored if \code{contrasts} is \code{FALSE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These functions are used for creating contrast matrices for use in
fitting analysis of variance and regression models.  The columns of
the resulting matrices contain contrasts which can be used for coding
a factor with \code{n} levels.  The returned value contains the
computed contrasts.  If the argument \code{contrasts} is \code{FALSE}
a square indicator matrix (the dummy coding) is returned \bold{except}
for \code{contr.poly} (which includes the 0-degree, i.e. constant,
polynomial when \code{contrasts = FALSE}).

\code{contr.helmert} returns Helmert contrasts, which contrast the
second level with the first, the third with the average of the first
two, and so on.  \code{contr.poly} returns contrasts based on
orthogonal polynomials. \code{contr.sum} uses `sum to zero
contrasts'.

\code{contr.treatment} contrasts each level with the baseline level
(specified by \code{base}): the baseline level is omitted.  Note that
this does not produce `contrasts' as defined in the standard
theory for linear models as they are not orthogonal to the intercept.

\code{contr.SAS} is a wrapper for \code{contr.treatment} that sets
the base level to be the last level of the factor.  The coefficients
produced when using these contrasts should be equivalent to those
produced by many (but not all) SAS procedures.

For consistency, \code{sparse} is an argument to all these contrast
functions, however \code{sparse = TRUE} for \code{contr.poly}
is typically pointless and is rarely useful for
\code{contr.helmert}.
\end{Details}
%
\begin{Value}
A matrix with \code{n} rows and \code{k} columns, with \code{k=n-1} if
\code{contrasts} is \code{TRUE} and \code{k=n} if \code{contrasts} is
\code{FALSE}.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical models.}
Chapter 2 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{contrasts}{contrasts}},
\code{\LinkA{C}{C}},
and
\code{\LinkA{aov}{aov}},
\code{\LinkA{glm}{glm}},
\code{\LinkA{lm}{lm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
(cH <- contr.helmert(4))
apply(cH, 2,sum) # column sums are 0
crossprod(cH) # diagonal -- columns are orthogonal
contr.helmert(4, contrasts = FALSE) # just the 4 x 4 identity matrix

(cT <- contr.treatment(5))
all(crossprod(cT) == diag(4)) # TRUE: even orthonormal

(cT. <- contr.SAS(5))
all(crossprod(cT.) == diag(4)) # TRUE

zapsmall(cP <- contr.poly(3)) # Linear and Quadratic
zapsmall(crossprod(cP), digits=15) # orthonormal up to fuzz
\end{ExampleCode}
\end{Examples}
\HeaderA{contrasts}{Get and Set Contrast Matrices}{contrasts}
\aliasA{contrasts<\Rdash}{contrasts}{contrasts<.Rdash.}
\keyword{design}{contrasts}
\keyword{regression}{contrasts}
%
\begin{Description}\relax
Set and view the contrasts associated with a factor.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
contrasts(x, contrasts = TRUE, sparse = FALSE)
contrasts(x, how.many) <- value
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a factor or a logical variable.
\item[\code{contrasts}] logical.  See `Details'.
\item[\code{sparse}] logical indicating if the result should be sparse
(of class \code{\LinkA{dgCMatrix}{dgCMatrix}}), using
package \Rhref{http://CRAN.R-project.org/package=Matrix}{\pkg{Matrix}}.
\item[\code{how.many}] How many contrasts should be made. Defaults to one
less than the number of levels of \code{x}.  This need not be the
same as the number of columns of \code{value}.
\item[\code{value}] either a numeric matrix (or a sparse or dense matrix of a
class extending \code{\LinkA{dMatrix}{dMatrix}} from
package \Rhref{http://CRAN.R-project.org/package=Matrix}{\pkg{Matrix}})  whose columns give coefficients for
contrasts in the levels of \code{x}, or the (quoted) name of a
function which computes such matrices.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If contrasts are not set for a factor the default functions from
\code{\LinkA{options}{options}("contrasts")} are used.

A logical vector \code{x} is converted into a two-level factor with
levels \code{c(FALSE, TRUE)} (regardless of which levels occur in the
variable).

The argument \code{contrasts} is ignored if \code{x} has a matrix
\code{contrasts} attribute set.  Otherwise if \code{contrasts = TRUE}
it is passed to a contrasts function such as
\code{\LinkA{contr.treatment}{contr.treatment}} and if \code{contrasts = FALSE}
an identity matrix is returned.  Suitable functions have a first
argument which is the character vector of levels, a named argument
\code{contrasts} (always called with \code{contrasts = TRUE}) and
optionally from \R{} 2.10.0 a logical argument \code{sparse}.

If \code{value} supplies more than \code{how.many} contrasts, the
first \code{how.many} are used.  If too few are supplied, a suitable
contrast matrix is created by extending \code{value} after ensuring
its columns are contrasts (orthogonal to the constant term) and not
collinear.
\end{Details}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical models.}
Chapter 2 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{C}{C}},
\code{\LinkA{contr.helmert}{contr.helmert}},
\code{\LinkA{contr.poly}{contr.poly}},
\code{\LinkA{contr.sum}{contr.sum}},
\code{\LinkA{contr.treatment}{contr.treatment}};
\code{\LinkA{glm}{glm}},
\code{\LinkA{aov}{aov}},
\code{\LinkA{lm}{lm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
utils::example(factor)
fff <- ff[, drop=TRUE]  # reduce to 5 levels.
contrasts(fff) # treatment contrasts by default
contrasts(C(fff, sum))
contrasts(fff, contrasts = FALSE) # the 5x5 identity matrix

contrasts(fff) <- contr.sum(5); contrasts(fff)  # set sum contrasts
contrasts(fff, 2) <- contr.sum(5); contrasts(fff)  # set 2 contrasts
# supply 2 contrasts, compute 2 more to make full set of 4.
contrasts(fff) <- contr.sum(5)[,1:2]; contrasts(fff)

## using sparse contrasts: % useful, once model.matrix() works with these :
ffs <- fff
contrasts(ffs) <- contr.sum(5, sparse=TRUE)[,1:2]; contrasts(ffs)
stopifnot(all.equal(ffs, fff))
contrasts(ffs) <- contr.sum(5, sparse=TRUE); contrasts(ffs)
\end{ExampleCode}
\end{Examples}
\HeaderA{convolve}{Convolution of Sequences via FFT}{convolve}
\keyword{math}{convolve}
\keyword{dplot}{convolve}
%
\begin{Description}\relax
Use the Fast Fourier Transform to compute the several kinds of
convolutions of two sequences.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
convolve(x, y, conj = TRUE, type = c("circular", "open", "filter"))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x,y}] numeric sequences \emph{of the same length} to be
convolved.
\item[\code{conj}] logical; if \code{TRUE}, take the complex \emph{conjugate}
before back-transforming (default, and used for usual convolution).
\item[\code{type}] character; one of \code{"circular"}, \code{"open"},
\code{"filter"} (beginning of word is ok).  For \code{circular}, the
two sequences are treated as \emph{circular}, i.e., periodic.

For \code{open} and \code{filter}, the sequences are padded with
\code{0}s (from left and right) first; \code{"filter"} returns the
middle sub-vector of \code{"open"}, namely, the result of running a
weighted mean of \code{x} with weights \code{y}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The Fast Fourier Transform, \code{\LinkA{fft}{fft}}, is used for efficiency.

The input sequences \code{x} and  \code{y} must have the same length if
\code{circular} is true.

Note that the usual definition of convolution of two sequences
\code{x} and \code{y} is given by \code{convolve(x, rev(y), type = "o")}.
\end{Details}
%
\begin{Value}
If \code{r <- convolve(x,y, type = "open")}
and \code{n <- length(x)}, \code{m <- length(y)}, then
\deqn{r_k = \sum_{i} x_{k-m+i} y_{i}}{}
where the sum is over all valid indices \eqn{i}{}, for
\eqn{k = 1, \dots, n+m-1}{}.

If \code{type == "circular"}, \eqn{n = m}{} is required, and the above is
true for \eqn{i , k = 1,\dots,n}{} when
\eqn{x_{j} := x_{n+j}}{} for \eqn{j < 1}{}.
\end{Value}
%
\begin{References}\relax
Brillinger, D. R. (1981)
\emph{Time Series: Data Analysis and Theory}, Second Edition.
San Francisco: Holden-Day.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{fft}{fft}}, \code{\LinkA{nextn}{nextn}}, and particularly
\code{\LinkA{filter}{filter}} (from the \pkg{stats} package) which may be
more appropriate.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

x <- c(0,0,0,100,0,0,0)
y <- c(0,0,1, 2 ,1,0,0)/4
zapsmall(convolve(x,y))         #  *NOT* what you first thought.
zapsmall(convolve(x, y[3:5], type="f")) # rather
x <- rnorm(50)
y <- rnorm(50)
# Circular convolution *has* this symmetry:
all.equal(convolve(x,y, conj = FALSE), rev(convolve(rev(y),x)))

n <- length(x <- -20:24)
y <- (x-10)^2/1000 + rnorm(x)/8

Han <- function(y) # Hanning
       convolve(y, c(1,2,1)/4, type = "filter")

plot(x,y, main="Using  convolve(.) for Hanning filters")
lines(x[-c(1  , n)      ], Han(y), col="red")
lines(x[-c(1:2, (n-1):n)], Han(Han(y)), lwd=2, col="dark blue")
\end{ExampleCode}
\end{Examples}
\HeaderA{cophenetic}{Cophenetic Distances for a Hierarchical Clustering}{cophenetic}
\methaliasA{cophenetic.default}{cophenetic}{cophenetic.default}
\methaliasA{cophenetic.dendrogram}{cophenetic}{cophenetic.dendrogram}
\keyword{cluster}{cophenetic}
\keyword{multivariate}{cophenetic}
%
\begin{Description}\relax
Computes the cophenetic distances for a hierarchical clustering.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
cophenetic(x)
## Default S3 method:
cophenetic(x)
## S3 method for class 'dendrogram'
cophenetic(x)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an R object representing a hierarchical clustering.
For the default method, an object of class \code{"\LinkA{hclust}{hclust}"} or
with a method for \code{\LinkA{as.hclust}{as.hclust}()} such as
\code{"\LinkA{agnes}{agnes}"} in package \Rhref{http://CRAN.R-project.org/package=cluster}{\pkg{cluster}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The cophenetic distance between two observations that have been
clustered is defined to be the intergroup dissimilarity at which the
two observations are first combined into a single cluster.
Note that this distance has many ties and restrictions.

It can be argued that a dendrogram is an appropriate summary of some
data if the correlation between the original distances and the
cophenetic distances is high.  Otherwise, it should simply be viewed as
the description of the output of the clustering algorithm.

\code{cophenetic} is a generic function.  Support for classes which
represent hierarchical clusterings (total indexed hierarchies) can be
added by providing an \code{\LinkA{as.hclust}{as.hclust}()} or, more directly, a
\code{cophenetic()} method for such a class.

The method for objects of class \code{"\LinkA{dendrogram}{dendrogram}"} requires
that all leaves of the dendrogram object have non-null labels.
\end{Details}
%
\begin{Value}
An object of class \code{"dist"}.
\end{Value}
%
\begin{Author}\relax
Robert Gentleman
\end{Author}
%
\begin{References}\relax
Sneath, P.H.A. and Sokal, R.R. (1973)
\emph{Numerical Taxonomy: The Principles and Practice of Numerical
Classification}, p. 278 ff;
Freeman, San Francisco.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{dist}{dist}},
\code{\LinkA{hclust}{hclust}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

d1 <- dist(USArrests)
hc <- hclust(d1, "ave")
d2 <- cophenetic(hc)
cor(d1,d2) # 0.7659

## Example from Sneath & Sokal, Fig. 5-29, p.279
d0 <- c(1,3.8,4.4,5.1, 4,4.2,5, 2.6,5.3, 5.4)
attributes(d0) <- list(Size = 5, diag=TRUE)
class(d0) <- "dist"
names(d0) <- letters[1:5]
d0
utils::str(upgma <- hclust(d0, method = "average"))
plot(upgma, hang = -1)
#
(d.coph <- cophenetic(upgma))
cor(d0, d.coph) # 0.9911
\end{ExampleCode}
\end{Examples}
\HeaderA{cor}{Correlation, Variance and Covariance (Matrices)}{cor}
\aliasA{cov}{cor}{cov}
\aliasA{cov2cor}{cor}{cov2cor}
\aliasA{var}{cor}{var}
\keyword{univar}{cor}
\keyword{multivariate}{cor}
\keyword{array}{cor}
%
\begin{Description}\relax
\code{var}, \code{cov} and \code{cor} compute the variance of \code{x}
and the covariance or correlation of \code{x} and \code{y} if these
are vectors.   If \code{x} and \code{y} are matrices then the
covariances (or correlations) between the columns of \code{x} and the
columns of \code{y} are computed.

\code{cov2cor} scales a covariance matrix into the corresponding
correlation matrix \emph{efficiently}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
var(x, y = NULL, na.rm = FALSE, use)

cov(x, y = NULL, use = "everything",
    method = c("pearson", "kendall", "spearman"))

cor(x, y = NULL, use = "everything",
    method = c("pearson", "kendall", "spearman"))

cov2cor(V)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector, matrix or data frame.
\item[\code{y}] \code{NULL} (default) or a vector, matrix or data frame with
compatible dimensions to \code{x}.   The default is equivalent to
\code{y = x} (but more efficient).
\item[\code{na.rm}] logical. Should missing values be removed?
\item[\code{use}] an optional character string giving a
method for computing covariances in the presence
of missing values.  This must be (an abbreviation of) one of the strings
\code{"everything"}, \code{"all.obs"}, \code{"complete.obs"},
\code{"na.or.complete"}, or \code{"pairwise.complete.obs"}.
\item[\code{method}] a character string indicating which correlation
coefficient (or covariance) is to be computed.  One of
\code{"pearson"} (default), \code{"kendall"}, or \code{"spearman"},
can be abbreviated.
\item[\code{V}] symmetric numeric matrix, usually positive definite such as a
covariance matrix.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For \code{cov} and \code{cor} one must \emph{either} give a matrix or
data frame for \code{x} \emph{or} give both \code{x} and \code{y}.

The inputs must be numeric (as determined by \code{\LinkA{is.numeric}{is.numeric}}:
logical values are also allowed for historical compatibility): the
\code{"kendall"} and \code{"spearman"} methods make sense for ordered
inputs but \code{\LinkA{xtfrm}{xtfrm}} can be used to find a suitable prior
transformation to numbers.

\code{var} is just another interface to \code{cov}, where
\code{na.rm} is used to determine the default for \code{use} when that
is unspecified.  If \code{na.rm} is \code{TRUE} then the complete
observations (rows) are used (\code{use = "na.or.complete"}) to
compute the variance.  Otherwise, by default \code{use = "everything"}.

If \code{use} is \code{"everything"}, \code{\LinkA{NA}{NA}}s will
propagate conceptually, i.e., a resulting value will be \code{NA}
whenever one of its contributing observations is \code{NA}.\\{}
If \code{use} is \code{"all.obs"}, then the presence of missing
observations will produce an error.  If \code{use} is
\code{"complete.obs"} then missing values are handled by casewise
deletion (and if there are no complete cases, that gives an error).
\\{}
\code{"na.or.complete"} is the same unless there are no complete
cases, that gives \code{NA}.
Finally, if \code{use} has the value \code{"pairwise.complete.obs"}
then the correlation or covariance between each pair of variables is
computed using all complete pairs of observations on those variables.
This can result in covariance or correlation matrices which are not positive
semi-definite, as well as \code{NA} entries if there are no complete
pairs for that pair of variables.   For \code{cov} and \code{var},
\code{"pairwise.complete.obs"} only works with the \code{"pearson"}
method.
Note that (the equivalent of) \code{var(double(0), use=*)} gives
\code{NA} for \code{use = "everything"} and \code{"na.or.complete"},
and gives an error in the other cases.

The denominator \eqn{n - 1}{} is used which gives an unbiased estimator
of the (co)variance for i.i.d. observations.
These functions return \code{\LinkA{NA}{NA}} when there is only one
observation (whereas S-PLUS has been returning \code{NaN}), and
fail if \code{x} has length zero.

For \code{cor()}, if \code{method} is \code{"kendall"} or
\code{"spearman"}, Kendall's \eqn{\tau}{} or Spearman's
\eqn{\rho}{} statistic is used to estimate a rank-based measure of
association.  These are more robust and have been recommended if the
data do not necessarily come from a bivariate normal distribution.\\{}
For \code{cov()}, a non-Pearson method is unusual but available for
the sake of completeness.  Note that \code{"spearman"} basically
computes \code{cor(R(x), R(y))} (or \code{cov(.,.)}) where
\code{R(u) := rank(u, na.last="keep")}. In the case of missing values, the
ranks are calculated depending on the value of \code{use}, either
based on complete observations, or based on pairwise completeness with
reranking for each pair.

Scaling a covariance matrix into a correlation one can be achieved in
many ways, mathematically most appealing by multiplication with a
diagonal matrix from left and right, or more efficiently by using
\code{\LinkA{sweep}{sweep}(.., FUN = "/")} twice.  The \code{cov2cor} function
is even a bit more efficient, and provided mostly for didactical
reasons.
\end{Details}
%
\begin{Value}
For \code{r <- cor(*, use = "all.obs")}, it is now guaranteed that
\code{all(r <= 1)}.
\end{Value}
%
\begin{Note}\relax
Some people have noted that the code for Kendall's tau is slow for
very large datasets (many more than 1000 cases).  It rarely makes
sense to do such a computation, but see function
\code{\LinkA{cor.fk}{cor.fk}} in package \Rhref{http://CRAN.R-project.org/package=pcaPP}{\pkg{pcaPP}}.
\end{Note}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{cor.test}{cor.test}} for confidence intervals (and tests).

\code{\LinkA{cov.wt}{cov.wt}} for \emph{weighted} covariance computation.

\code{\LinkA{sd}{sd}} for standard deviation (vectors).
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
var(1:10)# 9.166667

var(1:5,1:5)# 2.5

## Two simple vectors
cor(1:10,2:11)# == 1

## Correlation Matrix of Multivariate sample:
(Cl <- cor(longley))
## Graphical Correlation Matrix:
symnum(Cl) # highly correlated

## Spearman's rho  and  Kendall's tau
symnum(clS <- cor(longley, method = "spearman"))
symnum(clK <- cor(longley, method = "kendall"))
## How much do they differ?
i <- lower.tri(Cl)
cor(cbind(P = Cl[i], S = clS[i], K = clK[i]))


## cov2cor() scales a covariance matrix by its diagonal
##           to become the correlation matrix.
cov2cor # see the function definition {and learn ..}
stopifnot(all.equal(Cl, cov2cor(cov(longley))),
          all.equal(cor(longley, method="kendall"),
            cov2cor(cov(longley, method="kendall"))))

##--- Missing value treatment:
C1 <- cov(swiss)
range(eigen(C1, only.values=TRUE)$values) # 6.19        1921
swM <- swiss
swM[1,2] <- swM[7,3] <- swM[25,5] <- NA # create 3 "missing"
try(cov(swM)) # Error: missing obs...
C2 <- cov(swM, use = "complete")
range(eigen(C2, only.values=TRUE)$values) # 6.46        1930
C3 <- cov(swM, use = "pairwise")
range(eigen(C3, only.values=TRUE)$values) # 6.19        1938

symnum(cor(swM, method = "kendall", use = "complete"))
## Kendall's tau doesn't change much:
symnum(cor(swiss, method = "kendall"))
\end{ExampleCode}
\end{Examples}
\HeaderA{cor.test}{Test for Association/Correlation Between Paired Samples}{cor.test}
\methaliasA{cor.test.default}{cor.test}{cor.test.default}
\methaliasA{cor.test.formula}{cor.test}{cor.test.formula}
\keyword{htest}{cor.test}
%
\begin{Description}\relax
Test for association between paired samples, using one of
Pearson's product moment correlation coefficient,
Kendall's \eqn{\tau}{} or Spearman's \eqn{\rho}{}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
cor.test(x, ...)

## Default S3 method:
cor.test(x, y,
         alternative = c("two.sided", "less", "greater"),
         method = c("pearson", "kendall", "spearman"),
         exact = NULL, conf.level = 0.95, continuity = FALSE, ...)

## S3 method for class 'formula'
cor.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] numeric vectors of data values.  \code{x} and \code{y}
must have the same length.
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"}, \code{"greater"} or \code{"less"}.  You
can specify just the initial letter.  \code{"greater"} corresponds
to positive association, \code{"less"} to negative association.
\item[\code{method}] a character string indicating which correlation
coefficient is to be  used for the test.  One of \code{"pearson"},
\code{"kendall"}, or \code{"spearman"}, can be abbreviated.
\item[\code{exact}] a logical indicating whether an exact p-value should be
computed.  Used for Kendall's \eqn{\tau}{} and
Spearman's \eqn{\rho}{}.
See `Details' for the meaning of \code{NULL} (the default).
\item[\code{conf.level}] confidence level for the returned confidence
interval.  Currently only used for the Pearson product moment
correlation coefficient if there are at least 4 complete pairs of
observations.
\item[\code{continuity}] logical: if true, a continuity correction is used
for Kendall's \eqn{\tau}{} and Spearman's \eqn{\rho}{} when
not computed exactly.
\item[\code{formula}] a formula of the form \code{\textasciitilde{} u + v}, where each of
\code{u} and \code{v} are numeric variables giving the data values
for one sample.  The samples must be of the same length.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The three methods each estimate the association between paired samples
and compute a test of the value being zero.  They use different
measures of association, all in the range \eqn{[-1, 1]}{} with \eqn{0}{}
indicating no association.  These are sometimes referred to as tests
of no \emph{correlation}, but that term is often confined to the
default method.

If \code{method} is \code{"pearson"}, the test statistic is based on
Pearson's product moment correlation coefficient \code{cor(x, y)} and
follows a t distribution with \code{length(x)-2} degrees of freedom
if the samples follow independent normal distributions.  If there are
at least 4 complete pairs of observation, an asymptotic confidence
interval is given based on Fisher's Z transform.

If \code{method} is \code{"kendall"} or \code{"spearman"}, Kendall's
\eqn{\tau}{} or Spearman's \eqn{\rho}{} statistic is used to
estimate a rank-based measure of association.  These tests may be used
if the data do not necessarily come from a bivariate normal
distribution.

For Kendall's test, by default (if \code{exact} is NULL), an exact
p-value is computed if there are less than 50 paired samples containing
finite values and there are no ties.  Otherwise, the test statistic is
the estimate scaled to zero mean and unit variance, and is approximately
normally distributed.

For Spearman's test, p-values are computed using algorithm AS 89 for
\eqn{n < 1290}{} and \code{exact = TRUE}, otherwise via the asymptotic
\eqn{t}{} approximation.  Note that these are `exact' for \eqn{n
  < 10}{}, and use an Edgeworth series approximation for larger sample
sizes (the cutoff has been changed from the original paper).
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the test statistic.
\item[\code{parameter}] the degrees of freedom of the test statistic in the
case that it follows a t distribution.
\item[\code{p.value}] the p-value of the test.
\item[\code{estimate}] the estimated measure of association, with name
\code{"cor"}, \code{"tau"}, or \code{"rho"} corresponding
to the method employed.
\item[\code{null.value}] the value of the association measure under the
null hypothesis, always \code{0}.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] a character string indicating how the association was
measured.
\item[\code{data.name}] a character string giving the names of the data.
\item[\code{conf.int}] a confidence interval for the measure of association.
Currently only given for Pearson's product moment correlation
coefficient in case of at least 4 complete pairs of observations.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
D. J. Best \& D. E. Roberts (1975),
Algorithm AS 89: The Upper Tail Probabilities of Spearman's \eqn{\rho}{}.
\emph{Applied Statistics}, \bold{24}, 377--379.

Myles Hollander \& Douglas A. Wolfe (1973),
\emph{Nonparametric Statistical Methods.}
New York: John Wiley \& Sons.
Pages 185--194 (Kendall and Spearman tests).
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{Kendall}{Kendall}} in package \Rhref{http://CRAN.R-project.org/package=Kendall}{\pkg{Kendall}}.

\code{\LinkA{pKendall}{pKendall}} and
\code{\LinkA{pSpearman}{pSpearman}} in package
\Rhref{http://CRAN.R-project.org/package=SuppDists}{\pkg{SuppDists}},
\code{\LinkA{spearman.test}{spearman.test}} in package
\Rhref{http://CRAN.R-project.org/package=pspearman}{\pkg{pspearman}}, 
which supply different (and often more accurate) approximations.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Hollander & Wolfe (1973), p. 187f.
## Assessment of tuna quality.  We compare the Hunter L measure of
##  lightness to the averages of consumer panel scores (recoded as
##  integer values from 1 to 6 and averaged over 80 such values) in
##  9 lots of canned tuna.

x <- c(44.4, 45.9, 41.9, 53.3, 44.7, 44.1, 50.7, 45.2, 60.1)
y <- c( 2.6,  3.1,  2.5,  5.0,  3.6,  4.0,  5.2,  2.8,  3.8)

##  The alternative hypothesis of interest is that the
##  Hunter L value is positively associated with the panel score.

cor.test(x, y, method = "kendall", alternative = "greater")
## => p=0.05972

cor.test(x, y, method = "kendall", alternative = "greater",
         exact = FALSE) # using large sample approximation
## => p=0.04765

## Compare this to
cor.test(x, y, method = "spearm", alternative = "g")
cor.test(x, y,                    alternative = "g")

## Formula interface.
require(graphics)
pairs(USJudgeRatings)
cor.test(~ CONT + INTG, data = USJudgeRatings)
\end{ExampleCode}
\end{Examples}
\HeaderA{cov.wt}{Weighted Covariance Matrices}{cov.wt}
\keyword{multivariate}{cov.wt}
%
\begin{Description}\relax
Returns a list containing estimates of the weighted covariance matrix
and the mean of the data, and optionally of the (weighted) correlation
matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
cov.wt(x, wt = rep(1/nrow(x), nrow(x)), cor = FALSE, center = TRUE,
       method = c("unbiased", "ML"))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a matrix or data frame.  As usual, rows are observations and
columns are variables.
\item[\code{wt}] a non-negative and non-zero vector of weights for each
observation.  Its length must equal the number of rows of \code{x}.
\item[\code{cor}] a logical indicating whether the estimated correlation
weighted matrix will be returned as well.
\item[\code{center}] either a logical or a numeric vector specifying the
centers to be used when computing covariances.  If \code{TRUE}, the
(weighted) mean of each variable is used, if \code{FALSE}, zero is
used.  If \code{center} is numeric, its length must equal the number
of columns of \code{x}.
\item[\code{method}] string specifying how the result is scaled, see
`Details' below.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
By default, \code{method = "unbiased"},
The covariance matrix is divided by one minus the sum of squares of
the weights, so if the weights are the default (\eqn{1/n}{}) the conventional
unbiased estimate of the covariance matrix with divisor \eqn{(n - 1)}{}
is obtained.  This differs from the behaviour in S-PLUS which
corresponds to \code{method = "ML"} and does not divide.
\end{Details}
%
\begin{Value}
A list containing the following named components:
\begin{ldescription}
\item[\code{cov}] the estimated (weighted) covariance matrix
\item[\code{center}] an estimate for the center (mean) of the data.
\item[\code{n.obs}] the number of observations (rows) in \code{x}.
\item[\code{wt}] the weights used in the estimation.  Only returned if given
as an argument.
\item[\code{cor}] the estimated correlation matrix.  Only returned if
\code{cor} is \code{TRUE}.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{cov}{cov}} and \code{\LinkA{var}{var}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
 (xy <- cbind(x = 1:10, y = c(1:3, 8:5, 8:10)))
 w1 <- c(0,0,0,1,1,1,1,1,0,0)
 cov.wt(xy, wt = w1) # i.e. method = "unbiased"
 cov.wt(xy, wt = w1, method = "ML", cor = TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{cpgram}{Plot Cumulative Periodogram}{cpgram}
\keyword{ts}{cpgram}
\keyword{hplot}{cpgram}
%
\begin{Description}\relax
Plots a cumulative periodogram.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
cpgram(ts, taper = 0.1,
       main = paste("Series: ", deparse(substitute(ts))),
       ci.col = "blue")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{ts}] a univariate time series
\item[\code{taper}] proportion tapered in forming the periodogram
\item[\code{main}] main title
\item[\code{ci.col}] colour for confidence band.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
None.
\end{Value}
%
\begin{Section}{Side Effects}
Plots the cumulative periodogram in a square plot.
\end{Section}
%
\begin{Note}\relax
From package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}.
\end{Note}
%
\begin{Author}\relax
B.D. Ripley
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

par(pty = "s", mfrow = c(1,2))
cpgram(lh)
lh.ar <- ar(lh, order.max = 9)
cpgram(lh.ar$resid, main = "AR(3) fit to lh")

cpgram(ldeaths)
\end{ExampleCode}
\end{Examples}
\HeaderA{cutree}{Cut a Tree into Groups of Data}{cutree}
\keyword{multivariate}{cutree}
\keyword{cluster}{cutree}
%
\begin{Description}\relax
Cuts a tree, e.g., as resulting from \code{\LinkA{hclust}{hclust}}, into several
groups either by specifying the desired number(s) of groups or the cut
height(s).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
cutree(tree, k = NULL, h = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{tree}] a tree as produced by \code{\LinkA{hclust}{hclust}}. \code{cutree()}
only expects a list with components \code{merge}, \code{height}, and
\code{labels}, of appropriate content each.
\item[\code{k}] an integer scalar or vector with the desired number of groups
\item[\code{h}] numeric scalar or vector with heights where the tree should
be cut.
At least one of \code{k} or \code{h} must be specified, \code{k}
overrides \code{h} if both are given.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Cutting trees at a given height is only possible for ultrametric trees
(with monotone clustering heights).
\end{Details}
%
\begin{Value}
\code{cutree} returns a vector with group memberships if \code{k} or
\code{h} are scalar, otherwise a matrix with group memberships is returned
where each column corresponds to the elements of \code{k} or \code{h},
respectively (which are also used as column names).
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{hclust}{hclust}}, \code{\LinkA{dendrogram}{dendrogram}} for cutting trees themselves.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
hc <- hclust(dist(USArrests))

cutree(hc, k=1:5) #k = 1 is trivial
cutree(hc, h=250)

## Compare the 2 and 4 grouping:
g24 <- cutree(hc, k = c(2,4))
table(grp2=g24[,"2"], grp4=g24[,"4"])
\end{ExampleCode}
\end{Examples}
\HeaderA{decompose}{Classical Seasonal Decomposition by Moving Averages}{decompose}
\aliasA{plot.decomposed.ts}{decompose}{plot.decomposed.ts}
\keyword{ts}{decompose}
%
\begin{Description}\relax
Decompose a time series into seasonal, trend and irregular components
using moving averages.  Deals with additive or multiplicative
seasonal component.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
decompose(x, type = c("additive", "multiplicative"), filter = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A time series.
\item[\code{type}] The type of seasonal component. Can be abbreviated.
\item[\code{filter}] A vector of filter coefficients in reverse time order (as for
AR or MA coefficients), used for filtering out the seasonal
component.  If \code{NULL}, a moving average with symmetric window is
performed.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The additive model used is:
\deqn{Y_t = T_t + S_t + e_t}{}
The multiplicative model used is:
\deqn{Y_t = T_t\,S_t\, e_t}{}

The function first determines the trend component using a moving
average (if \code{filter} is \code{NULL}, a symmetric window with
equal weights is used), and removes it from the time series.  Then,
the seasonal figure is computed by averaging, for each time unit, over
all periods.  The seasonal figure is then centered.   Finally, the error
component is determined by removing trend and seasonal figure
(recycled as needed) from the original time series.

This only works well if \code{x} covers an integer number of complete
periods.
\end{Details}
%
\begin{Value}
An object of class \code{"decomposed.ts"} with following components:
\begin{ldescription}
\item[\code{x}] The original series.  (Only since \R{} 2.14.0.)
\item[\code{seasonal}] The seasonal component (i.e., the repeated seasonal figure).
\item[\code{figure}] The estimated seasonal figure only.
\item[\code{trend}] The trend component.
\item[\code{random}] The remainder part.
\item[\code{type}] The value of \code{type}.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The function \code{\LinkA{stl}{stl}} provides a much more sophisticated
decomposition.
\end{Note}
%
\begin{Author}\relax
David Meyer \email{David.Meyer@wu.ac.at}
\end{Author}
%
\begin{References}\relax
M. Kendall and A. Stuart (1983)
\emph{The Advanced Theory of Statistics}, Vol.3,
Griffin. pp. 410--414.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{stl}{stl}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

m <- decompose(co2)
m$figure
plot(m)

## example taken from Kendall/Stuart
x <- c(-50, 175, 149, 214, 247, 237, 225, 329, 729, 809, 
       530, 489, 540, 457, 195, 176, 337, 239, 128, 102, 232, 429, 3, 
       98, 43, -141, -77, -13, 125, 361, -45, 184)
x <- ts(x, start = c(1951, 1), end = c(1958, 4), frequency = 4)
m <- decompose(x)
## seasonal figure: 6.25, 8.62, -8.84, -6.03
round(decompose(x)$figure / 10, 2)
\end{ExampleCode}
\end{Examples}
\HeaderA{delete.response}{Modify Terms Objects}{delete.response}
\aliasA{drop.terms}{delete.response}{drop.terms}
\aliasA{reformulate}{delete.response}{reformulate}
\aliasA{[.terms}{delete.response}{[.terms}
\keyword{programming}{delete.response}
%
\begin{Description}\relax
\code{delete.response} returns a \code{terms} object for the same
model but with no response variable.

\code{drop.terms} removes variables from the right-hand side of the
model. There is also a \code{"[.terms"} method to perform the same
function (with \code{keep.response=TRUE}).

\code{reformulate} creates a formula from a character vector.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
delete.response(termobj)

reformulate(termlabels, response = NULL, intercept = TRUE)

drop.terms(termobj, dropx = NULL, keep.response = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{termobj}] A \code{terms} object
\item[\code{termlabels}] character vector giving the right-hand side of a
model formula.  Cannot be zero-length.
\item[\code{response}] character string, symbol or call giving the left-hand side of
a model formula, or \code{NULL}.
\item[\code{intercept}] logical: should the formula have an intercept?  New
in \R{} 2.13.0.
\item[\code{dropx}] vector of positions of variables to drop from the
right-hand side of the model.
\item[\code{keep.response}] Keep the response in the resulting object?
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{delete.response} and \code{drop.terms} return a \code{terms}
object.

\code{reformulate} returns a \code{formula}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{terms}{terms}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ff <- y ~ z + x + w
tt <- terms(ff)
tt
delete.response(tt)
drop.terms(tt, 2:3, keep.response = TRUE)
tt[-1]
tt[2:3]
reformulate(attr(tt, "term.labels"))

## keep LHS :
reformulate("x*w", ff[[2]])
fS <- surv(ft, case) ~ a + b
reformulate(c("a", "b*f"), fS[[2]])

stopifnot(identical(      ~ var, reformulate("var")),
          identical(~ a + b + c, reformulate(letters[1:3])),
          identical(  y ~ a + b, reformulate(letters[1:2], "y"))
         )
\end{ExampleCode}
\end{Examples}
\HeaderA{dendrapply}{Apply a Function to All Nodes of a Dendrogram}{dendrapply}
\keyword{iteration}{dendrapply}
%
\begin{Description}\relax
Apply function \code{FUN} to each node of a \code{\LinkA{dendrogram}{dendrogram}}
recursively.  When  \code{y <- dendrapply(x, fn)}, then \code{y} is a
dendrogram of the same graph structure as \code{x} and for each node,
\code{y.node[j] <- FUN( x.node[j], ...)} (where \code{y.node[j]} is an
(invalid!) notation for the j-th node of y.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dendrapply(X, FUN, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{X}] an object of class \code{"\LinkA{dendrogram}{dendrogram}"}.
\item[\code{FUN}] an \R{} function to be applied to each dendrogram node,
typically working on its \code{\LinkA{attributes}{attributes}} alone, returning an
altered version of the same node.
\item[\code{...}] potential further arguments passed to \code{FUN}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Usually a dendrogram of the same (graph) structure as \code{X}.
For that, the function must be conceptually of the form
\code{FUN <- function(X) \{ attributes(X) <- .....;  X \}},
i.e. returning the node with some attributes added or changed.
\end{Value}
%
\begin{Note}\relax
this is still somewhat experimental, and suggestions for
enhancements (or nice examples of usage) are very welcome.
\end{Note}
%
\begin{Author}\relax
Martin Maechler
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{as.dendrogram}{as.dendrogram}}, \code{\LinkA{lapply}{lapply}} for applying
a function to each component of a \code{list}, \code{\LinkA{rapply}{rapply}}
for doing so to each non-list component of a nested list.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## a smallish simple dendrogram
dhc <- as.dendrogram(hc <- hclust(dist(USArrests), "ave"))
(dhc21 <- dhc[[2]][[1]])

## too simple:
dendrapply(dhc21, function(n) utils::str(attributes(n)))

## toy example to set colored leaf labels :
local({
  colLab <<- function(n) {
      if(is.leaf(n)) {
        a <- attributes(n)
        i <<- i+1
        attr(n, "nodePar") <-
            c(a$nodePar, list(lab.col = mycols[i], lab.font= i%%3))
      }
      n
  }
  mycols <- grDevices::rainbow(attr(dhc21,"members"))
  i <- 0
 })
dL <- dendrapply(dhc21, colLab)
op <- par(mfrow=2:1)
 plot(dhc21)
 plot(dL) ## --> colored labels!
par(op)
\end{ExampleCode}
\end{Examples}
\HeaderA{dendrogram}{General Tree Structures}{dendrogram}
\aliasA{as.dendrogram}{dendrogram}{as.dendrogram}
\methaliasA{as.dendrogram.dendrogram}{dendrogram}{as.dendrogram.dendrogram}
\methaliasA{as.dendrogram.hclust}{dendrogram}{as.dendrogram.hclust}
\aliasA{as.hclust.dendrogram}{dendrogram}{as.hclust.dendrogram}
\aliasA{cut.dendrogram}{dendrogram}{cut.dendrogram}
\aliasA{is.leaf}{dendrogram}{is.leaf}
\aliasA{merge.dendrogram}{dendrogram}{merge.dendrogram}
\aliasA{plot.dendrogram}{dendrogram}{plot.dendrogram}
\aliasA{print.dendrogram}{dendrogram}{print.dendrogram}
\aliasA{rev.dendrogram}{dendrogram}{rev.dendrogram}
\aliasA{str.dendrogram}{dendrogram}{str.dendrogram}
\aliasA{[[.dendrogram}{dendrogram}{[[.dendrogram}
\keyword{multivariate}{dendrogram}
\keyword{tree}{dendrogram}
\keyword{hplot}{dendrogram}
%
\begin{Description}\relax
Class \code{"dendrogram"} provides general functions for handling
tree-like structures.  It is intended as a replacement for similar
functions in hierarchical clustering and classification/regression
trees, such that all of these can use the same engine for plotting or
cutting trees.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
as.dendrogram(object, ...)
## S3 method for class 'hclust'
as.dendrogram(object, hang = -1, ...)

## S3 method for class 'dendrogram'
as.hclust(x, ...)

## S3 method for class 'dendrogram'
plot(x, type = c("rectangle", "triangle"),
      center = FALSE,
      edge.root = is.leaf(x) || !is.null(attr(x,"edgetext")),
      nodePar = NULL, edgePar = list(),
      leaflab = c("perpendicular", "textlike", "none"),
      dLeaf = NULL, xlab = "", ylab = "", xaxt = "n", yaxt = "s",
      horiz = FALSE, frame.plot = FALSE, xlim, ylim, ...)

## S3 method for class 'dendrogram'
cut(x, h, ...)

## S3 method for class 'dendrogram'
merge(x, y, ..., height)

## S3 method for class 'dendrogram'
print(x, digits, ...)

## S3 method for class 'dendrogram'
rev(x)

## S3 method for class 'dendrogram'
str(object, max.level = NA, digits.d = 3,
    give.attr = FALSE, wid = getOption("width"),
    nest.lev = 0, indent.str = "",
    last.str = getOption("str.dendrogram.last"), stem = "--",
    ...)

is.leaf(object)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] any \R{} object that can be made into one of class
\code{"dendrogram"}.
\item[\code{x, y}] object(s) of class \code{"dendrogram"}.
\item[\code{hang}] numeric scalar indicating how the \emph{height} of leaves
should be computed from the heights of their parents; see
\code{\LinkA{plot.hclust}{plot.hclust}}.
\item[\code{type}] type of plot.
\item[\code{center}] logical; if \code{TRUE}, nodes are plotted centered with
respect to the leaves in the branch.  Otherwise (default), plot them
in the middle of all direct child nodes.
\item[\code{edge.root}] logical; if true, draw an edge to the root node.
\item[\code{nodePar}] a \code{list} of plotting parameters to use for the
nodes (see \code{\LinkA{points}{points}}) or \code{NULL} by default which
does not draw symbols at the nodes.  The list may contain components
named \code{pch}, \code{cex}, \code{col}, \code{xpd},
and/or \code{bg} each of
which can have length two for specifying separate attributes for
\emph{inner} nodes and \emph{leaves}.  Note that the default of
\code{pch} is \code{1:2}, so you may want to use \code{pch = NA} if
you specify \code{nodePar}.
\item[\code{edgePar}] a \code{list} of plotting parameters to use for the
edge \code{\LinkA{segments}{segments}} and labels (if there's an
\code{edgetext}).  The list may contain components
named \code{col}, \code{lty} and \code{lwd} (for the segments),
\code{p.col}, \code{p.lwd}, and \code{p.lty} (for the
\code{\LinkA{polygon}{polygon}} around the text) and \code{t.col} for the text
color.  As with \code{nodePar}, each can have length two for
differentiating leaves and inner nodes.

\item[\code{leaflab}] a string specifying how leaves are labeled.  The
default \code{"perpendicular"} write text vertically (by default).\\{}
\code{"textlike"} writes text horizontally (in a rectangle), and \\{}
\code{"none"} suppresses leaf labels.
\item[\code{dLeaf}] a number specifying the \bold{d}istance in user
coordinates between the tip of a leaf and its label.  If \code{NULL}
as per default, 3/4 of a letter width or height is used.
\item[\code{horiz}] logical indicating if the dendrogram should be drawn
\emph{horizontally} or not.
\item[\code{frame.plot}] logical indicating if a box around the plot should
be drawn, see \code{\LinkA{plot.default}{plot.default}}.
\item[\code{h}] height at which the tree is cut.
\item[\code{height}] height at which the two dendrogram should be merged.  If not
specified (or \code{NULL}), the default is ten percent larger than
the (larger of the) two component heights.
\item[\code{xlim, ylim}] optional x- and y-limits of the plot, passed to
\code{\LinkA{plot.default}{plot.default}}.  The defaults for these show the full
dendrogram.
\item[\code{..., xlab, ylab, xaxt, yaxt}] graphical parameters, or arguments for
other methods.
\item[\code{digits}] integer specifying the precision for printing, see
\code{\LinkA{print.default}{print.default}}.
\item[\code{max.level, digits.d, give.attr, wid, nest.lev, indent.str}] arguments
to \code{str}, see \code{\LinkA{str.default}{str.default}()}.  Note that
\code{give.attr = FALSE} still shows \code{height} and \code{members}
attributes for each node.
\item[\code{last.str, stem}] strings used for \code{str()} specifying how the
last branch (at each level) should start and the \emph{stem}
to use for each dendrogram branch.  In some environments, using
\code{last.str = "'"} will provide much nicer looking output, than
the historical default \code{last.str = "`"}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The dendrogram is directly represented as a nested list where each
component corresponds to a branch of the tree.  Hence, the first
branch of tree \code{z} is \code{z[[1]]}, the second branch of the
corresponding subtree is \code{z[[1]][[2]]}, or shorter
\code{z[[c(1,2)]]}, etc..  Each node of the tree
carries some information needed for efficient plotting or cutting as
attributes, of which only \code{members}, \code{height} and
\code{leaf} for leaves are compulsory:
\begin{description}

\item[\code{members}] total number of leaves in the branch
\item[\code{height}] numeric non-negative height at which the node
is plotted.
\item[\code{midpoint}] numeric horizontal distance of the node from
the left border (the leftmost leaf) of the branch (unit 1 between
all leaves).  This is used for \code{plot(*, center=FALSE)}.
\item[\code{label}] character; the label of the node
\item[\code{x.member}] for \code{cut()\$upper},
the number of \emph{former} members; more generally a substitute
for the \code{members} component used for `horizontal'
(when \code{horiz = FALSE}, else `vertical') alignment.
\item[\code{edgetext}] character; the label for the edge leading to
the node
\item[\code{nodePar}] a named list (of length-1 components)
specifying node-specific attributes for \code{\LinkA{points}{points}}
plotting, see the \code{nodePar} argument above.
\item[\code{edgePar}] a named list (of length-1 components)
specifying attributes for \code{\LinkA{segments}{segments}} plotting of the
edge leading to the node, and drawing of the \code{edgetext} if
available, see the \code{edgePar} argument above.
\item[\code{leaf}] logical, if \code{TRUE}, the node is a leaf of
the tree.


\end{description}


\code{cut.dendrogram()} returns a list with components \code{\$upper}
and \code{\$lower}, the first is a truncated version of the original
tree, also of class \code{dendrogram}, the latter a list with the
branches obtained from cutting the tree, each a \code{dendrogram}.

There are \code{\LinkA{[[}{[[}}, \code{\LinkA{print}{print}}, and \code{\LinkA{str}{str}}
methods for \code{"dendrogram"} objects where the first one
(extraction) ensures that selecting sub-branches keeps the class.

Objects of class \code{"hclust"} can be converted to class
\code{"dendrogram"} using method \code{as.dendrogram()}, and since R
2.13.0, there is also a \code{\LinkA{as.hclust}{as.hclust}()} method as an inverse.

\code{rev.dendrogram} simply returns the dendrogram \code{x} with
reversed nodes, see also \code{\LinkA{reorder.dendrogram}{reorder.dendrogram}}.

The \code{\LinkA{merge}{merge}(x, y, ...)} method which merges two or more
dendrograms into a new one which has \code{x} and \code{y} (and
optional further arguments) as branches.

\code{is.leaf(object)} returns logical indicating if \code{object} is a
leaf (the most simple dendrogram).

\code{plotNode()} and \code{plotNodeLimit()} are helper functions.
\end{Details}
%
\begin{Section}{Warning}
Some operations on dendrograms (including plotting) make use of
recursion.  For very deep trees It may be necessary to increase
\code{\LinkA{options}{options}("expressions")}: if you do you are likely to need
to set the C stack size larger than the default where possible.
\end{Section}
%
\begin{Note}\relax
\begin{description}

\item[\code{plot()}:] When using \code{type = "triangle"},
\code{center = TRUE} often looks better.
\item[\code{str(d)}:] If you really want to see the \emph{internal}
structure, use \code{str(unclass(d))} instead.

\end{description}

\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{dendrapply}{dendrapply}} for applying a function to \emph{each} node.
\code{\LinkA{order.dendrogram}{order.dendrogram}} and \code{\LinkA{reorder.dendrogram}{reorder.dendrogram}};
further, the \code{\LinkA{labels}{labels}} method.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics); require(utils)

hc <- hclust(dist(USArrests), "ave")
(dend1 <- as.dendrogram(hc)) # "print()" method
str(dend1)          # "str()" method
str(dend1, max = 2, last.str= "'") # only the first two sub-levels
oo <- options(str.dendrogram.last = "\\") # yet another possibility
str(dend1, max = 2) # only the first two sub-levels
options(oo)# .. resetting them

op <- par(mfrow= c(2,2), mar = c(5,2,1,4))
plot(dend1)
## "triangle" type and show inner nodes:
plot(dend1, nodePar=list(pch = c(1,NA), cex=0.8, lab.cex = 0.8),
      type = "t", center=TRUE)
plot(dend1, edgePar=list(col = 1:2, lty = 2:3),
     dLeaf=1, edge.root = TRUE)
plot(dend1, nodePar=list(pch = 2:1,cex=.4*2:1, col = 2:3),
     horiz=TRUE)

## simple test for as.hclust() as the inverse of as.dendrogram():
stopifnot(identical(as.hclust(dend1)[1:4], hc[1:4]))

dend2 <- cut(dend1, h=70)
plot(dend2$upper)
## leaves are wrong horizontally:
plot(dend2$upper, nodePar=list(pch = c(1,7), col = 2:1))
##  dend2$lower is *NOT* a dendrogram, but a list of .. :
plot(dend2$lower[[3]], nodePar=list(col=4), horiz = TRUE, type = "tr")
## "inner" and "leaf" edges in different type & color :
plot(dend2$lower[[2]], nodePar=list(col=1),# non empty list
     edgePar = list(lty=1:2, col=2:1), edge.root=TRUE)
par(op)
d3 <- dend2$lower[[2]][[2]][[1]]
stopifnot(identical(d3, dend2$lower[[2]][[c(2,1)]]))
str(d3, last.str="'")

## merge() to join dendrograms:
(d13 <- merge(dend2$lower[[1]], dend2$lower[[3]]))
## merge() all parts back (using default 'height' instead of original one):
den.1 <- Reduce(merge, dend2$lower)
## or merge() all four parts at same height --> 4 branches (!)
d. <- merge(dend2$lower[[1]], dend2$lower[[2]], dend2$lower[[3]],
            dend2$lower[[4]])
## (with a warning) or the same using  do.call :
stopifnot(identical(d., do.call(merge, dend2$lower)))
plot(d., main="merge(d1, d2, d3, d4)  |->  dendrogram with a 4-split")

## "Zoom" in to the first dendrogram :
plot(dend1, xlim = c(1,20), ylim = c(1,50))

nP <- list(col=3:2, cex=c(2.0, 0.75), pch= 21:22,
           bg= c("light blue", "pink"),
           lab.cex = 0.75, lab.col = "tomato")
plot(d3, nodePar= nP, edgePar = list(col="gray", lwd=2), horiz = TRUE)

addE <- function(n) {
      if(!is.leaf(n)) {
        attr(n, "edgePar") <- list(p.col="plum")
        attr(n, "edgetext") <- paste(attr(n,"members"),"members")
      }
      n
}
d3e <- dendrapply(d3, addE)
plot(d3e, nodePar= nP)
plot(d3e, nodePar= nP, leaflab = "textlike")



\end{ExampleCode}
\end{Examples}
\HeaderA{density}{Kernel Density Estimation}{density}
\methaliasA{density.default}{density}{density.default}
\aliasA{print.density}{density}{print.density}
\keyword{distribution}{density}
\keyword{smooth}{density}
%
\begin{Description}\relax
The (S3) generic function \code{density} computes kernel density
estimates.  Its default method does so with the given kernel and
bandwidth for univariate observations.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
density(x, ...)
## Default S3 method:
density(x, bw = "nrd0", adjust = 1,
        kernel = c("gaussian", "epanechnikov", "rectangular",
                   "triangular", "biweight",
                   "cosine", "optcosine"),
        weights = NULL, window = kernel, width,
        give.Rkern = FALSE,
        n = 512, from, to, cut = 3, na.rm = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] the data from which the estimate is to be computed.

\item[\code{bw}] the smoothing bandwidth to be used.  The kernels are scaled
such that this is the standard deviation of the smoothing kernel.
(Note this differs from the reference books cited below, and from S-PLUS.)

\code{bw} can also be a character string giving a rule to choose the
bandwidth.  See \code{\LinkA{bw.nrd}{bw.nrd}}. \\{}
The default, \code{"nrd0"}, has remained the default for historical
and compatibility reasons, rather than as a general recommendation,
where e.g., \code{"SJ"} would rather fit, see also V\&R (2002).

The specified (or computed) value of \code{bw} is multiplied by
\code{adjust}.

\item[\code{adjust}] the bandwidth used is actually \code{adjust*bw}.
This makes it easy to specify values like `half the default'
bandwidth.

\item[\code{kernel, window}] a character string giving the smoothing kernel
to be used. This must be one of \code{"gaussian"},
\code{"rectangular"}, \code{"triangular"}, \code{"epanechnikov"},
\code{"biweight"}, \code{"cosine"} or \code{"optcosine"}, with default
\code{"gaussian"}, and may be abbreviated to a unique prefix (single
letter).

\code{"cosine"} is smoother than \code{"optcosine"}, which is the
usual `cosine' kernel in the literature and almost MSE-efficient.
However, \code{"cosine"} is the version used by S.


\item[\code{weights}] numeric vector of non-negative observation weights,
hence of same length as \code{x}. The default \code{NULL} is
equivalent to \code{weights = rep(1/nx, nx)} where \code{nx} is the
length of (the finite entries of) \code{x[]}.

\item[\code{width}] this exists for compatibility with S; if given, and
\code{bw} is not, will set \code{bw} to \code{width} if this is a
character string, or to a kernel-dependent multiple of \code{width}
if this is numeric.

\item[\code{give.Rkern}] logical; if true, \emph{no} density is estimated, and
the `canonical bandwidth' of the chosen \code{kernel} is returned
instead.

\item[\code{n}] the number of equally spaced points at which the density is
to be estimated.  When \code{n > 512}, it is rounded up to a power
of 2 during the calculations (as \code{\LinkA{fft}{fft}} is used) and the
final result is interpolated by \code{\LinkA{approx}{approx}}.  So it almost
always makes sense to specify \code{n} as a power of two.


\item[\code{from,to}] the left and right-most points of the grid at which the
density is to be estimated; the defaults are \code{cut * bw} outside
of \code{range(x)}.

\item[\code{cut}] by default, the values of \code{from} and \code{to} are
\code{cut} bandwidths beyond the extremes of the data.  This allows
the estimated density to drop to approximately zero at the extremes.

\item[\code{na.rm}] logical; if \code{TRUE}, missing values are removed
from \code{x}. If \code{FALSE} any missing values cause an error.
\item[\code{...}] further arguments for (non-default) methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The algorithm used in \code{density.default} disperses the mass of the
empirical distribution function over a regular grid of at least 512
points and then uses the fast Fourier transform to convolve this
approximation with a discretized version of the kernel and then uses
linear approximation to evaluate the density at the specified points.

The statistical properties of a kernel are determined by
\eqn{\sigma^2_K = \int t^2 K(t) dt}{}
which is always \eqn{= 1}{} for our kernels (and hence the bandwidth
\code{bw} is the standard deviation of the kernel) and
\eqn{R(K) = \int K^2(t) dt}{}.\\{}
MSE-equivalent bandwidths (for different kernels) are proportional to
\eqn{\sigma_K R(K)}{} which is scale invariant and for our
kernels equal to \eqn{R(K)}{}.  This value is returned when
\code{give.Rkern = TRUE}.  See the examples for using exact equivalent
bandwidths.

Infinite values in \code{x} are assumed to correspond to a point mass at
\code{+/-Inf} and the density estimate is of the sub-density on
\code{(-Inf, +Inf)}.
\end{Details}
%
\begin{Value}
If \code{give.Rkern} is true, the number \eqn{R(K)}{}, otherwise
an object with class \code{"density"} whose
underlying structure is a list containing the following components.
\begin{ldescription}
\item[\code{x}] the \code{n} coordinates of the points where the density is
estimated.
\item[\code{y}] the estimated density values.  These will be non-negative,
but can be zero.
\item[\code{bw}] the bandwidth used.
\item[\code{n}] the sample size after elimination of missing values.
\item[\code{call}] the call which produced the result.
\item[\code{data.name}] the deparsed name of the \code{x} argument.
\item[\code{has.na}] logical, for compatibility (always \code{FALSE}).

\end{ldescription}
The \code{print} method reports \code{\LinkA{summary}{summary}} values on the
\code{x} and \code{y} components.
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole (for S version).

Scott, D. W. (1992)
\emph{Multivariate Density Estimation. Theory, Practice and Visualization}.
New York: Wiley.

Sheather, S. J. and Jones M. C. (1991)
A reliable data-based bandwidth selection method for kernel density
estimation.
\emph{J. Roy. Statist. Soc.} \bold{B}, 683--690.

Silverman, B. W. (1986)
\emph{Density Estimation}.
London: Chapman and Hall.

Venables, W. N. and Ripley, B. D. (2002)
\emph{Modern Applied Statistics with S}.
New York: Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{bw.nrd}{bw.nrd}},
\code{\LinkA{plot.density}{plot.density}}, \code{\LinkA{hist}{hist}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

plot(density(c(-20,rep(0,98),20)), xlim = c(-4,4))# IQR = 0

# The Old Faithful geyser data
d <- density(faithful$eruptions, bw = "sj")
d
plot(d)

plot(d, type = "n")
polygon(d, col = "wheat")

## Missing values:
x <- xx <- faithful$eruptions
x[i.out <- sample(length(x), 10)] <- NA
doR <- density(x, bw = 0.15, na.rm = TRUE)
lines(doR, col = "blue")
points(xx[i.out], rep(0.01, 10))

## Weighted observations:
fe <- sort(faithful$eruptions) # has quite a few non-unique values
## use 'counts / n' as weights:
dw <- density(unique(fe), weights = table(fe)/length(fe), bw = d$bw)
utils::str(dw) ## smaller n: only 126, but identical estimate:
stopifnot(all.equal(d[1:3], dw[1:3]))

## simulation from a density() fit:
# a kernel density fit is an equally-weighted mixture.
fit <- density(xx)
N <- 1e6
x.new <- rnorm(N, sample(xx, size = N, replace = TRUE), fit$bw)
plot(fit)
lines(density(x.new), col="blue")


(kernels <- eval(formals(density.default)$kernel))

## show the kernels in the R parametrization
plot (density(0, bw = 1), xlab = "",
      main="R's density() kernels with bw = 1")
for(i in 2:length(kernels))
   lines(density(0, bw = 1, kernel =  kernels[i]), col = i)
legend(1.5,.4, legend = kernels, col = seq(kernels),
       lty = 1, cex = .8, y.intersp = 1)

## show the kernels in the S parametrization
plot(density(0, from=-1.2, to=1.2, width=2, kernel="gaussian"), type="l",
     ylim = c(0, 1), xlab="", main="R's density() kernels with width = 1")
for(i in 2:length(kernels))
   lines(density(0, width = 2, kernel =  kernels[i]), col = i)
legend(0.6, 1.0, legend = kernels, col = seq(kernels), lty = 1)

##-------- Semi-advanced theoretic from here on -------------


(RKs <- cbind(sapply(kernels,
                     function(k) density(kernel = k, give.Rkern = TRUE))))
100*round(RKs["epanechnikov",]/RKs, 4) ## Efficiencies

bw <- bw.SJ(precip) ## sensible automatic choice
plot(density(precip, bw = bw),
     main = "same sd bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(precip, bw = bw, kernel = kernels[i]), col = i)

## Bandwidth Adjustment for "Exactly Equivalent Kernels"
h.f <- sapply(kernels, function(k)density(kernel = k, give.Rkern = TRUE))
(h.f <- (h.f["gaussian"] / h.f)^ .2)
## -> 1, 1.01, .995, 1.007,... close to 1 => adjustment barely visible..

plot(density(precip, bw = bw),
     main = "equivalent bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(precip, bw = bw, adjust = h.f[i], kernel = kernels[i]),
         col = i)
legend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)
\end{ExampleCode}
\end{Examples}
\HeaderA{deriv}{Symbolic and Algorithmic Derivatives of Simple Expressions}{deriv}
\aliasA{D}{deriv}{D}
\methaliasA{deriv.default}{deriv}{deriv.default}
\methaliasA{deriv.formula}{deriv}{deriv.formula}
\aliasA{deriv3}{deriv}{deriv3}
\methaliasA{deriv3.default}{deriv}{deriv3.default}
\methaliasA{deriv3.formula}{deriv}{deriv3.formula}
\keyword{math}{deriv}
\keyword{nonlinear}{deriv}
%
\begin{Description}\relax
Compute derivatives of simple expressions, symbolically.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
    D (expr, name)
 deriv(expr, ...)
deriv3(expr, ...)

 ## Default S3 method:
deriv(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = FALSE, ...)
 ## S3 method for class 'formula'
deriv(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = FALSE, ...)

## Default S3 method:
deriv3(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = TRUE, ...)
## S3 method for class 'formula'
deriv3(expr, namevec, function.arg = NULL, tag = ".expr",
       hessian = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{expr}] A \code{\LinkA{expression}{expression}} or \code{\LinkA{call}{call}} or
(except \code{D}) a formula with no lhs.
\item[\code{name,namevec}] character vector, giving the variable names (only
one for \code{D()}) with respect to which derivatives will be
computed.
\item[\code{function.arg}] If specified and non-\code{NULL}, a character
vector of arguments for a function return, or a function (with empty
body) or \code{TRUE}, the latter indicating that a function with
argument names \code{namevec} should be used.
\item[\code{tag}] character; the prefix to be used for the locally created
variables in result.
\item[\code{hessian}] a logical value indicating whether the second derivatives
should be calculated and incorporated in the return value.
\item[\code{...}] arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{D} is modelled after its S namesake for taking simple symbolic
derivatives.

\code{deriv} is a \emph{generic} function with a default and a
\code{\LinkA{formula}{formula}} method.  It returns a \code{\LinkA{call}{call}} for
computing the \code{expr} and its (partial) derivatives,
simultaneously.  It uses so-called \emph{algorithmic derivatives}.  If
\code{function.arg} is a function, its arguments can have default
values, see the \code{fx} example below.

Currently, \code{deriv.formula} just calls \code{deriv.default} after
extracting the expression to the right of \code{\textasciitilde{}}.

\code{deriv3} and its methods are equivalent to \code{deriv} and its
methods except that \code{hessian} defaults to \code{TRUE} for
\code{deriv3}.

The internal code knows about the arithmetic operators \code{+},
\code{-}, \code{*}, \code{/} and \code{\textasciicircum{}}, and the single-variable
functions \code{exp}, \code{log}, \code{sin}, \code{cos}, \code{tan},
\code{sinh}, \code{cosh}, \code{sqrt}, \code{pnorm}, \code{dnorm},
\code{asin}, \code{acos}, \code{atan}, \code{gamma}, \code{lgamma},
\code{digamma} and \code{trigamma}, as well as \code{psigamma} for one
or two arguments (but derivative only with respect to the first).
(Note that only the standard normal distribution is considered.)
\end{Details}
%
\begin{Value}
\code{D} returns a call and therefore can easily be iterated
for higher derivatives.

\code{deriv} and \code{deriv3} normally return an
\code{\LinkA{expression}{expression}} object whose evaluation returns the function
values with a \code{"gradient"} attribute containing the gradient
matrix.  If \code{hessian} is \code{TRUE} the evaluation also returns
a \code{"hessian"} attribute containing the Hessian array.

If \code{function.arg} is not \code{NULL}, \code{deriv} and
\code{deriv3} return a function with those arguments rather than an
expression.
\end{Value}
%
\begin{References}\relax
Griewank, A.  and  Corliss, G. F. (1991)
\emph{Automatic Differentiation of Algorithms: Theory, Implementation,
and Application}.
SIAM proceedings, Philadelphia.

Bates, D. M. and Chambers, J. M. (1992)
\emph{Nonlinear models.}
Chapter 10 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{nlm}{nlm}} and \code{\LinkA{optim}{optim}} for numeric minimization
which could make use of derivatives,
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## formula argument :
dx2x <- deriv(~ x^2, "x") ; dx2x
## Not run: expression({
         .value <- x^2
         .grad <- array(0, c(length(.value), 1), list(NULL, c("x")))
         .grad[, "x"] <- 2 * x
         attr(.value, "gradient") <- .grad
         .value
})
## End(Not run)
mode(dx2x)
x <- -1:2
eval(dx2x)

## Something 'tougher':
trig.exp <- expression(sin(cos(x + y^2)))
( D.sc <- D(trig.exp, "x") )
all.equal(D(trig.exp[[1]], "x"), D.sc)

( dxy <- deriv(trig.exp, c("x", "y")) )
y <- 1
eval(dxy)
eval(D.sc)

## function returned:
deriv((y ~ sin(cos(x) * y)), c("x","y"), func = TRUE)

## function with defaulted arguments:
(fx <- deriv(y ~ b0 + b1 * 2^(-x/th), c("b0", "b1", "th"),
             function(b0, b1, th, x = 1:7){} ) )
fx(2,3,4)

## Higher derivatives
deriv3(y ~ b0 + b1 * 2^(-x/th), c("b0", "b1", "th"),
     c("b0", "b1", "th", "x") )

## Higher derivatives:
DD <- function(expr,name, order = 1) {
   if(order < 1) stop("'order' must be >= 1")
   if(order == 1) D(expr,name)
   else DD(D(expr, name), name, order - 1)
}
DD(expression(sin(x^2)), "x", 3)
## showing the limits of the internal "simplify()" :
## Not run: 
-sin(x^2) * (2 * x) * 2 + ((cos(x^2) * (2 * x) * (2 * x) + sin(x^2) *
    2) * (2 * x) + sin(x^2) * (2 * x) * 2)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{deviance}{Model Deviance}{deviance}
\keyword{models}{deviance}
%
\begin{Description}\relax
Returns the deviance of a fitted model object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
deviance(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object for which the deviance is desired.
\item[\code{...}] additional optional argument.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function which can be used to extract deviances for
fitted models.  Consult the individual modeling functions for details
on how to use this function.
\end{Details}
%
\begin{Value}
The value of the deviance extracted from the object \code{object}.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S.}
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{df.residual}{df.residual}},
\code{\LinkA{extractAIC}{extractAIC}},
\code{\LinkA{glm}{glm}},
\code{\LinkA{lm}{lm}}.
\end{SeeAlso}
\HeaderA{df.residual}{Residual Degrees-of-Freedom}{df.residual}
\keyword{models}{df.residual}
\keyword{regression}{df.residual}
%
\begin{Description}\relax
Returns the residual degrees-of-freedom extracted from a fitted model
object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
df.residual(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object for which the degrees-of-freedom are desired.
\item[\code{...}] additional optional arguments.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function which can be used to extract residual
degrees-of-freedom for fitted models.  Consult the individual modeling
functions for details on how to use this function.

The default method just extracts the \code{df.residual} component.
\end{Details}
%
\begin{Value}
The value of the residual degrees-of-freedom extracted from the object
\code{x}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{deviance}{deviance}}, \code{\LinkA{glm}{glm}}, \code{\LinkA{lm}{lm}}.
\end{SeeAlso}
\HeaderA{diffinv}{Discrete Integration: Inverse of Differencing}{diffinv}
\methaliasA{diffinv.default}{diffinv}{diffinv.default}
\methaliasA{diffinv.ts}{diffinv}{diffinv.ts}
\keyword{ts}{diffinv}
%
\begin{Description}\relax
Computes the inverse function of the lagged differences function
\code{\LinkA{diff}{diff}}. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
diffinv(x, ...)

## Default S3 method:
diffinv(x, lag = 1, differences = 1, xi, ...)
## S3 method for class 'ts'
diffinv(x, lag = 1, differences = 1, xi, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector, matrix, or time series.
\item[\code{lag}] a scalar lag parameter.
\item[\code{differences}] an integer representing the order of the
difference.
\item[\code{xi}] a numeric vector, matrix, or time series containing the
initial values for the integrals.  If missing, zeros are used.
\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{diffinv} is a generic function with methods for class \code{"ts"}
and \code{default} for vectors and matrices.

Missing values are not handled.
\end{Details}
%
\begin{Value}
A numeric vector, matrix, or time series (the latter for the
\code{"ts"} method) representing the discrete integral of \code{x}. 
\end{Value}
%
\begin{Author}\relax
A. Trapletti
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{diff}{diff}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
s <- 1:10
d <- diff(s)
diffinv(d, xi = 1)
\end{ExampleCode}
\end{Examples}
\HeaderA{dist}{Distance Matrix Computation}{dist}
\aliasA{as.dist}{dist}{as.dist}
\methaliasA{as.dist.default}{dist}{as.dist.default}
\aliasA{as.matrix.dist}{dist}{as.matrix.dist}
\aliasA{format.dist}{dist}{format.dist}
\aliasA{labels.dist}{dist}{labels.dist}
\aliasA{print.dist}{dist}{print.dist}
\keyword{multivariate}{dist}
\keyword{cluster}{dist}
%
\begin{Description}\relax
This function computes and returns the distance matrix computed by
using the specified distance measure to compute the distances between
the rows of a data matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dist(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)

as.dist(m, diag = FALSE, upper = FALSE)
## Default S3 method:
as.dist(m, diag = FALSE, upper = FALSE)

## S3 method for class 'dist'
print(x, diag = NULL, upper = NULL,
      digits = getOption("digits"), justify = "none",
      right = TRUE, ...)

## S3 method for class 'dist'
as.matrix(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric matrix, data frame or \code{"dist"} object.
\item[\code{method}] the distance measure to be used. This must be one of
\code{"euclidean"}, \code{"maximum"}, \code{"manhattan"},
\code{"canberra"}, \code{"binary"} or \code{"minkowski"}.
Any unambiguous substring can be given.
\item[\code{diag}] logical value indicating whether the diagonal of the
distance matrix should be printed by \code{print.dist}.
\item[\code{upper}] logical value indicating whether the upper triangle of the
distance matrix should be printed by \code{print.dist}.
\item[\code{p}] The power of the Minkowski distance.
\item[\code{m}] An object with distance information to be converted to a
\code{"dist"} object.  For the default method, a \code{"dist"}
object, or a matrix (of distances) or an object which can be coerced
to such a matrix using \code{\LinkA{as.matrix}{as.matrix}()}.  (Only the lower
triangle of the matrix is used, the rest is ignored).
\item[\code{digits, justify}] passed to \code{\LinkA{format}{format}} inside of
\code{print()}.
\item[\code{right, ...}] further arguments, passed to other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Available distance measures are (written for two vectors \eqn{x}{} and
\eqn{y}{}):
\begin{description}

\item[\code{euclidean}:] Usual square distance between the two
vectors (2 norm).

\item[\code{maximum}:] Maximum distance between two components of \eqn{x}{}
and \eqn{y}{} (supremum norm)

\item[\code{manhattan}:] Absolute distance between the two vectors
(1 norm).

\item[\code{canberra}:] 
\eqn{\sum_i |x_i - y_i| / |x_i + y_i|}{}.
Terms with zero numerator and denominator are omitted from the sum
and treated as if the values were missing.

This is intended for non-negative values (e.g. counts): taking the
absolute value of the denominator is a 1998 \R{} modification to
avoid negative distances.


\item[\code{binary}:] (aka \emph{asymmetric binary}): The vectors
are regarded as binary bits, so non-zero elements are `on'
and zero elements are `off'.  The distance is the
\emph{proportion} of bits in which only one is on amongst those in
which at least one is on.

\item[\code{minkowski}:] The \eqn{p}{} norm, the \eqn{p}{}th root of the
sum of the \eqn{p}{}th powers of the differences of the components.

\end{description}


Missing values are allowed, and are excluded from all computations
involving the rows within which they occur.
Further, when \code{Inf} values are involved, all pairs of values are
excluded when their contribution to the distance gave \code{NaN} or
\code{NA}.
If some columns are excluded in calculating a Euclidean, Manhattan,
Canberra or Minkowski distance, the sum is scaled up proportionally to
the number of columns used.  If all pairs are excluded when
calculating a particular distance, the value is \code{NA}.

The \code{"dist"} method of \code{as.matrix()} and \code{as.dist()}
can be used for conversion between objects of class \code{"dist"}
and conventional distance matrices.

\code{as.dist()} is a generic function.  Its default method handles
objects inheriting from class \code{"dist"}, or coercible to matrices
using \code{\LinkA{as.matrix}{as.matrix}()}.  Support for classes representing
distances (also known as dissimilarities) can be added by providing an
\code{\LinkA{as.matrix}{as.matrix}()} or, more directly, an \code{as.dist} method
for such a class.
\end{Details}
%
\begin{Value}
\code{dist} returns an object of class \code{"dist"}.

The lower triangle of the distance matrix stored by columns in a
vector, say \code{do}. If \code{n} is the number of
observations, i.e., \code{n <- attr(do, "Size")}, then
for \eqn{i < j \le n}{}, the dissimilarity between (row) i and j is
\code{do[n*(i-1) - i*(i-1)/2 + j-i]}.
The length of the vector is \eqn{n*(n-1)/2}{}, i.e., of order \eqn{n^2}{}.

The object has the following attributes (besides \code{"class"} equal
to \code{"dist"}):
\begin{ldescription}
\item[\code{Size}] integer, the number of observations in the dataset.
\item[\code{Labels}] optionally, contains the labels, if any, of the
observations of the dataset.
\item[\code{Diag, Upper}] logicals corresponding to the arguments \code{diag}
and \code{upper} above, specifying how the object should be printed.
\item[\code{call}] optionally, the \code{\LinkA{call}{call}} used to create the
object.
\item[\code{method}] optionally, the distance method used; resulting from
\code{\LinkA{dist}{dist}()}, the (\code{\LinkA{match.arg}{match.arg}()}ed) \code{method}
argument.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Mardia, K. V., Kent, J. T. and Bibby, J. M. (1979)
\emph{Multivariate Analysis.} Academic Press.

Borg, I. and Groenen, P. (1997)
\emph{Modern Multidimensional Scaling.  Theory and Applications.}
Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{daisy}{daisy}} in the \Rhref{http://CRAN.R-project.org/package=cluster}{\pkg{cluster}} package with more
possibilities in the case of \emph{mixed} (continuous / categorical)
variables.
\code{\LinkA{hclust}{hclust}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

x <- matrix(rnorm(100), nrow=5)
dist(x)
dist(x, diag = TRUE)
dist(x, upper = TRUE)
m <- as.matrix(dist(x))
d <- as.dist(m)
stopifnot(d == dist(x))

## Use correlations between variables "as distance"
dd <- as.dist((1 - cor(USJudgeRatings))/2)
round(1000 * dd) # (prints more nicely)
plot(hclust(dd)) # to see a dendrogram of clustered variables

## example of binary and canberra distances.
x <- c(0, 0, 1, 1, 1, 1)
y <- c(1, 0, 1, 1, 0, 1)
dist(rbind(x,y), method= "binary")
## answer 0.4 = 2/5
dist(rbind(x,y), method= "canberra")
## answer 2 * (6/5)

## To find the names
labels(eurodist)

## Examples involving "Inf" :
## 1)
x[6] <- Inf
(m2 <- rbind(x,y))
dist(m2, method="binary")# warning, answer 0.5 = 2/4
## These all give "Inf":
stopifnot(Inf == dist(m2, method= "euclidean"),
          Inf == dist(m2, method= "maximum"),
          Inf == dist(m2, method= "manhattan"))
##  "Inf" is same as very large number:
x1 <- x; x1[6] <- 1e100
stopifnot(dist(cbind(x ,y), method="canberra") ==
    print(dist(cbind(x1,y), method="canberra")))

## 2)
y[6] <- Inf #-> 6-th pair is excluded
dist(rbind(x,y), method="binary")   # warning; 0.5
dist(rbind(x,y), method="canberra") # 3
dist(rbind(x,y), method="maximum")  # 1
dist(rbind(x,y), method="manhattan")# 2.4
\end{ExampleCode}
\end{Examples}
\HeaderA{Distributions}{Distributions in the stats package}{Distributions}
\aliasA{distribution}{Distributions}{distribution}
\aliasA{distributions}{Distributions}{distributions}
\keyword{distribution}{Distributions}
%
\begin{Description}\relax
Density, cumulative distribution function, quantile function and random
variate generation for many standard probability distributions are
available in the \pkg{stats} package.
\end{Description}
%
\begin{Details}\relax
The functions for the density/mass function, cumulative distribution
function, quantile function and random variate generation are named in the
form \code{dxxx}, \code{pxxx}, \code{qxxx} and \code{rxxx} respectively.

For the beta distribution see \code{\LinkA{dbeta}{dbeta}}.

For the binomial (including Bernoulli) distribution see
\code{\LinkA{dbinom}{dbinom}}.

For the Cauchy distribution see \code{\LinkA{dcauchy}{dcauchy}}.

For the chi-squared distribution see \code{\LinkA{dchisq}{dchisq}}.

For the exponential distribution see \code{\LinkA{dexp}{dexp}}.

For the F distribution see \code{\LinkA{df}{df}}.

For the gamma distribution see \code{\LinkA{dgamma}{dgamma}}.

For the geometric distribution see \code{\LinkA{dgeom}{dgeom}}.  (This is also
a special case of the negative binomial.)

For the hypergeometric distribution see \code{\LinkA{dhyper}{dhyper}}.

For the log-normal distribution see \code{\LinkA{dlnorm}{dlnorm}}.

For the multinomial distribution see \code{\LinkA{dmultinom}{dmultinom}}.

For the negative binomial distribution see \code{\LinkA{dnbinom}{dnbinom}}.

For the normal distribution see \code{\LinkA{dnorm}{dnorm}}.

For the Poisson distribution see \code{\LinkA{dpois}{dpois}}.

For the Student's t distribution see \code{\LinkA{dt}{dt}}.

For the uniform distribution see \code{\LinkA{dunif}{dunif}}.

For the Weibull distribution see \code{\LinkA{dweibull}{dweibull}}.

For less common distributions of test statistics see
\code{\LinkA{pbirthday}{pbirthday}}, \code{\LinkA{dsignrank}{dsignrank}},
\code{\LinkA{ptukey}{ptukey}} and \code{\LinkA{dwilcox}{dwilcox}} (and see the
`See Also' section of \code{\LinkA{cor.test}{cor.test}}).
\end{Details}
%
\begin{SeeAlso}\relax
\code{\LinkA{RNG}{RNG}} about random number generation in \R{}.

The CRAN task view on distributions,
\url{http://cran.r-project.org/web/views/Distributions.html},
mentioning several CRAN packages for additional distributions.
\end{SeeAlso}
\HeaderA{dummy.coef}{Extract Coefficients in Original Coding}{dummy.coef}
\methaliasA{dummy.coef.aovlist}{dummy.coef}{dummy.coef.aovlist}
\methaliasA{dummy.coef.lm}{dummy.coef}{dummy.coef.lm}
\keyword{models}{dummy.coef}
%
\begin{Description}\relax
This extracts coefficients in terms of the original levels of the
coefficients rather than the coded variables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dummy.coef(object, ...)

## S3 method for class 'lm'
dummy.coef(object, use.na = FALSE, ...)

## S3 method for class 'aovlist'
dummy.coef(object, use.na = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a linear model fit.
\item[\code{use.na}] logical flag for coefficients in a singular model. If
\code{use.na} is true, undetermined coefficients will be missing; if
false they will get one possible value.
\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
A fitted linear model has coefficients for the contrasts of the factor
terms, usually one less in number than the number of levels.  This
function re-expresses the coefficients in the original coding; as the
coefficients will have been fitted in the reduced basis, any implied
constraints (e.g., zero sum for \code{contr.helmert} or \code{contr.sum})
will be respected.  There will be little point in using
\code{dummy.coef} for \code{contr.treatment} contrasts, as the missing
coefficients are by definition zero.

The method used has some limitations, and will give incomplete results
for terms such as \code{poly(x, 2)}.  However, it is adequate for
its main purpose, \code{aov} models.
\end{Details}
%
\begin{Value}
A list giving for each term the values of the coefficients. For a
multistratum \code{aov} model, such a list for each stratum.
\end{Value}
%
\begin{Section}{Warning}
This function is intended for human inspection of the
output: it should not be used for calculations.  Use coded variables
for all calculations.

The results differ from S for singular values, where S can be incorrect.
\end{Section}
%
\begin{SeeAlso}\relax
\code{\LinkA{aov}{aov}}, \code{\LinkA{model.tables}{model.tables}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
options(contrasts=c("contr.helmert", "contr.poly"))
## From Venables and Ripley (2002) p.165.
utils::data(npk, package="MASS")
npk.aov <- aov(yield ~ block + N*P*K, npk)
dummy.coef(npk.aov)

npk.aovE <- aov(yield ~  N*P*K + Error(block), npk)
dummy.coef(npk.aovE)
\end{ExampleCode}
\end{Examples}
\HeaderA{ecdf}{Empirical Cumulative Distribution Function}{ecdf}
\aliasA{plot.ecdf}{ecdf}{plot.ecdf}
\aliasA{print.ecdf}{ecdf}{print.ecdf}
\aliasA{quantile.ecdf}{ecdf}{quantile.ecdf}
\aliasA{summary.ecdf}{ecdf}{summary.ecdf}
\keyword{dplot}{ecdf}
\keyword{hplot}{ecdf}
%
\begin{Description}\relax
Compute an empirical cumulative distribution function, with several
methods for plotting, printing and computing with such an
``ecdf'' object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ecdf(x)

## S3 method for class 'ecdf'
plot(x, ..., ylab="Fn(x)", verticals = FALSE,
     col.01line = "gray70", pch = 19)

## S3 method for class 'ecdf'
print(x, digits= getOption("digits") - 2, ...)

## S3 method for class 'ecdf'
summary(object, ...)
## S3 method for class 'ecdf'
quantile(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, object}] numeric vector of the observations for \code{ecdf};  for
the methods, an object inheriting from class \code{"ecdf"}.
\item[\code{...}] arguments to be passed to subsequent methods, e.g.,
\code{\LinkA{plot.stepfun}{plot.stepfun}} for the \code{plot} method.
\item[\code{ylab}] label for the y-axis.
\item[\code{verticals}] see \code{\LinkA{plot.stepfun}{plot.stepfun}}.
\item[\code{col.01line}] numeric or character specifying the color of the
horizontal lines at y = 0 and 1, see \code{\LinkA{colors}{colors}}.
\item[\code{pch}] plotting character.
\item[\code{digits}] number of significant digits to use, see
\code{\LinkA{print}{print}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The e.c.d.f. (empirical cumulative distribution function)
\eqn{F_n}{} is a step function with jumps \eqn{i/n}{} at
observation values, where \eqn{i}{} is the number of tied observations
at that value.  Missing values are ignored.

For observations
\code{x}\eqn{= (}{}\eqn{x_1,x_2}{}, \ldots \eqn{x_n)}{},
\eqn{F_n}{} is the fraction of observations less or equal to \eqn{t}{},
i.e.,
\deqn{F_n(t) = \#\{x_i\le t\}\ / n
               = \frac1 n\sum_{i=1}^n \mathbf{1}_{[x_i \le t]}.}{}

The function \code{plot.ecdf} which implements the \code{\LinkA{plot}{plot}}
method for \code{ecdf} objects, is implemented via a call to
\code{\LinkA{plot.stepfun}{plot.stepfun}}; see its documentation.
\end{Details}
%
\begin{Value}
For \code{ecdf}, a function of class \code{"ecdf"}, inheriting from the
\code{"\LinkA{stepfun}{stepfun}"} class, and hence inheriting a
\code{\LinkA{knots}{knots}()} method.

For the \code{summary} method, a summary of the knots of \code{object}
with a \code{"header"} attribute.

The \code{\LinkA{quantile}{quantile}(obj, ...)} method computes the same quantiles as
\code{quantile(x, ...)} would where \code{x} is the original sample.
\end{Value}
%
\begin{Author}\relax
Martin Maechler, \email{maechler@stat.math.ethz.ch}.\\{}
Corrections by R-core.
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{stepfun}{stepfun}}, the more general class of step functions,
\code{\LinkA{approxfun}{approxfun}} and \code{\LinkA{splinefun}{splinefun}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
##-- Simple didactical  ecdf  example :
x <- rnorm(12)
Fn <- ecdf(x)
Fn     # a *function*
Fn(x)  # returns the percentiles for x
tt <- seq(-2,2, by = 0.1)
12 * Fn(tt) # Fn is a 'simple' function {with values k/12}
summary(Fn)
##--> see below for graphics
knots(Fn)# the unique data values {12 of them if there were no ties}

y <- round(rnorm(12),1); y[3] <- y[1]
Fn12 <- ecdf(y)
Fn12
knots(Fn12)# unique values (always less than 12!)
summary(Fn12)
summary.stepfun(Fn12)

## Advanced: What's inside the function closure?
print(ls.Fn12 <- ls(environment(Fn12)))
##[1] "f"  "method"  "n"  "x"  "y"  "yleft"  "yright"
utils::ls.str(environment(Fn12))
stopifnot(all.equal(quantile(Fn12), quantile(y)))

###----------------- Plotting --------------------------
require(graphics)

op <- par(mfrow=c(3,1), mgp=c(1.5, 0.8,0), mar= .1+c(3,3,2,1))

F10 <- ecdf(rnorm(10))
summary(F10)

plot(F10)
plot(F10, verticals= TRUE, do.points = FALSE)

plot(Fn12 , lwd = 2) ; mtext("lwd = 2", adj=1)
xx <- unique(sort(c(seq(-3, 2, length=201), knots(Fn12))))
lines(xx, Fn12(xx), col='blue')
abline(v=knots(Fn12),lty=2,col='gray70')

plot(xx, Fn12(xx), type='o', cex=.1)#- plot.default {ugly}
plot(Fn12, col.hor='red', add= TRUE)  #- plot method
abline(v=knots(Fn12),lty=2,col='gray70')
## luxury plot
plot(Fn12, verticals=TRUE, col.points='blue',
     col.hor='red', col.vert='bisque')

##-- this works too (automatic call to  ecdf(.)):
plot.ecdf(rnorm(24))
title("via  simple  plot.ecdf(x)", adj=1)

par(op)
\end{ExampleCode}
\end{Examples}
\HeaderA{eff.aovlist}{Compute Efficiencies of Multistratum Analysis of Variance}{eff.aovlist}
\keyword{models}{eff.aovlist}
%
\begin{Description}\relax
Computes the efficiencies of fixed-effect terms in an analysis of
variance model with multiple strata.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
eff.aovlist(aovlist)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{aovlist}] 
The result of a call to \code{aov} with an \code{Error} term.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Fixed-effect terms in an analysis of variance model with multiple strata
may be estimable in more than one stratum, in which case there is less
than complete information in each.  The efficiency for a term
is the fraction of the maximum possible precision (inverse variance)
obtainable by estimating in just that stratum.  Under the assumption
of balance, this is the same for all contrasts involving that term.

This function is used to pick strata in which to estimate terms in
\code{\LinkA{model.tables.aovlist}{model.tables.aovlist}} and
\code{\LinkA{se.contrast.aovlist}{se.contrast.aovlist}}.

In many cases terms will only occur in one stratum, when all the
efficiencies will be one: this is detected and no further calculations
are done.

The calculation used requires orthogonal contrasts for each term, and
will throw an error if non-orthogonal contrasts (e.g. treatment
contrasts or an unbalanced design) are detected.
\end{Details}
%
\begin{Value}
A matrix giving for each non-pure-error stratum (row) the efficiencies
for each fixed-effect term in the model.
\end{Value}
%
\begin{References}\relax
Heiberger, R. M. (1989)
\emph{Computation for the Analysis of Designed Experiments}.  Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{aov}{aov}}, \code{\LinkA{model.tables.aovlist}{model.tables.aovlist}},
\code{\LinkA{se.contrast.aovlist}{se.contrast.aovlist}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## An example from Yates (1932),
## a 2^3 design in 2 blocks replicated 4 times

Block <- gl(8, 4)
A <- factor(c(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,
              0,1,0,1,0,1,0,1,0,1,0,1))
B <- factor(c(0,0,1,1,0,0,1,1,0,1,0,1,1,0,1,0,0,0,1,1,
              0,0,1,1,0,0,1,1,0,0,1,1))
C <- factor(c(0,1,1,0,1,0,0,1,0,0,1,1,0,0,1,1,0,1,0,1,
              1,0,1,0,0,0,1,1,1,1,0,0))
Yield <- c(101, 373, 398, 291, 312, 106, 265, 450, 106, 306, 324, 449,
           272, 89, 407, 338, 87, 324, 279, 471, 323, 128, 423, 334,
           131, 103, 445, 437, 324, 361, 302, 272)
aovdat <- data.frame(Block, A, B, C, Yield)

old <- getOption("contrasts")
options(contrasts=c("contr.helmert", "contr.poly"))
(fit <- aov(Yield ~ A*B*C + Error(Block), data = aovdat))
eff.aovlist(fit)
options(contrasts = old)
\end{ExampleCode}
\end{Examples}
\HeaderA{effects}{Effects from Fitted Model}{effects}
\methaliasA{effects.glm}{effects}{effects.glm}
\methaliasA{effects.lm}{effects}{effects.lm}
\keyword{models}{effects}
\keyword{regression}{effects}
%
\begin{Description}\relax
Returns (orthogonal) effects from a fitted model, usually a linear
model. This is a generic function, but currently only has a methods for
objects inheriting from classes \code{"lm"} and \code{"glm"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
effects(object, ...)

## S3 method for class 'lm'
effects(object, set.sign = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an \R{} object; typically, the result of a model fitting function
such as \code{\LinkA{lm}{lm}}.
\item[\code{set.sign}] logical. If \code{TRUE}, the sign of the effects
corresponding to coefficients in the model will be set to agree with the
signs of the corresponding coefficients, otherwise the sign is
arbitrary.
\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For a linear model fitted by \code{\LinkA{lm}{lm}} or \code{\LinkA{aov}{aov}},
the effects are the uncorrelated single-degree-of-freedom values
obtained by projecting the data onto the successive orthogonal
subspaces generated by the QR decomposition during the fitting
process. The first \eqn{r}{} (the rank of the model) are associated with
coefficients and the remainder span the space of residuals (but are
not associated with particular residuals).

Empty models do not have effects.
\end{Details}
%
\begin{Value}
A (named) numeric vector of the same length as
\code{\LinkA{residuals}{residuals}}, or a matrix if there were multiple responses
in the fitted model, in either case of class \code{"coef"}.

The first \eqn{r}{} rows are labelled by the corresponding coefficients,
and the remaining rows are unlabelled.  Note that in rank-deficient
models the corresponding coefficients will be in a different
order if pivoting occurred.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S.}
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{coef}{coef}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
y <- c(1:3,7,5)
x <- c(1:3,6:7)
( ee <- effects(lm(y ~ x)) )
c( round(ee - effects(lm(y+10 ~ I(x-3.8))), 3) )
# just the first is different
\end{ExampleCode}
\end{Examples}
\HeaderA{embed}{Embedding a Time Series}{embed}
\keyword{ts}{embed}
%
\begin{Description}\relax
Embeds the time series \code{x} into a low-dimensional
Euclidean space.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
embed (x, dimension = 1) 
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector, matrix, or time series.
\item[\code{dimension}] a scalar representing the embedding dimension.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Each row of the resulting matrix consists of sequences
\code{x[t]}, \code{x[t-1]}, \dots, \code{x[t-dimension+1]}, where
\code{t} is the original index of \code{x}. If \code{x} is a matrix,
i.e., \code{x} contains more than one variable, then \code{x[t]}
consists of the \code{t}th observation on each variable.
\end{Details}
%
\begin{Value}
A matrix containing the embedded time series \code{x}.
\end{Value}
%
\begin{Author}\relax
A. Trapletti, B.D. Ripley
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:10
embed (x, 3)
\end{ExampleCode}
\end{Examples}
\HeaderA{expand.model.frame}{Add new variables to a model frame}{expand.model.frame}
\keyword{manip}{expand.model.frame}
\keyword{regression}{expand.model.frame}
%
\begin{Description}\relax
Evaluates new variables as if they had been part of the formula of the
specified model.  This ensures that the same \code{na.action} and
\code{subset} arguments are applied and allows, for example, \code{x}
to be recovered for a model using \code{sin(x)} as a predictor.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
expand.model.frame(model, extras,
                   envir = environment(formula(model)),
                   na.expand = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] a fitted model
\item[\code{extras}] one-sided formula or vector of character strings
describing new variables to be added
\item[\code{envir}] an environment to evaluate things in
\item[\code{na.expand}] logical; see below
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{na.expand=FALSE} then \code{NA} values in the extra variables
will be passed to the \code{na.action} function used in
\code{model}.  This may result in a shorter data frame (with
\code{\LinkA{na.omit}{na.omit}}) or an error (with \code{\LinkA{na.fail}{na.fail}}).  If
\code{na.expand=TRUE} the returned data frame will have precisely the
same rows as \code{model.frame(model)}, but the columns corresponding to
the extra variables may contain \code{NA}.
\end{Details}
%
\begin{Value}
A data frame.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{model.frame}{model.frame}},\code{\LinkA{predict}{predict}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
model <- lm(log(Volume) ~ log(Girth) + log(Height), data=trees)
expand.model.frame(model, ~ Girth) # prints data.frame like

dd <- data.frame(x=1:5, y=rnorm(5), z=c(1,2,NA,4,5))
model <- glm(y ~ x, data=dd, subset=1:4, na.action=na.omit)
expand.model.frame(model, "z", na.expand=FALSE) # = default
expand.model.frame(model, "z", na.expand=TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{Exponential}{The Exponential Distribution}{Exponential}
\aliasA{dexp}{Exponential}{dexp}
\aliasA{pexp}{Exponential}{pexp}
\aliasA{qexp}{Exponential}{qexp}
\aliasA{rexp}{Exponential}{rexp}
\keyword{distribution}{Exponential}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the exponential distribution with rate \code{rate}
(i.e., mean \code{1/rate}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dexp(x, rate = 1, log = FALSE)
pexp(q, rate = 1, lower.tail = TRUE, log.p = FALSE)
qexp(p, rate = 1, lower.tail = TRUE, log.p = FALSE)
rexp(n, rate = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{rate}] vector of rates.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{rate} is not specified, it assumes the default value of
\code{1}.

The exponential distribution with rate \eqn{\lambda}{} has density
\deqn{f(x) = \lambda {e}^{- \lambda x}}{} for \eqn{x \ge 0}{}.
\end{Details}
%
\begin{Value}
\code{dexp} gives the density,
\code{pexp} gives the distribution function,
\code{qexp} gives the quantile function, and
\code{rexp} generates random deviates.
\end{Value}
%
\begin{Note}\relax
The cumulative hazard \eqn{H(t) = - \log(1 - F(t))}{}
is \code{-pexp(t, r, lower = FALSE, log = TRUE)}.
\end{Note}
%
\begin{Source}\relax
\code{dexp}, \code{pexp} and \code{qexp} are all calculated
from numerically stable versions of the definitions.

\code{rexp} uses

Ahrens, J. H. and Dieter, U. (1972).
Computer methods for sampling from the exponential and normal distributions.
\emph{Communications of the ACM}, \bold{15}, 873--882.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 1, chapter 19.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{exp}{exp}} for the exponential function.

\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dgamma}{dgamma}} for the gamma distribution and
\code{\LinkA{dweibull}{dweibull}} for the Weibull distribution, both of which
generalize the exponential.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
dexp(1) - exp(-1) #-> 0
\end{ExampleCode}
\end{Examples}
\HeaderA{extractAIC}{Extract AIC from a Fitted Model}{extractAIC}
\keyword{models}{extractAIC}
%
\begin{Description}\relax
Computes the (generalized) Akaike \bold{A}n \bold{I}nformation
\bold{C}riterion for a fitted parametric model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
extractAIC(fit, scale, k = 2, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{fit}] fitted model, usually the result of a fitter like
\code{\LinkA{lm}{lm}}.
\item[\code{scale}] optional numeric specifying the scale parameter of the
model, see \code{scale} in \code{\LinkA{step}{step}}.  Currently only used
in the \code{"lm"} method, where \code{scale} specifies the estimate
of the error variance, and \code{scale = 0} indicates that it is to
be estimated by maximum likelihood.

\item[\code{k}] numeric specifying the `weight' of the
\emph{equivalent degrees of freedom} (\eqn{\equiv}{} \code{edf})
part in the AIC formula.
\item[\code{...}] further arguments (currently unused in base \R{}).
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function, with methods in base \R{} for classes
\code{"aov"}, \code{"glm"} and \code{"lm"} as well as for
\code{"negbin"} (package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}) and \code{"coxph"} and
\code{"survreg"} (package \Rhref{http://CRAN.R-project.org/package=survival}{\pkg{survival}}).

The criterion used is
\deqn{AIC = - 2\log L +  k \times \mbox{edf},}{}
where \eqn{L}{} is the likelihood and \code{edf} the equivalent degrees
of freedom (i.e., the number of free parameters for usual parametric
models) of \code{fit}.

For linear models with unknown scale (i.e., for \code{\LinkA{lm}{lm}} and
\code{\LinkA{aov}{aov}}), \eqn{-2\log L}{} is computed from the
\emph{deviance} and uses a different additive constant to
\code{\LinkA{logLik}{logLik}} and hence \code{\LinkA{AIC}{AIC}}.  If \eqn{RSS}{}
denotes the (weighted) residual sum of squares then \code{extractAIC}
uses for \eqn{- 2\log L}{} the formulae \eqn{RSS/s - n}{} (corresponding
to Mallows' \eqn{C_p}{}) in the case of known scale \eqn{s}{} and
\eqn{n \log (RSS/n)}{} for unknown scale.
\code{\LinkA{AIC}{AIC}} only handles unknown scale and uses the formula
\eqn{n \log (RSS/n) + n + n \log 2\pi - \sum \log w}{}
where \eqn{w}{} are the weights.  Further \code{AIC} counts the scale
estimation as a parameter in the \code{edf} and \code{extractAIC} does not.

For \code{glm} fits the family's \code{aic()} function is used to
compute the AIC: see the note under \code{logLik} about the
assumptions this makes.

\code{k = 2} corresponds to the traditional AIC, using \code{k =
    log(n)} provides the BIC (Bayesian IC) instead.

Note that the methods for this function may differ in their
assumptions from those of methods for \code{\LinkA{AIC}{AIC}} (usually
\emph{via} a method for \code{\LinkA{logLik}{logLik}}).  We have already
mentioned the case of \code{"lm"} models with estimated scale, and
there are similar issues in the \code{"glm"} and \code{"negbin"}
methods where the dispersion parameter may or may not be taken as
`free'.  This is immaterial as \code{extractAIC} is only used
to compare models of the same class (where only differences in AIC
values are considered).
\end{Details}
%
\begin{Value}
A numeric vector of length 2, with first and second elements giving

\begin{ldescription}
\item[\code{edf}] the `\bold{e}quivalent \bold{d}egrees of \bold{f}reedom'
for the fitted model \code{fit}.

\item[\code{AIC}] the (generalized) Akaike Information Criterion for \code{fit}.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
This function is used in \code{\LinkA{add1}{add1}}, \code{\LinkA{drop1}{drop1}}
and \code{\LinkA{step}{step}} and the similar functions in package
\Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} from which it was adopted.
\end{Note}
%
\begin{Author}\relax
B. D. Ripley
\end{Author}
%
\begin{References}\relax
Venables, W. N. and Ripley, B. D. (2002)
\emph{Modern Applied Statistics with S.}
New York: Springer (4th ed).
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{AIC}{AIC}}, \code{\LinkA{deviance}{deviance}}, \code{\LinkA{add1}{add1}},
\code{\LinkA{step}{step}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

utils::example(glm)
extractAIC(glm.D93)  #>>  5  15.129
\end{ExampleCode}
\end{Examples}
\HeaderA{factanal}{Factor Analysis}{factanal}
\keyword{multivariate}{factanal}
%
\begin{Description}\relax
Perform maximum-likelihood factor analysis on a covariance matrix or
data matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
factanal(x, factors, data = NULL, covmat = NULL, n.obs = NA,
         subset, na.action, start = NULL,
         scores = c("none", "regression", "Bartlett"),
         rotation = "varimax", control = NULL, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A formula or a numeric matrix or an object that can be
coerced to a numeric matrix.
\item[\code{factors}] The number of factors to be fitted.
\item[\code{data}] An optional data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}), used only if \code{x} is a formula.  By
default the variables are taken from \code{environment(formula)}.
\item[\code{covmat}] A covariance matrix, or a covariance list as returned by
\code{\LinkA{cov.wt}{cov.wt}}.  Of course, correlation matrices are covariance
matrices.
\item[\code{n.obs}] The number of observations, used if \code{covmat} is a
covariance matrix.
\item[\code{subset}] A specification of the cases to be used, if \code{x} is
used as a matrix or formula.
\item[\code{na.action}] The \code{na.action} to be used if \code{x} is
used as a formula.
\item[\code{start}] \code{NULL} or a matrix of starting values, each column
giving an initial set of uniquenesses.
\item[\code{scores}] Type of scores to produce, if any.  The default is none,
\code{"regression"} gives Thompson's scores, \code{"Bartlett"} given
Bartlett's weighted least-squares scores. Partial matching allows
these names to be abbreviated.
\item[\code{rotation}] character. \code{"none"} or the name of a function
to be used to rotate the factors: it will be called with first
argument the loadings matrix, and should return a list with component
\code{loadings} giving the rotated loadings, or just the rotated loadings.
\item[\code{control}] A list of control values,
\begin{description}

\item[nstart] The number of starting values to be tried if
\code{start = NULL}. Default 1.
\item[trace] logical. Output tracing information? Default \code{FALSE}.
\item[lower] The lower bound for uniquenesses during
optimization. Should be > 0. Default 0.005.
\item[opt] A list of control values to be passed to
\code{\LinkA{optim}{optim}}'s \code{control} argument.
\item[rotate] a list of additional arguments for the rotation function.

\end{description}


\item[\code{...}] Components of \code{control} can also be supplied as
named arguments to \code{factanal}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The factor analysis model is
\deqn{x = \Lambda f + e}{}
for a \eqn{p}{}--element row-vector \eqn{x}{}, a \eqn{p \times k}{}
matrix \eqn{\Lambda}{} of \emph{loadings}, a \eqn{k}{}--element vector
\eqn{f}{} of \emph{scores} and a \eqn{p}{}--element vector \eqn{e}{}of
errors.  None of the components other than \eqn{x}{} is observed, but
the major restriction is that the scores be uncorrelated and of unit
variance, and that the errors be independent with variances
\eqn{\Psi}{}, the \emph{uniquenesses}.  It is also common to
scale the observed variables to unit variance, and done in this function.

Thus factor analysis is in essence a model for the correlation matrix
of \eqn{x}{},
\deqn{\Sigma = \Lambda^\prime\Lambda + \Psi}{}
There is still some indeterminacy in the model for it is unchanged
if \eqn{\Lambda}{} is replaced by \eqn{G \Lambda}{} for
any orthogonal matrix \eqn{G}{}.  Such matrices \eqn{G}{} are known as
\emph{rotations} (although the term is applied also to non-orthogonal
invertible matrices).

If \code{covmat} is supplied it is used.  Otherwise \code{x} is used
if it is a matrix, or a formula \code{x} is used with \code{data} to
construct a model matrix, and that is used to construct a covariance
matrix.  (It makes no sense for the formula to have a response, and
all the variables must be numeric.)  Once a covariance matrix is found
or calculated from \code{x}, it is converted to a correlation matrix
for analysis.  The correlation matrix is returned as component
\code{correlation} of the result.

The fit is done by optimizing the log likelihood assuming multivariate
normality over the uniquenesses.  (The maximizing loadings for given
uniquenesses can be found analytically: Lawley \& Maxwell (1971,
p. 27).)  All the starting values supplied in \code{start} are tried
in turn and the best fit obtained is used.  If \code{start = NULL}
then the first fit is started at the value suggested by
JÃ¶reskog (1963) and given by Lawley \& Maxwell
(1971, p. 31), and then \code{control\$nstart - 1} other values are
tried, randomly selected as equal values of the uniquenesses.

The uniquenesses are technically constrained to lie in \eqn{[0, 1]}{},
but near-zero values are problematical, and the optimization is
done with a lower bound of \code{control\$lower}, default 0.005
(Lawley \& Maxwell, 1971, p. 32).

Scores can only be produced if a data matrix is supplied and used.
The first method is the regression method of Thomson (1951), the
second the weighted least squares method of Bartlett (1937, 8).
Both are estimates of the unobserved scores \eqn{f}{}.  Thomson's method
regresses (in the population) the unknown \eqn{f}{} on \eqn{x}{} to yield
\deqn{\hat f = \Lambda^\prime \Sigma^{-1} x}{}
and then substitutes the sample estimates of the quantities on the
right-hand side.  Bartlett's method minimizes the sum of squares of
standardized errors over the choice of \eqn{f}{}, given (the fitted)
\eqn{\Lambda}{}.

If \code{x} is a formula then the standard \code{NA}-handling is
applied to the scores (if requested): see \code{\LinkA{napredict}{napredict}}.

The \code{print} method (documented under \code{\LinkA{loadings}{loadings}})
follows the factor analysis convention of drawing attention to the
patterns of the results, so the default precision is three decimal
places, and small loadings are suppressed.
\end{Details}
%
\begin{Value}
An object of class \code{"factanal"} with components
\begin{ldescription}
\item[\code{loadings}] A matrix of loadings, one column for each factor.  The
factors are ordered in decreasing order of sums of squares of
loadings, and given the sign that will make the sum of the loadings
positive.  This is of class \code{"loadings"}: see
\code{\LinkA{loadings}{loadings}} for its \code{print} method.
\item[\code{uniquenesses}] The uniquenesses computed.
\item[\code{correlation}] The correlation matrix used.
\item[\code{criteria}] The results of the optimization: the value of the
negative log-likelihood and information on the iterations used.
\item[\code{factors}] The argument \code{factors}.
\item[\code{dof}] The number of degrees of freedom of the factor analysis model.
\item[\code{method}] The method: always \code{"mle"}.
\item[\code{rotmat}] The rotation matrix if relevant.
\item[\code{scores}] If requested, a matrix of scores.  \code{napredict} is
applied to handle the treatment of values omitted by the \code{na.action}.
\item[\code{n.obs}] The number of observations if available, or \code{NA}.
\item[\code{call}] The matched call.
\item[\code{na.action}] If relevant.
\item[\code{STATISTIC, PVAL}] The significance-test statistic and P value, if
it can be computed.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
There are so many variations on factor analysis that it is hard to
compare output from different programs.  Further, the optimization in
maximum likelihood factor analysis is hard, and many other examples we
compared had less good fits than produced by this function.  In
particular, solutions which are `Heywood cases' (with one or more
uniquenesses essentially zero) are much more common than most texts
and some other programs would lead one to believe.
\end{Note}
%
\begin{References}\relax
Bartlett, M. S. (1937) The statistical conception of mental factors.
\emph{British Journal of Psychology}, \bold{28}, 97--104.

Bartlett, M. S. (1938) Methods of estimating mental
factors. \emph{Nature}, \bold{141}, 609--610.

JÃ¶reskog, K. G. (1963)
\emph{Statistical Estimation in Factor Analysis.}  Almqvist and Wicksell.

Lawley, D. N. and Maxwell, A. E. (1971) \emph{Factor Analysis as a
Statistical Method.} Second edition. Butterworths.

Thomson, G. H. (1951) \emph{The Factorial Analysis of Human Ability.}
London University Press.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{loadings}{loadings}} (which explains some details of the
\code{print} method), \code{\LinkA{varimax}{varimax}}, \code{\LinkA{princomp}{princomp}},
\code{\LinkA{ability.cov}{ability.cov}}, \code{\LinkA{Harman23.cor}{Harman23.cor}},
\code{\LinkA{Harman74.cor}{Harman74.cor}}.

Other rotation methods are available in various contributed packages,
including \Rhref{http://CRAN.R-project.org/package=GPArotation}{\pkg{GPArotation}} and \Rhref{http://CRAN.R-project.org/package=psych}{\pkg{psych}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# A little demonstration, v2 is just v1 with noise,
# and same for v4 vs. v3 and v6 vs. v5
# Last four cases are there to add noise
# and introduce a positive manifold (g factor)
v1 <- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6)
v2 <- c(1,2,1,1,1,1,2,1,2,1,3,4,3,3,3,4,6,5)
v3 <- c(3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6)
v4 <- c(3,3,4,3,3,1,1,2,1,1,1,1,2,1,1,5,6,4)
v5 <- c(1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5)
v6 <- c(1,1,1,2,1,3,3,3,4,3,1,1,1,2,1,6,5,4)
m1 <- cbind(v1,v2,v3,v4,v5,v6)
cor(m1)
factanal(m1, factors = 3) # varimax is the default
factanal(m1, factors = 3, rotation = "promax")
# The following shows the g factor as PC1
prcomp(m1) # signs may depend on platform

## formula interface
factanal(~v1+v2+v3+v4+v5+v6, factors = 3,
         scores = "Bartlett")$scores

## a realistic example from Bartholomew (1987, pp. 61-65)
utils::example(ability.cov)
\end{ExampleCode}
\end{Examples}
\HeaderA{factor.scope}{Compute Allowed Changes in Adding to or Dropping from a Formula}{factor.scope}
\aliasA{add.scope}{factor.scope}{add.scope}
\aliasA{drop.scope}{factor.scope}{drop.scope}
\keyword{models}{factor.scope}
%
\begin{Description}\relax
\code{add.scope} and \code{drop.scope} compute those terms that can be
individually added to or dropped from a model while respecting the
hierarchy of terms.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
add.scope(terms1, terms2)

drop.scope(terms1, terms2)

factor.scope(factor, scope)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{terms1}] the terms or formula for the base model.
\item[\code{terms2}] the terms or formula for the upper (\code{add.scope}) or
lower (\code{drop.scope}) scope. If missing for \code{drop.scope} it is
taken to be the null formula, so all terms (except any intercept) are
candidates to be dropped.
\item[\code{factor}] the \code{"factor"} attribute of the terms of the base object.
\item[\code{scope}] a list with one or both components \code{drop} and
\code{add} giving the \code{"factor"} attribute of the lower and
upper scopes respectively.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{factor.scope} is not intended to be called directly by users.
\end{Details}
%
\begin{Value}
For \code{add.scope} and \code{drop.scope} a character vector of
terms labels.  For \code{factor.scope}, a list with components
\code{drop} and \code{add}, character vectors of terms labels.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{add1}{add1}}, \code{\LinkA{drop1}{drop1}},
\code{\LinkA{aov}{aov}}, \code{\LinkA{lm}{lm}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
add.scope( ~ a + b + c + a:b,  ~ (a + b + c)^3)
# [1] "a:c" "b:c"
drop.scope( ~ a + b + c + a:b)
# [1] "c"   "a:b"
\end{ExampleCode}
\end{Examples}
\HeaderA{family}{Family Objects for Models}{family}
\aliasA{binomial}{family}{binomial}
\aliasA{Gamma}{family}{Gamma}
\aliasA{gaussian}{family}{gaussian}
\aliasA{inverse.gaussian}{family}{inverse.gaussian}
\aliasA{poisson}{family}{poisson}
\aliasA{print.family}{family}{print.family}
\aliasA{quasi}{family}{quasi}
\aliasA{quasibinomial}{family}{quasibinomial}
\aliasA{quasipoisson}{family}{quasipoisson}
\keyword{models}{family}
%
\begin{Description}\relax
Family objects provide a convenient way to specify the details of the
models used by functions such as \code{\LinkA{glm}{glm}}.  See the
documentation for \code{\LinkA{glm}{glm}} for the details on how such model
fitting takes place.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
family(object, ...)

binomial(link = "logit")
gaussian(link = "identity")
Gamma(link = "inverse")
inverse.gaussian(link = "1/mu^2")
poisson(link = "log")
quasi(link = "identity", variance = "constant")
quasibinomial(link = "logit")
quasipoisson(link = "log")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{link}] a specification for the model link function.  This can be
a name/expression, a literal character string, a length-one character
vector or an object of class
\code{"\LinkA{link-glm}{make.link}"} (such as generated by
\code{\LinkA{make.link}{make.link}}) provided it is not specified
\emph{via} one of the standard names given next.

The \code{gaussian} family accepts the links (as names)
\code{identity}, \code{log} and \code{inverse};
the \code{binomial} family the links \code{logit},
\code{probit}, \code{cauchit}, (corresponding to logistic,
normal and Cauchy CDFs respectively) \code{log} and
\code{cloglog} (complementary log-log);
the \code{Gamma} family the links \code{inverse}, \code{identity}
and \code{log};
the \code{poisson} family the links \code{log}, \code{identity},
and \code{sqrt} and the \code{inverse.gaussian} family the links
\code{1/mu\textasciicircum{}2}, \code{inverse}, \code{identity}
and \code{log}.

The \code{quasi} family accepts the links \code{logit}, \code{probit},
\code{cloglog},  \code{identity}, \code{inverse},
\code{log}, \code{1/mu\textasciicircum{}2} and \code{sqrt}, and
the function \code{\LinkA{power}{power}} can be used to create a
power link function.

\item[\code{variance}] for all families other than \code{quasi}, the variance
function is determined by the family.  The \code{quasi} family will
accept the literal character string (or unquoted as a name/expression)
specifications \code{"constant"}, \code{"mu(1-mu)"}, \code{"mu"},
\code{"mu\textasciicircum{}2"} and \code{"mu\textasciicircum{}3"}, a length-one character vector
taking one of those values, or a list containing components
\code{varfun}, \code{validmu}, \code{dev.resids}, \code{initialize}
and \code{name}.

\item[\code{object}] the function \code{family} accesses the \code{family}
objects which are stored within objects created by modelling
functions (e.g., \code{glm}).
\item[\code{...}] further arguments passed to methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{family} is a generic function with methods for classes
\code{"glm"} and \code{"lm"} (the latter returning \code{gaussian()}).

The \code{quasibinomial} and \code{quasipoisson} families differ from
the \code{binomial} and \code{poisson} families only in that the
dispersion parameter is not fixed at one, so they can model
over-dispersion.  For the binomial case see McCullagh and Nelder
(1989, pp. 124--8).  Although they show that there is (under some
restrictions) a model with
variance proportional to mean as in the quasi-binomial model, note
that \code{glm} does not compute maximum-likelihood estimates in that
model.  The behaviour of S is closer to the quasi- variants.
\end{Details}
%
\begin{Value}
An object of class \code{"family"} (which has a concise print method).
This is a list with elements
\begin{ldescription}
\item[\code{family}] character: the family name.
\item[\code{link}] character: the link name.
\item[\code{linkfun}] function: the link.
\item[\code{linkinv}] function: the inverse of the link function.
\item[\code{variance}] function: the variance as a function of the mean.
\item[\code{dev.resids}] function giving the deviance residuals as a function
of \code{(y, mu, wt)}.
\item[\code{aic}] function giving the AIC value if appropriate (but \code{NA}
for the quasi- families).  See \code{\LinkA{logLik}{logLik}} for the assumptions
made about the dispersion parameter.
\item[\code{mu.eta}] function: derivative \code{function(eta)}
\eqn{d\mu/d\eta}{}.
\item[\code{initialize}] expression.  This needs to set up whatever data
objects are needed for the family as well as \code{n} (needed for
AIC in the binomial family) and \code{mustart} (see \code{\LinkA{glm}{glm}}.
\item[\code{valid.mu}] logical function.  Returns \code{TRUE} if a mean
vector \code{mu} is within the domain of \code{variance}.
\item[\code{valid.eta}] logical function.   Returns \code{TRUE} if a linear
predictor \code{eta} is within the domain of \code{linkinv}.
\item[\code{simulate}] (optional) function \code{simulate(object, nsim)} to be
called by the \code{"lm"} method of \code{\LinkA{simulate}{simulate}}.  It will
normally return a matrix with \code{nsim} columns and one row for
each fitted value, but it can also return a list of length
\code{nsim}. Clearly this will be missing for `quasi-' families.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The \code{link} and \code{variance} arguments have rather awkward
semantics for back-compatibility.  The recommended way is to supply
them is as quoted character strings, but they can also be supplied
unquoted (as names or expressions).  In addition, they can also be
supplied as a length-one character vector giving the name of one of
the options, or as a list (for \code{link}, of class
\code{"link-glm"}).  The restrictions apply only to links given as
names: when given as a character string all the links known to
\code{\LinkA{make.link}{make.link}} are accepted.

This is potentially ambiguous: supplying \code{link=logit} could mean
the unquoted name of a link or the value of object \code{logit}.  It
is interpreted if possible as the name of an allowed link, then
as an object.  (You can force the interpretation to always be the value of
an object via \code{logit[1]}.)
\end{Note}
%
\begin{Author}\relax
The design was inspired by S functions of the same names described
in Hastie \& Pregibon (1992) (except \code{quasibinomial} and
\code{quasipoisson}).
\end{Author}
%
\begin{References}\relax
McCullagh P. and Nelder, J. A. (1989)
\emph{Generalized Linear Models.}
London: Chapman and Hall.

Dobson, A. J. (1983)
\emph{An Introduction to Statistical Modelling.}
London: Chapman and Hall.

Cox, D. R. and  Snell, E. J. (1981).
\emph{Applied Statistics; Principles and Examples.}
London: Chapman and Hall.

Hastie, T. J. and Pregibon, D. (1992)
\emph{Generalized linear models.}
Chapter 6 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{glm}{glm}}, \code{\LinkA{power}{power}}, \code{\LinkA{make.link}{make.link}}.

For binomial \emph{coefficients}, \code{\LinkA{choose}{choose}};
the binomial and negative binomial \emph{distributions},
\code{\LinkA{Binomial}{Binomial}}, and \code{\LinkA{NegBinomial}{NegBinomial}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(utils) # for str

nf <- gaussian()# Normal family
nf
str(nf)# internal STRucture

gf <- Gamma()
gf
str(gf)
gf$linkinv
gf$variance(-3:4) #- == (.)^2


## quasipoisson. compare with example(glm)
counts <- c(18,17,15,20,10,20,25,13,12)
outcome <- gl(3,1,9)
treatment <- gl(3,3)
d.AD <- data.frame(treatment, outcome, counts)
glm.qD93 <- glm(counts ~ outcome + treatment, family=quasipoisson())

glm.qD93
anova(glm.qD93, test="F")
summary(glm.qD93)
## for Poisson results use
anova(glm.qD93, dispersion = 1, test="Chisq")
summary(glm.qD93, dispersion = 1)


## Example of user-specified link, a logit model for p^days
## See Shaffer, T.  2004. Auk 121(2): 526-540.
logexp <- function(days = 1)
{
    linkfun <- function(mu) qlogis(mu^(1/days))
    linkinv <- function(eta) plogis(eta)^days
    mu.eta <- function(eta) days * plogis(eta)^(days-1) * binomial()$mu_eta
    valideta <- function(eta) TRUE
    link <- paste0("logexp(", days, ")")
    structure(list(linkfun = linkfun, linkinv = linkinv,
                   mu.eta = mu.eta, valideta = valideta, name = link),
              class = "link-glm")
}
binomial(logexp(3))
## in practice this would be used with a vector of 'days', in
## which case use an offset of 0 in the corresponding formula
## to get the null deviance right.

## Binomial with identity link: often not a good idea.
## Not run: binomial(link=make.link("identity"))

## tests of quasi
x <- rnorm(100)
y <- rpois(100, exp(1+x))
glm(y ~x, family=quasi(variance="mu", link="log"))
# which is the same as
glm(y ~x, family=poisson)
glm(y ~x, family=quasi(variance="mu^2", link="log"))
## Not run: glm(y ~x, family=quasi(variance="mu^3", link="log")) # fails
y <- rbinom(100, 1, plogis(x))
# needs to set a starting value for the next fit
glm(y ~x, family=quasi(variance="mu(1-mu)", link="logit"), start=c(0,1))
\end{ExampleCode}
\end{Examples}
\HeaderA{FDist}{The F Distribution}{FDist}
\aliasA{df}{FDist}{df}
\aliasA{pf}{FDist}{pf}
\aliasA{qf}{FDist}{qf}
\aliasA{rf}{FDist}{rf}
\keyword{distribution}{FDist}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the F distribution with \code{df1} and \code{df2}
degrees of freedom (and optional non-centrality parameter \code{ncp}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
df(x, df1, df2, ncp, log = FALSE)
pf(q, df1, df2, ncp, lower.tail = TRUE, log.p = FALSE)
qf(p, df1, df2, ncp, lower.tail = TRUE, log.p = FALSE)
rf(n, df1, df2, ncp)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{df1, df2}] degrees of freedom.  \code{Inf} is allowed.
\item[\code{ncp}] non-centrality parameter. If omitted the central F is assumed.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The F distribution with \code{df1 =} \eqn{n_1}{} and \code{df2 =}
\eqn{n_2}{} degrees of freedom has density
\deqn{
    f(x) = \frac{\Gamma(n_1/2 + n_2/2)}{\Gamma(n_1/2)\Gamma(n_2/2)}
    \left(\frac{n_1}{n_2}\right)^{n_1/2} x^{n_1/2 -1}
    \left(1 + \frac{n_1 x}{n_2}\right)^{-(n_1 + n_2) / 2}%
  }{}
for \eqn{x > 0}{}.

It is the distribution of the ratio of the mean squares of
\eqn{n_1}{} and \eqn{n_2}{} independent standard normals, and hence
of the ratio of two independent chi-squared variates each divided by its
degrees of freedom.  Since the ratio of a normal and the root
mean-square of \eqn{m}{} independent normals has a Student's \eqn{t_m}{}
distribution, the square of a \eqn{t_m}{} variate has a F distribution on
1 and \eqn{m}{} degrees of freedom.

The non-central F distribution is again the ratio of mean squares of
independent normals of unit variance, but those in the numerator are
allowed to have non-zero means and \code{ncp} is the sum of squares of
the means.  See \LinkA{Chisquare}{Chisquare} for further details on
non-central distributions.
\end{Details}
%
\begin{Value}
\code{df} gives the density,
\code{pf} gives the distribution function
\code{qf} gives the quantile function, and
\code{rf} generates random deviates.

Invalid arguments will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Note}\relax
Supplying \code{ncp = 0} uses the algorithm for the non-central
distribution, which is not the same algorithm used if \code{ncp} is
omitted.  This is to give consistent behaviour in extreme cases with
values of \code{ncp} very near zero.

The code for non-zero \code{ncp} is principally intended to be used
for moderate values of \code{ncp}: it will not be highly accurate,
especially in the tails, for large values.
\end{Note}
%
\begin{Source}\relax
For the central case of \code{df}, computed \emph{via} a binomial
probability, code contributed by Catherine Loader (see
\code{\LinkA{dbinom}{dbinom}}); for the non-central case computed \emph{via}
\code{\LinkA{dbeta}{dbeta}}, code contributed by Peter Ruckdeschel.

For \code{pf}, \emph{via} \code{\LinkA{pbeta}{pbeta}} (or for large
\code{df2}, \emph{via} \code{\LinkA{pchisq}{pchisq}}).

For \code{qf}, \emph{via} \code{\LinkA{qchisq}{qchisq}} for large \code{df2},
else \emph{via} \code{\LinkA{qbeta}{qbeta}}.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 2, chapters 27 and 30.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dchisq}{dchisq}} for chi-squared and \code{\LinkA{dt}{dt}} for Student's
t distributions.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## the density of the square of a t_m is 2*dt(x, m)/(2*x)
# check this is the same as the density of F_{1,m}
x <- seq(0.001, 5, len=100)
all.equal(df(x^2, 1, 5), dt(x, 5)/x)

## Identity:  qf(2*p - 1, 1, df)) == qt(p, df)^2)  for  p >= 1/2
p <- seq(1/2, .99, length=50); df <- 10
rel.err <- function(x,y) ifelse(x==y,0, abs(x-y)/mean(abs(c(x,y))))
quantile(rel.err(qf(2*p - 1, df1=1, df2=df), qt(p, df)^2), .90)# ~= 7e-9
\end{ExampleCode}
\end{Examples}
\HeaderA{fft}{Fast Discrete Fourier Transform}{fft}
\aliasA{mvfft}{fft}{mvfft}
\keyword{math}{fft}
\keyword{dplot}{fft}
%
\begin{Description}\relax
Performs the Fast Fourier Transform of an array.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fft(z, inverse = FALSE)
mvfft(z, inverse = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{z}] a real or complex array containing the values to be
transformed.
\item[\code{inverse}] if \code{TRUE}, the unnormalized inverse transform is
computed (the inverse has a \code{+} in the exponent of \eqn{e}{},
but here, we do \emph{not} divide by \code{1/length(x)}).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
When \code{z} is a vector, the value computed and returned by
\code{fft} is the unnormalized univariate Fourier transform of the
sequence of values in \code{z}.




When \code{z} contains an array, \code{fft} computes and returns the
multivariate (spatial) transform.  If \code{inverse} is \code{TRUE},
the (unnormalized) inverse Fourier transform is returned, i.e.,
if \code{y <- fft(z)}, then \code{z} is
\code{fft(y, inverse = TRUE) / length(y)}.

By contrast, \code{mvfft} takes a real or complex matrix as argument,
and returns a similar shaped matrix, but with each column replaced by
its discrete Fourier transform.  This is useful for analyzing
vector-valued series.

The FFT is fastest when the length of the series being transformed
is highly composite (i.e., has many factors).  If this is not the
case, the transform may take a long time to compute and will use a
large amount of memory.
\end{Value}
%
\begin{Source}\relax
Uses C translation of Fortran code in Singleton (1979).
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Singleton, R. C. (1979)
Mixed Radix Fast Fourier Transforms,
in \emph{Programs for Digital Signal Processing},
IEEE Digital Signal Processing Committee eds.
IEEE Press.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{convolve}{convolve}}, \code{\LinkA{nextn}{nextn}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:4
fft(x)
fft(fft(x), inverse = TRUE)/length(x)
\end{ExampleCode}
\end{Examples}
\HeaderA{filter}{Linear Filtering on a Time Series}{filter}
\keyword{ts}{filter}
%
\begin{Description}\relax
Applies linear filtering to a univariate time series or to each series
separately of a multivariate time series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
filter(x, filter, method = c("convolution", "recursive"),
       sides = 2, circular = FALSE, init)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a univariate or multivariate time series.

\item[\code{filter}] a vector of filter coefficients in reverse time order
(as for AR or MA coefficients).

\item[\code{method}] Either \code{"convolution"} or \code{"recursive"} (and
can be abbreviated). If \code{"convolution"} a moving average is
used: if \code{"recursive"} an autoregression is used.

\item[\code{sides}] for convolution filters only. If \code{sides = 1} the
filter coefficients are for past values only; if \code{sides = 2}
they are centred around lag 0.  In this case the length of the
filter should be odd, but if it is even, more of the filter
is forward in time than backward.

\item[\code{circular}] for convolution filters only.  If \code{TRUE}, wrap
the filter around the ends of the series, otherwise assume
external values are missing (\code{NA}).

\item[\code{init}] for recursive filters only. Specifies the initial values
of the time series just prior to the start value, in reverse
time order. The default is a set of zeros.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Missing values are allowed in \code{x} but not in \code{filter}
(where they would lead to missing values everywhere in the output).

Note that there is an implied coefficient 1 at lag 0 in the
recursive filter, which gives
\deqn{y_i = x_i + f_1y_{i-1} + \cdots + f_py_{i-p}}{}
No check is made to see if recursive filter is invertible:
the output may diverge if it is not.

The convolution filter is
\deqn{y_i = f_1x_{i+o} + \cdots + f_px_{i+o-(p-1)}}{}

where \code{o} is the offset: see \code{sides} for how it is determined.
\end{Details}
%
\begin{Value}
A time series object.
\end{Value}
%
\begin{Note}\relax
\code{\LinkA{convolve}{convolve}(, type="filter")} uses the FFT for computations
and so \emph{may} be faster for long filters on univariate series,
but it does not return a time series (and so the  time alignment is
unclear), nor does it handle missing values.  \code{filter} is
faster for a filter of length 100 on a series of length 1000,
for example.
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{convolve}{convolve}}, \code{\LinkA{arima.sim}{arima.sim}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:100
filter(x, rep(1, 3))
filter(x, rep(1, 3), sides = 1)
filter(x, rep(1, 3), sides = 1, circular = TRUE)

filter(presidents, rep(1,3))
\end{ExampleCode}
\end{Examples}
\HeaderA{fisher.test}{Fisher's Exact Test for Count Data}{fisher.test}
\keyword{htest}{fisher.test}
%
\begin{Description}\relax
Performs Fisher's exact test for testing the null of independence of
rows and columns in a contingency table with fixed marginals.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fisher.test(x, y = NULL, workspace = 200000, hybrid = FALSE,
            control = list(), or = 1, alternative = "two.sided",
            conf.int = TRUE, conf.level = 0.95,
            simulate.p.value = FALSE, B = 2000)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] either a two-dimensional contingency table in matrix form,
or a factor object.
\item[\code{y}] a factor object; ignored if \code{x} is a matrix.
\item[\code{workspace}] an integer specifying the size of the workspace
used in the network algorithm.  In units of 4 bytes.  Only used for
non-simulated p-values larger than \eqn{2 \times 2}{} tables.
\item[\code{hybrid}] a logical. Only used for larger than \eqn{2 \times 2}{}
tables, in which cases it indicates whether the exact probabilities
(default) or a hybrid approximation thereof should be computed.
See `Details'.
\item[\code{control}] a list with named components for low level algorithm
control.  At present the only one used is \code{"mult"}, a positive
integer \eqn{\ge 2}{} with default 30 used only for larger than
\eqn{2 \times 2}{} tables.  This says how many times as much
space should be allocated to paths as to keys: see file
\file{fexact.c} in the sources of this package.
\item[\code{or}] the hypothesized odds ratio.  Only used in the
\eqn{2 \times 2}{} case.
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"}, \code{"greater"} or \code{"less"}.
You can specify just the initial letter.  Only used in the
\eqn{2 \times 2}{} case.
\item[\code{conf.int}] logical indicating if a confidence interval for the
odds ratio in a \eqn{2 \times 2}{} table should be
computed (and returned).
\item[\code{conf.level}] confidence level for the returned confidence
interval.  Only used in the \eqn{2 \times 2}{} case and if
\code{conf.int = TRUE}.
\item[\code{simulate.p.value}] a logical indicating whether to compute
p-values by Monte Carlo simulation, in larger than \eqn{2 \times
      2}{} tables.
\item[\code{B}] an integer specifying the number of replicates used in the
Monte Carlo test.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{x} is a matrix, it is taken as a two-dimensional contingency
table, and hence its entries should be nonnegative integers.
Otherwise, both \code{x} and \code{y} must be vectors of the same
length.  Incomplete cases are removed, the vectors are coerced into
factor objects, and the contingency table is computed from these.

For \eqn{2 \times 2}{} cases, p-values are obtained directly
using the (central or non-central) hypergeometric
distribution. Otherwise, computations are based on a C version of the
FORTRAN subroutine FEXACT which implements the network developed by
Mehta and Patel (1986) and improved by Clarkson, Fan and Joe (1993).
The FORTRAN code can be obtained from
\url{http://www.netlib.org/toms/643}.  Note this fails (with an error
message) when the entries of the table are too large.  (It transposes
the table if necessary so it has no more rows than columns.  One
constraint is that the product of the row marginals be less than
\eqn{2^{31} - 1}{}.)

For \eqn{2 \times 2}{} tables, the null of conditional
independence is equivalent to the hypothesis that the odds ratio
equals one.  `Exact' inference can be based on observing that in
general, given all marginal totals fixed, the first element of the
contingency table has a non-central hypergeometric distribution with
non-centrality parameter given by the odds ratio (Fisher, 1935).  The
alternative for a one-sided test is based on the odds ratio, so
\code{alternative = "greater"} is a test of the odds ratio being bigger
than \code{or}.

Two-sided tests are based on the probabilities of the tables, and take
as `more extreme' all tables with probabilities less than or
equal to that of the observed table, the p-value being the sum of such
probabilities.

For larger than  \eqn{2 \times 2}{} tables and \code{hybrid =
    TRUE}, asymptotic chi-squared probabilities are only used if the
`Cochran conditions' are satisfied, that is if no cell has
count zero, and more than 80\% of the cells have counts at least 5:
otherwise the exact calculation is used.

Simulation is done conditional on the row and column marginals, and
works only if the marginals are strictly positive.  (A C translation
of the algorithm of Patefield (1981) is used.)
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{p.value}] the p-value of the test.
\item[\code{conf.int}] a confidence interval for the odds ratio.
Only present in the \eqn{2 \times 2}{} case and if argument
\code{conf.int = TRUE}.
\item[\code{estimate}] an estimate of the odds ratio.  Note that the
\emph{conditional} Maximum Likelihood Estimate (MLE) rather than the
unconditional MLE (the sample odds ratio) is used.
Only present in the \eqn{2 \times 2}{} case.
\item[\code{null.value}] the odds ratio under the null, \code{or}.
Only present in the \eqn{2 \times 2}{} case.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] the character string
\code{"Fisher's Exact Test for Count Data"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Agresti, A. (1990)
\emph{Categorical data analysis}.
New York: Wiley.
Pages 59--66.

Agresti, A. (2002)
\emph{Categorical data analysis}. Second edition.
New York: Wiley.
Pages 91--101.

Fisher, R. A. (1935)
The logic of inductive inference.
\emph{Journal of the Royal Statistical Society Series A} \bold{98},
39--54.

Fisher, R. A. (1962)
Confidence limits for a cross-product ratio.
\emph{Australian Journal of Statistics} \bold{4}, 41.

Fisher, R. A. (1970)
\emph{Statistical Methods for Research Workers.}  Oliver \& Boyd.

Mehta, C. R. and Patel, N. R. (1986)
Algorithm 643. FEXACT: A Fortran subroutine for Fisher's exact test
on unordered \eqn{r*c}{} contingency tables.
\emph{ACM Transactions on Mathematical Software}, \bold{12},
154--161.

Clarkson, D. B., Fan, Y. and Joe, H. (1993)
A Remark on Algorithm 643: FEXACT: An Algorithm for Performing
Fisher's Exact Test in \eqn{r \times c}{} Contingency Tables.
\emph{ACM Transactions on Mathematical Software}, \bold{19},
484--488.

Patefield, W. M. (1981)
Algorithm AS159.  An efficient method of generating r x c tables
with given row and column totals.
\emph{Applied Statistics} \bold{30}, 91--97.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{chisq.test}{chisq.test}}

\code{fisher.exact} in package \Rhref{http://CRAN.R-project.org/package=exact2x2}{\pkg{exact2x2}} for alternative
interpretations of two-sided tests and confidence intervals for
\eqn{2 \times 2}{} tables.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Agresti (1990, p. 61f; 2002, p. 91) Fisher's Tea Drinker
## A British woman claimed to be able to distinguish whether milk or
##  tea was added to the cup first.  To test, she was given 8 cups of
##  tea, in four of which milk was added first.  The null hypothesis
##  is that there is no association between the true order of pouring
##  and the woman's guess, the alternative that there is a positive
##  association (that the odds ratio is greater than 1).
TeaTasting <-
matrix(c(3, 1, 1, 3),
       nrow = 2,
       dimnames = list(Guess = c("Milk", "Tea"),
                       Truth = c("Milk", "Tea")))
fisher.test(TeaTasting, alternative = "greater")
## => p=0.2429, association could not be established

## Fisher (1962, 1970), Criminal convictions of like-sex twins
Convictions <-
matrix(c(2, 10, 15, 3),
       nrow = 2,
       dimnames =
       list(c("Dizygotic", "Monozygotic"),
            c("Convicted", "Not convicted")))
Convictions
fisher.test(Convictions, alternative = "less")
fisher.test(Convictions, conf.int = FALSE)
fisher.test(Convictions, conf.level = 0.95)$conf.int
fisher.test(Convictions, conf.level = 0.99)$conf.int

## A r x c table  Agresti (2002, p. 57) Job Satisfaction
Job <- matrix(c(1,2,1,0, 3,3,6,1, 10,10,14,9, 6,7,12,11), 4, 4,
dimnames = list(income=c("< 15k", "15-25k", "25-40k", "> 40k"),
                satisfaction=c("VeryD", "LittleD", "ModerateS", "VeryS")))
fisher.test(Job)
fisher.test(Job, simulate.p.value=TRUE, B=1e5)
\end{ExampleCode}
\end{Examples}
\HeaderA{fitted}{Extract Model Fitted Values}{fitted}
\methaliasA{fitted.default}{fitted}{fitted.default}
\methaliasA{fitted.values}{fitted}{fitted.values}
\keyword{models}{fitted}
\keyword{regression}{fitted}
%
\begin{Description}\relax
\code{fitted} is a generic function which extracts fitted values from
objects returned by modeling functions.  \code{fitted.values} is an
alias for it.

All object classes which are returned by model fitting functions
should provide a \code{fitted} method.  (Note that the generic is
\code{fitted} and not \code{fitted.values}.)

Methods can make use of \code{\LinkA{napredict}{napredict}} methods to compensate
for the omission of missing values.  The default and \code{\LinkA{nls}{nls}}
methods do.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fitted(object, ...)
fitted.values(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object for which the extraction of model fitted values is
meaningful.
\item[\code{...}] other arguments.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Fitted values extracted from the object \code{object}.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{coefficients}{coefficients}}, \code{\LinkA{glm}{glm}}, \code{\LinkA{lm}{lm}},
\code{\LinkA{residuals}{residuals}}.
\end{SeeAlso}
\HeaderA{fivenum}{Tukey Five-Number Summaries}{fivenum}
\keyword{univar}{fivenum}
\keyword{robust}{fivenum}
\keyword{distribution}{fivenum}
%
\begin{Description}\relax
Returns Tukey's five number summary (minimum, lower-hinge, median,
upper-hinge, maximum) for the input data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fivenum(x, na.rm = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric, maybe including \code{\LinkA{NA}{NA}}s and
\eqn{\pm}{}\code{\LinkA{Inf}{Inf}}s.
\item[\code{na.rm}] logical; if \code{TRUE}, all \code{\LinkA{NA}{NA}} and
\code{\LinkA{NaN}{NaN}}s are dropped, before the statistics are computed.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric vector of length 5 containing the summary information.  See
\code{\LinkA{boxplot.stats}{boxplot.stats}} for more details.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{IQR}{IQR}},
\code{\LinkA{boxplot.stats}{boxplot.stats}},
\code{\LinkA{median}{median}},
\code{\LinkA{quantile}{quantile}},
\code{\LinkA{range}{range}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
fivenum(c(rnorm(100),-1:1/0))
\end{ExampleCode}
\end{Examples}
\HeaderA{fligner.test}{Fligner-Killeen Test of Homogeneity of Variances}{fligner.test}
\methaliasA{fligner.test.default}{fligner.test}{fligner.test.default}
\methaliasA{fligner.test.formula}{fligner.test}{fligner.test.formula}
\keyword{htest}{fligner.test}
%
\begin{Description}\relax
Performs a Fligner-Killeen (median) test of the null that the
variances in each of the groups (samples) are the same.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fligner.test(x, ...)

## Default S3 method:
fligner.test(x, g, ...)

## S3 method for class 'formula'
fligner.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector of data values, or a list of numeric data
vectors.
\item[\code{g}] a vector or factor object giving the group for the
corresponding elements of \code{x}.
Ignored if \code{x} is a list.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
gives the data values and \code{rhs} the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods. 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{x} is a list, its elements are taken as the samples to be
compared for homogeneity of variances, and hence have to be numeric
data vectors.  In this case, \code{g} is ignored, and one can simply
use \code{fligner.test(x)} to perform the test.  If the samples are
not yet contained in a list, use \code{fligner.test(list(x, ...))}.

Otherwise, \code{x} must be a numeric data vector, and \code{g} must
be a vector or factor object of the same length as \code{x} giving the
group for the corresponding elements of \code{x}.

The Fligner-Killeen (median) test has been determined in a simulation
study as one of the many tests for homogeneity of variances which is
most robust against departures from normality, see Conover, Johnson \&
Johnson (1981).  It is a \eqn{k}{}-sample simple linear rank which uses
the ranks of the absolute values of the centered samples and weights
\eqn{a(i) = \mathrm{qnorm}((1 + i/(n+1))/2)}{}.  The version implemented here uses median centering in
each of the samples (F-K:med \eqn{X^2}{} in the reference).
\end{Details}
%
\begin{Value}
A list of class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the Fligner-Killeen:med \eqn{X^2}{} test statistic.
\item[\code{parameter}] the degrees of freedom of the approximate chi-squared
distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] the character string
\code{"Fligner-Killeen test of homogeneity of variances"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
William J. Conover, Mark E. Johnson and Myrle M. Johnson (1981).
A comparative study of tests for homogeneity of variances, with
applications to the outer continental shelf bidding data.
\emph{Technometrics} \bold{23}, 351--361.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ansari.test}{ansari.test}} and \code{\LinkA{mood.test}{mood.test}} for rank-based
two-sample test for a difference in scale parameters;
\code{\LinkA{var.test}{var.test}} and \code{\LinkA{bartlett.test}{bartlett.test}} for parametric
tests for the homogeneity of variances.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

plot(count ~ spray, data = InsectSprays)
fligner.test(InsectSprays$count, InsectSprays$spray)
fligner.test(count ~ spray, data = InsectSprays)
## Compare this to bartlett.test()
\end{ExampleCode}
\end{Examples}
\HeaderA{formula}{Model Formulae}{formula}
\aliasA{as.formula}{formula}{as.formula}
\methaliasA{formula.data.frame}{formula}{formula.data.frame}
\methaliasA{formula.default}{formula}{formula.default}
\methaliasA{formula.formula}{formula}{formula.formula}
\methaliasA{formula.terms}{formula}{formula.terms}
\aliasA{print.formula}{formula}{print.formula}
\aliasA{[.formula}{formula}{[.formula}
\keyword{models}{formula}
%
\begin{Description}\relax
The generic function \code{formula} and its specific methods provide a
way of extracting formulae which have been included in other objects.

\code{as.formula} is almost identical, additionally preserving
attributes when \code{object} already inherits from
\code{"formula"}.  The default value of the \code{env} argument is
used only when the formula would otherwise lack an environment.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
formula(x, ...)
as.formula(object, env = parent.frame())

## S3 method for class 'formula'
print(x, showEnv = !identical(e, .GlobalEnv), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, object}] \R{} object.
\item[\code{...}] further arguments passed to or from other methods.
\item[\code{env}] the environment to associate with the result.
\item[\code{showEnv}] logical indicating if the environment should be printed
as well.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The models fit by, e.g., the \code{\LinkA{lm}{lm}} and \code{\LinkA{glm}{glm}} functions
are specified in a compact symbolic form.
The \code{\textasciitilde{}} operator is basic in the formation of such models.
An expression of the form \code{y \textasciitilde{} model} is interpreted
as a specification that the response \code{y} is modelled
by a linear predictor specified symbolically by \code{model}.
Such a model consists of a series of terms separated
by \code{+} operators.
The terms themselves consist of variable and factor
names separated by \code{:} operators.
Such a term is interpreted as the interaction of
all the variables and factors appearing in the term.

In addition to \code{+} and \code{:}, a number of other operators are
useful in model formulae.  The \code{*} operator denotes factor
crossing: \code{a*b} interpreted as \code{a+b+a:b}.  The \code{\textasciicircum{}}
operator indicates crossing to the specified degree.  For example
\code{(a+b+c)\textasciicircum{}2} is identical to \code{(a+b+c)*(a+b+c)} which in turn
expands to a formula containing the main effects for \code{a},
\code{b} and \code{c} together with their second-order interactions.
The \code{\%in\%} operator indicates that the terms on its left are
nested within those on the right.  For example \code{a + b \%in\% a}
expands to the formula \code{a + a:b}.  The \code{-} operator removes
the specified terms, so that \code{(a+b+c)\textasciicircum{}2 - a:b} is identical to
\code{a + b + c + b:c + a:c}.  It can also used to remove the
intercept term: when fitting a linear model \code{y \textasciitilde{} x - 1} specifies
a line through the origin.  A model with no intercept can be also
specified as \code{y \textasciitilde{} x + 0} or \code{y \textasciitilde{} 0 + x}.

While formulae usually involve just variable and factor
names, they can also involve arithmetic expressions.
The formula \code{log(y) \textasciitilde{} a + log(x)} is quite legal.
When such arithmetic expressions involve
operators which are also used symbolically
in model formulae, there can be confusion between
arithmetic and symbolic operator use.

To avoid this confusion, the function \code{\LinkA{I}{I}()}
can be used to bracket those portions of a model
formula where the operators are used in their
arithmetic sense.  For example, in the formula
\code{y \textasciitilde{} a + I(b+c)}, the term \code{b+c} is to be
interpreted as the sum of \code{b} and \code{c}.

Variable names can be quoted by backticks \code{`like this`} in
formulae, although there is no guarantee that all code using formulae
will accept such non-syntactic names.

Most model-fitting functions accept formulae with right-hand-side
including the function \code{\LinkA{offset}{offset}} to indicate terms with a
fixed coefficient of one.  Some functions accept other
`specials' such as \code{strata} or \code{cluster} (see the
\code{specials} argument of \code{\LinkA{terms.formula}{terms.formula})}.

There are two special interpretations of \code{.} in a formula.  The
usual one is in the context of a \code{data} argument of model
fitting functions and means `all columns not otherwise in the
formula': see \code{\LinkA{terms.formula}{terms.formula}}.  In the context of
\code{\LinkA{update.formula}{update.formula}}, \bold{only}, it means `what was
previously in this part of the formula'.

When \code{formula} is called on a fitted model object, either a
specific method is used (such as that for class \code{"nls"}) or the
default method.  The default first looks for a \code{"formula"}
component of the object (and evaluates it), then a \code{"terms"}
component, then a \code{formula} parameter of the call (and evaluates
its value) and finally a \code{"formula"} attribute.

There is a \code{formula} method for data frames.  If there is only
one column this forms the RHS with an empty LHS.  For more columns,
the first column is the LHS of the formula and the remaining columns
separated by \code{+} form the RHS.
\end{Details}
%
\begin{Value}
All the functions above produce an object of class \code{"formula"}
which contains a symbolic model formula.
\end{Value}
%
\begin{Section}{Environments}
A formula object has an associated environment, and
this environment (rather than the parent
environment) is used by \code{\LinkA{model.frame}{model.frame}} to evaluate variables
that are not found in the supplied \code{data} argument.

Formulas created with the \code{\textasciitilde{}} operator use the
environment in which they were created.  Formulas created with
\code{as.formula} will use the \code{env} argument for their
environment.  Pre-existing formulas extracted with
\code{as.formula} will only have their environment changed if
\code{env} is given explicitly.
\end{Section}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical models.}
Chapter 2 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{I}{I}}, \code{\LinkA{offset}{offset}}.

For formula manipulation: \code{\LinkA{terms}{terms}}, and \code{\LinkA{all.vars}{all.vars}};
for typical use: \code{\LinkA{lm}{lm}}, \code{\LinkA{glm}{glm}}, and
\code{\LinkA{coplot}{coplot}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
class(fo <- y ~ x1*x2) # "formula"
fo
typeof(fo)# R internal : "language"
terms(fo)

environment(fo)
environment(as.formula("y ~ x"))
environment(as.formula("y ~ x", env=new.env()))


## Create a formula for a model with a large number of variables:
xnam <- paste0("x", 1:25)
(fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
\end{ExampleCode}
\end{Examples}
\HeaderA{formula.nls}{Extract Model Formula from nls Object}{formula.nls}
\keyword{models}{formula.nls}
%
\begin{Description}\relax
Returns the model used to fit \code{object}. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'nls'
formula(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object inheriting from class \code{"nls"}, representing
a nonlinear least squares fit.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a formula representing the model used to obtain \code{object}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{formula}{formula}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
fm1 <- nls(circumference ~ A/(1+exp((B-age)/C)), Orange,
           start = list(A=160, B=700, C = 350))
formula(fm1)
\end{ExampleCode}
\end{Examples}
\HeaderA{friedman.test}{Friedman Rank Sum Test}{friedman.test}
\methaliasA{friedman.test.default}{friedman.test}{friedman.test.default}
\methaliasA{friedman.test.formula}{friedman.test}{friedman.test.formula}
\keyword{htest}{friedman.test}
%
\begin{Description}\relax
Performs a Friedman rank sum test with unreplicated blocked data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
friedman.test(y, ...)

## Default S3 method:
friedman.test(y, groups, blocks, ...)

## S3 method for class 'formula'
friedman.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{y}] either a numeric vector of data values, or a data matrix.
\item[\code{groups}] a vector giving the group for the corresponding
elements of \code{y} if this is a vector;  ignored if \code{y}
is a matrix.  If not a factor object, it is coerced to one.
\item[\code{blocks}] a vector giving the block for the corresponding
elements of \code{y} if this is a vector;  ignored if \code{y}
is a matrix.  If not a factor object, it is coerced to one.
\item[\code{formula}] a formula of the form \code{a \textasciitilde{} b | c}, where \code{a},
\code{b} and \code{c} give the data values and corresponding groups
and blocks, respectively.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.   
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{friedman.test} can be used for analyzing unreplicated complete
block designs (i.e., there is exactly one observation in \code{y}
for each combination of levels of \code{groups} and \code{blocks})
where the normality assumption may be violated.

The null hypothesis is that apart from an effect of \code{blocks},
the location parameter of \code{y} is the same in each of the
\code{groups}. 

If \code{y} is a matrix, \code{groups} and \code{blocks} are
obtained from the column and row indices, respectively.  \code{NA}'s
are not allowed in \code{groups} or \code{blocks};  if \code{y}
contains \code{NA}'s, corresponding blocks are removed.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of Friedman's chi-squared statistic.
\item[\code{parameter}] the degrees of freedom of the approximate
chi-squared distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] the character string \code{"Friedman rank sum test"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Myles Hollander and Douglas A. Wolfe (1973),
\emph{Nonparametric Statistical Methods.}
New York: John Wiley \& Sons.
Pages 139--146.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{quade.test}{quade.test}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Hollander & Wolfe (1973), p. 140ff.
## Comparison of three methods ("round out", "narrow angle", and
##  "wide angle") for rounding first base.  For each of 18 players
##  and the three method, the average time of two runs from a point on
##  the first base line 35ft from home plate to a point 15ft short of
##  second base is recorded.
RoundingTimes <-
matrix(c(5.40, 5.50, 5.55,
         5.85, 5.70, 5.75,
         5.20, 5.60, 5.50,
         5.55, 5.50, 5.40,
         5.90, 5.85, 5.70,
         5.45, 5.55, 5.60,
         5.40, 5.40, 5.35,
         5.45, 5.50, 5.35,
         5.25, 5.15, 5.00,
         5.85, 5.80, 5.70,
         5.25, 5.20, 5.10,
         5.65, 5.55, 5.45,
         5.60, 5.35, 5.45,
         5.05, 5.00, 4.95,
         5.50, 5.50, 5.40,
         5.45, 5.55, 5.50,
         5.55, 5.55, 5.35,
         5.45, 5.50, 5.55,
         5.50, 5.45, 5.25,
         5.65, 5.60, 5.40,
         5.70, 5.65, 5.55,
         6.30, 6.30, 6.25),
       nrow = 22,
       byrow = TRUE,
       dimnames = list(1 : 22,
                       c("Round Out", "Narrow Angle", "Wide Angle")))
friedman.test(RoundingTimes)
## => strong evidence against the null that the methods are equivalent
##    with respect to speed

wb <- aggregate(warpbreaks$breaks,
                by = list(w = warpbreaks$wool,
                          t = warpbreaks$tension),
                FUN = mean)
wb
friedman.test(wb$x, wb$w, wb$t)
friedman.test(x ~ w | t, data = wb)
\end{ExampleCode}
\end{Examples}
\HeaderA{ftable}{Flat Contingency Tables}{ftable}
\methaliasA{ftable.default}{ftable}{ftable.default}
\aliasA{print.ftable}{ftable}{print.ftable}
\keyword{category}{ftable}
%
\begin{Description}\relax
Create `flat' contingency tables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ftable(x, ...)

## Default S3 method:
ftable(..., exclude = c(NA, NaN), row.vars = NULL,
       col.vars = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, ...}] \R{} objects which can be interpreted as factors (including
character strings), or a list (or data frame) whose components can
be so interpreted, or a contingency table object of class
\code{"table"} or \code{"ftable"}.
\item[\code{exclude}] values to use in the exclude argument of \code{factor}
when interpreting non-factor objects.
\item[\code{row.vars}] a vector of integers giving the numbers of the
variables, or a character vector giving the names of the variables
to be used for the rows of the flat contingency table.
\item[\code{col.vars}] a vector of integers giving the numbers of the
variables, or a character vector giving the names of the variables
to be used for the columns of the flat contingency table.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{ftable} creates `flat' contingency tables.  Similar to the
usual contingency tables, these contain the counts of each combination
of the levels of the variables (factors) involved.  This information
is then re-arranged as a matrix whose rows and columns correspond to
unique combinations of the levels of the row and column variables (as
specified by \code{row.vars} and \code{col.vars}, respectively).  The
combinations are created by looping over the variables in reverse
order (so that the levels of the left-most variable vary the
slowest).  Displaying a contingency table in this flat matrix form
(via \code{print.ftable}, the print method for objects of class
\code{"ftable"}) is often preferable to showing it as a
higher-dimensional array.

\code{ftable} is a generic function.  Its default method,
\code{ftable.default}, first creates a contingency table in array
form from all arguments except \code{row.vars} and \code{col.vars}.
If the first argument is of class \code{"table"}, it represents a
contingency table and is used as is; if it is a flat table of class
\code{"ftable"}, the information it contains is converted to the usual
array representation using \code{as.ftable}.  Otherwise, the arguments
should be \R{} objects which can be interpreted as factors (including
character strings), or a list (or data frame) whose components can be
so interpreted, which are cross-tabulated using \code{\LinkA{table}{table}}.
Then, the arguments \code{row.vars} and \code{col.vars} are used to
collapse the contingency table into flat form.  If neither of these
two is given, the last variable is used for the columns.  If both are
given and their union is a proper subset of all variables involved,
the other variables are summed out.

When the arguments are \R{} expressions interpreted as factors,
additional arguments will be passed to \code{table} to control how
the variable names are displayed; see the last example below.

Function \code{\LinkA{ftable.formula}{ftable.formula}} provides a formula method for
creating flat contingency tables.

There are methods for \code{\LinkA{as.table}{as.table}} and
\code{\LinkA{as.data.frame}{as.data.frame}}.
\end{Details}
%
\begin{Value}
\code{ftable} returns an object of class \code{"ftable"}, which is a
matrix with counts of each combination of the levels of variables with
information on the names and levels of the (row and columns) variables
stored as attributes \code{"row.vars"} and \code{"col.vars"}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{ftable.formula}{ftable.formula}} for the formula interface (which allows a
\code{data = .} argument);
\code{\LinkA{read.ftable}{read.ftable}} for information on reading, writing and
coercing flat contingency tables;
\code{\LinkA{table}{table}} for ordinary cross-tabulation;
\code{\LinkA{xtabs}{xtabs}} for formula-based cross-tabulation.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Start with a contingency table.
ftable(Titanic, row.vars = 1:3)
ftable(Titanic, row.vars = 1:2, col.vars = "Survived")
ftable(Titanic, row.vars = 2:1, col.vars = "Survived")

## Start with a data frame.
x <- ftable(mtcars[c("cyl", "vs", "am", "gear")])
x
ftable(x, row.vars = c(2, 4))

## Start with expressions, use table()'s "dnn" to change labels
ftable(mtcars$cyl, mtcars$vs, mtcars$am, mtcars$gear, row.vars = c(2, 4),
       dnn = c("Cylinders", "V/S", "Transmission", "Gears"))
\end{ExampleCode}
\end{Examples}
\HeaderA{ftable.formula}{Formula Notation for Flat Contingency Tables}{ftable.formula}
\keyword{category}{ftable.formula}
%
\begin{Description}\relax
Produce or manipulate a flat contingency table using
formula notation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'formula'
ftable(formula, data = NULL, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a formula object with both left and right hand sides
specifying the column and row variables of the flat table.
\item[\code{data}] a data frame, list or environment (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables
to be cross-tabulated, or a contingency table (see below).
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
Ignored if \code{data} is a contingency table.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.
Ignored if \code{data} is a contingency table.
\item[\code{...}] further arguments to the default ftable method may also
be passed as arguments, see \code{\LinkA{ftable.default}{ftable.default}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a method of the generic function \code{\LinkA{ftable}{ftable}}.

The left and right hand side of \code{formula} specify the column and
row variables, respectively, of the flat contingency table to be
created.  Only the \code{+} operator is allowed for combining the
variables.  A \code{.} may be used once in the formula to indicate
inclusion of all the remaining variables.

If \code{data} is an object of class \code{"table"} or an array with
more than 2 dimensions, it is taken as a contingency table, and hence
all entries should be nonnegative.  Otherwise, if it is not a flat
contingency table (i.e., an object of class \code{"ftable"}), it
should be a data frame or matrix, list or environment containing the
variables to be cross-tabulated.  In this case, \code{na.action} is
applied to the data to handle missing values, and, after possibly
selecting a subset of the data as specified by the \code{subset}
argument, a contingency table is computed from the variables.

The contingency table is then collapsed to a flat table, according to
the row and column variables specified by \code{formula}.
\end{Details}
%
\begin{Value}
A flat contingency table which contains the counts of each combination
of the levels of the variables, collapsed into a matrix for suitably
displaying the counts.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{ftable}{ftable}},
\code{\LinkA{ftable.default}{ftable.default}};
\code{\LinkA{table}{table}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
Titanic
x <- ftable(Survived ~ ., data = Titanic)
x
ftable(Sex ~ Class + Age, data = x)
\end{ExampleCode}
\end{Examples}
\HeaderA{GammaDist}{The Gamma Distribution}{GammaDist}
\aliasA{dgamma}{GammaDist}{dgamma}
\aliasA{pgamma}{GammaDist}{pgamma}
\aliasA{qgamma}{GammaDist}{qgamma}
\aliasA{rgamma}{GammaDist}{rgamma}
\keyword{distribution}{GammaDist}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the Gamma distribution with parameters \code{shape} and
\code{scale}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dgamma(x, shape, rate = 1, scale = 1/rate, log = FALSE)
pgamma(q, shape, rate = 1, scale = 1/rate, lower.tail = TRUE,
       log.p = FALSE)
qgamma(p, shape, rate = 1, scale = 1/rate, lower.tail = TRUE,
       log.p = FALSE)
rgamma(n, shape, rate = 1, scale = 1/rate)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{rate}] an alternative way to specify the scale.
\item[\code{shape, scale}] shape and scale parameters.  Must be positive,
\code{scale} strictly.
\item[\code{log, log.p}] logical; if \code{TRUE}, probabilities/densities \eqn{p}{}
are returned as \eqn{log(p)}{}.
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{scale} is omitted, it assumes the default value of \code{1}.

The Gamma distribution with parameters \code{shape} \eqn{=\alpha}{}
and \code{scale} \eqn{=\sigma}{} has density
\deqn{
    f(x)= \frac{1}{{\sigma}^{\alpha}\Gamma(\alpha)} {x}^{\alpha-1} e^{-x/\sigma}%
  }{}
for \eqn{x \ge 0}{}, \eqn{\alpha > 0}{} and \eqn{\sigma > 0}{}.
(Here \eqn{\Gamma(\alpha)}{} is the function implemented by \R{}'s
\code{\LinkA{gamma}{gamma}()} and defined in its help.  Note that \eqn{a=0}{}
corresponds to the trivial distribution with all mass at point 0.)

The mean and variance are
\eqn{E(X) = \alpha\sigma}{} and
\eqn{Var(X) = \alpha\sigma^2}{}.

The cumulative hazard \eqn{H(t) = - \log(1 - F(t))}{}
is \code{-pgamma(t, ..., lower = FALSE, log = TRUE)}.

Note that for smallish values of \code{shape} (and moderate
\code{scale}) a large parts of the mass of the Gamma distribution is
on values of \eqn{x}{} so near zero that they will be represented as
zero in computer arithmetic.  So \code{rgamma} can well return values
which will be represented as zero.  (This will also happen for very
large values of \code{scale} since the actual generation is done for
\code{scale=1}.)
\end{Details}
%
\begin{Value}
\code{dgamma} gives the density,
\code{pgamma} gives the distribution function,
\code{qgamma} gives the quantile function, and
\code{rgamma} generates random deviates.

Invalid arguments will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Note}\relax
The S parametrization is via \code{shape} and \code{rate}: S has no
\code{scale} parameter.  In \R{}, \code{scale} takes precedence over
\code{rate} if both are supplied.

\code{pgamma} is closely related to the incomplete gamma function.  As
defined by Abramowitz and Stegun 6.5.1 (and by `Numerical
Recipes') this is
\deqn{P(a,x) = \frac{1}{\Gamma(a)} \int_0^x t^{a-1} e^{-t} dt}{}
\eqn{P(a, x)}{} is \code{pgamma(x, a)}.  Other authors (for example
Karl Pearson in his 1922 tables) omit the normalizing factor,
defining the incomplete gamma function as \code{pgamma(x, a) * gamma(a)}.
A few use the `upper' incomplete gamma function, the integral
from \eqn{x}{} to \eqn{\infty}{} which can be computed by
\code{pgamma(x, a, lower=FALSE) * gamma(a)}, or its normalized
version. See also
\url{http://en.wikipedia.org/wiki/Incomplete_gamma_function}.
\end{Note}
%
\begin{Source}\relax
\code{dgamma} is computed via the Poisson density, using code contributed
by Catherine Loader (see \code{\LinkA{dbinom}{dbinom}}).

\code{pgamma} uses an unpublished (and not otherwise documented)
algorithm `mainly by Morten Welinder'.

\code{qgamma} is based on a C translation of

Best, D. J. and D. E. Roberts (1975).
Algorithm AS91. Percentage points of the chi-squared distribution.
\emph{Applied Statistics},  \bold{24}, 385--388.

plus a final Newton step to improve the approximation.

\code{rgamma} for \code{shape >= 1} uses

Ahrens, J. H. and Dieter, U. (1982).
Generating gamma variates by a modified rejection technique.
\emph{Communications of the ACM}, \bold{25}, 47--54,

and for \code{0 < shape < 1} uses

Ahrens, J. H. and Dieter, U. (1974).
Computer methods for sampling from gamma, beta, Poisson and binomial
distributions. \emph{Computing}, \bold{12}, 223--246.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Shea, B. L. (1988)
Algorithm AS 239,  Chi-squared and incomplete Gamma integral,
\emph{Applied Statistics (JRSS C)} \bold{37}, 466--473.

Abramowitz, M. and Stegun, I. A. (1972)
\emph{Handbook of Mathematical Functions.} New York: Dover.
Chapter 6: Gamma and Related Functions.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{gamma}{gamma}} for the gamma function.

\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dbeta}{dbeta}} for the Beta distribution and \code{\LinkA{dchisq}{dchisq}}
for the chi-squared distribution which is a special case of the Gamma
distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
-log(dgamma(1:4, shape=1))
p <- (1:9)/10
pgamma(qgamma(p,shape=2), shape=2)
1 - 1/exp(qgamma(p, shape=1))

# even for shape = 0.001 about half the mass is on numbers
# that cannot be represented accurately (and most of those as zero)
pgamma(.Machine$double.xmin, 0.001)
pgamma(5e-324, 0.001)  # on most machines 5e-324 is the smallest
                       # representable non-zero number
table(rgamma(1e4, 0.001) == 0)/1e4
\end{ExampleCode}
\end{Examples}
\HeaderA{Geometric}{The Geometric Distribution}{Geometric}
\aliasA{dgeom}{Geometric}{dgeom}
\aliasA{pgeom}{Geometric}{pgeom}
\aliasA{qgeom}{Geometric}{qgeom}
\aliasA{rgeom}{Geometric}{rgeom}
\keyword{distribution}{Geometric}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the geometric distribution with parameter \code{prob}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dgeom(x, prob, log = FALSE)
pgeom(q, prob, lower.tail = TRUE, log.p = FALSE)
qgeom(p, prob, lower.tail = TRUE, log.p = FALSE)
rgeom(n, prob)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles representing the number of failures in
a sequence of Bernoulli trials before success occurs.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{prob}] probability of success in each trial. \code{0 < prob <= 1}.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The geometric distribution with \code{prob} \eqn{= p}{} has density
\deqn{p(x) = p {(1-p)}^{x}}{}
for \eqn{x = 0, 1, 2, \ldots}{}, \eqn{0 < p \le 1}{}.

If an element of \code{x} is not integer, the result of \code{dgeom}
is zero, with a warning.

The quantile is defined as the smallest value \eqn{x}{} such that
\eqn{F(x) \ge p}{}, where \eqn{F}{} is the distribution function.
\end{Details}
%
\begin{Value}
\code{dgeom} gives the density,
\code{pgeom} gives the distribution function,
\code{qgeom} gives the quantile function, and
\code{rgeom} generates random deviates.

Invalid \code{prob} will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Source}\relax
\code{dgeom} computes via \code{dbinom}, using code contributed by
Catherine Loader (see \code{\LinkA{dbinom}{dbinom}}).

\code{pgeom} and \code{qgeom} are based on the closed-form formulae.

\code{rgeom} uses the derivation as an exponential mixture of Poissons, see

Devroye, L. (1986) \emph{Non-Uniform Random Variate Generation.}
Springer-Verlag, New York. Page 480.
\end{Source}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dnbinom}{dnbinom}} for the negative binomial which generalizes
the geometric distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
qgeom((1:9)/10, prob = .2)
Ni <- rgeom(20, prob = 1/4); table(factor(Ni, 0:max(Ni)))
\end{ExampleCode}
\end{Examples}
\HeaderA{getInitial}{Get Initial Parameter Estimates}{getInitial}
\methaliasA{getInitial.default}{getInitial}{getInitial.default}
\methaliasA{getInitial.formula}{getInitial}{getInitial.formula}
\methaliasA{getInitial.selfStart}{getInitial}{getInitial.selfStart}
\keyword{models}{getInitial}
\keyword{nonlinear}{getInitial}
\keyword{manip}{getInitial}
%
\begin{Description}\relax
This function evaluates initial parameter estimates for a nonlinear
regression model.  If \code{data} is a parameterized data frame or
\code{pframe} object, its \code{parameters} attribute is returned.
Otherwise the object is examined to see if it contains a call to a
\code{selfStart} object whose \code{initial} attribute can be
evaluated.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
getInitial(object, data, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a formula or a \code{selfStart} model that defines a
nonlinear regression model
\item[\code{data}] a data frame in which the expressions in the formula or
arguments to the \code{selfStart} model can be evaluated
\item[\code{...}] optional additional arguments
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A named numeric vector or list of starting estimates for the
parameters.  The construction of many \code{selfStart} models is such
that these "starting" estimates are, in fact, the converged parameter
estimates.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, 
\code{\LinkA{selfStart}{selfStart}},
\code{\LinkA{selfStart.default}{selfStart.default}}, \code{\LinkA{selfStart.formula}{selfStart.formula}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
PurTrt <- Puromycin[ Puromycin$state == "treated", ]
print(getInitial( rate ~ SSmicmen( conc, Vm, K ), PurTrt ), digits = 3)
\end{ExampleCode}
\end{Examples}
\HeaderA{glm}{Fitting Generalized Linear Models}{glm}
\methaliasA{glm.fit}{glm}{glm.fit}
\aliasA{print.glm}{glm}{print.glm}
\aliasA{weights.glm}{glm}{weights.glm}
\keyword{models}{glm}
\keyword{regression}{glm}
%
\begin{Description}\relax
\code{glm} is used to fit generalized linear models, specified by
giving a symbolic description of the linear predictor and a
description of the error distribution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
glm(formula, family = gaussian, data, weights, subset,
    na.action, start = NULL, etastart, mustart, offset,
    control = list(...), model = TRUE, method = "glm.fit",
    x = FALSE, y = TRUE, contrasts = NULL, ...)

glm.fit(x, y, weights = rep(1, nobs),
        start = NULL, etastart = NULL, mustart = NULL,
        offset = rep(0, nobs), family = gaussian(),
        control = list(), intercept = TRUE)

## S3 method for class 'glm'
weights(object, type = c("prior", "working"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] an object of class \code{"\LinkA{formula}{formula}"} (or one that
can be coerced to that class): a symbolic description of the
model to be fitted.  The details of model specification are given
under `Details'.

\item[\code{family}] a description of the error distribution and link
function to be used in the model. This can be a character string
naming a family function, a family function or the result of a call
to a family function.  (See \code{\LinkA{family}{family}} for details of
family functions.)

\item[\code{data}] an optional data frame, list or environment (or object
coercible by \code{\LinkA{as.data.frame}{as.data.frame}} to a data frame) containing
the variables in the model.  If not found in \code{data}, the
variables are taken from \code{environment(formula)},
typically the environment from which \code{glm} is called.

\item[\code{weights}] an optional vector of `prior weights' to be used
in the fitting process.  Should be \code{NULL} or a numeric vector.

\item[\code{subset}] an optional vector specifying a subset of observations
to be used in the fitting process.

\item[\code{na.action}] a function which indicates what should happen
when the data contain \code{NA}s.  The default is set by
the \code{na.action} setting of \code{\LinkA{options}{options}}, and is
\code{\LinkA{na.fail}{na.fail}} if that is unset.  The `factory-fresh'
default is \code{\LinkA{na.omit}{na.omit}}.  Another possible value is
\code{NULL}, no action.  Value \code{\LinkA{na.exclude}{na.exclude}} can be useful.

\item[\code{start}] starting values for the parameters in the linear predictor.

\item[\code{etastart}] starting values for the linear predictor.

\item[\code{mustart}] starting values for the vector of means.

\item[\code{offset}] this can be used to specify an \emph{a priori} known
component to be included in the linear predictor during fitting.
This should be \code{NULL} or a numeric vector of length equal to
the number of cases.  One or more \code{\LinkA{offset}{offset}} terms can be
included in the formula instead or as well, and if more than one is
specified their sum is used.  See \code{\LinkA{model.offset}{model.offset}}.

\item[\code{control}] a list of parameters for controlling the fitting
process.  For \code{glm.fit} this is passed to
\code{\LinkA{glm.control}{glm.control}}.

\item[\code{model}] a logical value indicating whether \emph{model frame}
should be included as a component of the returned value.

\item[\code{method}] the method to be used in fitting the model.  The default
method \code{"glm.fit"} uses iteratively reweighted least squares
(IWLS): the alternative \code{"model.frame"} returns the model frame
and does no fitting.

User-supplied fitting functions can be supplied either as a function
or a character string naming a function, with a function which takes
the same arguments as \code{glm.fit}.  If specified as a character
string it is looked up from within the \pkg{stats} namespace.


\item[\code{x, y}] For \code{glm}:
logical values indicating whether the response vector and model
matrix used in the fitting process should be returned as components
of the returned value.

For \code{glm.fit}: \code{x} is a design matrix of dimension
\code{n * p}, and \code{y} is a vector of observations of length
\code{n}.


\item[\code{contrasts}] an optional list. See the \code{contrasts.arg}
of \code{model.matrix.default}.

\item[\code{intercept}] logical. Should an intercept be included in the
\emph{null} model?

\item[\code{object}] an object inheriting from class \code{"glm"}.
\item[\code{type}] character, partial matching allowed.  Type of weights to
extract from the fitted model object.

\item[\code{...}] 
For \code{glm}: arguments to be used to form the default
\code{control} argument if it is not supplied directly.

For \code{weights}: further arguments passed to or from other methods.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
A typical predictor has the form \code{response \textasciitilde{} terms} where
\code{response} is the (numeric) response vector and \code{terms} is a
series of terms which specifies a linear predictor for
\code{response}.  For \code{binomial} and \code{quasibinomial}
families the response can also be specified as a \code{\LinkA{factor}{factor}}
(when the first level denotes failure and all others success) or as a
two-column matrix with the columns giving the numbers of successes and
failures.  A terms specification of the form \code{first + second}
indicates all the terms in \code{first} together with all the terms in
\code{second} with any duplicates removed.

A specification of the form \code{first:second} indicates the the set
of terms obtained by taking the interactions of all terms in
\code{first} with all terms in \code{second}.  The specification
\code{first*second} indicates the \emph{cross} of \code{first} and
\code{second}.  This is the same as \code{first + second +
  first:second}.

The terms in the formula will be re-ordered so that main effects come
first, followed by the interactions, all second-order, all third-order
and so on: to avoid this pass a \code{terms} object as the formula.

Non-\code{NULL} \code{weights} can be used to indicate that different
observations have different dispersions (with the values in
\code{weights} being inversely proportional to the dispersions); or
equivalently, when the elements of \code{weights} are positive
integers \eqn{w_i}{}, that each response \eqn{y_i}{} is the mean of
\eqn{w_i}{} unit-weight observations.  For a binomial GLM prior weights
are used to give the number of trials when the response is the
proportion of successes: they would rarely be used for a Poisson GLM.


\code{glm.fit} is the workhorse function: it is not normally called
directly but can be more efficient where the response vector and
design matrix have already been calculated.

If more than one of \code{etastart}, \code{start} and \code{mustart}
is specified, the first in the list will be used.  It is often
advisable to supply starting values for a \code{\LinkA{quasi}{quasi}} family,
and also for families with unusual links such as \code{gaussian("log")}.

All of \code{weights}, \code{subset}, \code{offset}, \code{etastart}
and \code{mustart} are evaluated in the same way as variables in
\code{formula}, that is first in \code{data} and then in the
environment of \code{formula}.

For the background to warning messages about `fitted probabilities
numerically 0 or 1 occurred' for binomial GLMs, see Venables \&
Ripley (2002, pp. 197--8).
\end{Details}
%
\begin{Value}
\code{glm} returns an object of class inheriting from \code{"glm"}
which inherits from the class \code{"lm"}. See later in this section.
If a non-standard \code{method} is used, the object will also inherit
from the class (if any) returned by that function.

The function \code{\LinkA{summary}{summary}} (i.e., \code{\LinkA{summary.glm}{summary.glm}}) can
be used to obtain or print a summary of the results and the function
\code{\LinkA{anova}{anova}} (i.e., \code{\LinkA{anova.glm}{anova.glm}})
to produce an analysis of variance table.

The generic accessor functions \code{\LinkA{coefficients}{coefficients}},
\code{effects}, \code{fitted.values} and \code{residuals} can be used to
extract various useful features of the value returned by \code{glm}.

\code{weights} extracts a vector of weights, one for each case in the
fit (after subsetting and \code{na.action}).

An object of class \code{"glm"} is a list containing at least the
following components:

\begin{ldescription}
\item[\code{coefficients}] a named vector of coefficients
\item[\code{residuals}] the \emph{working} residuals, that is the residuals
in the final iteration of the IWLS fit.  Since cases with zero
weights are omitted, their working residuals are \code{NA}.
\item[\code{fitted.values}] the fitted mean values, obtained by transforming
the linear predictors by the inverse of the link function.
\item[\code{rank}] the numeric rank of the fitted linear model.
\item[\code{family}] the \code{\LinkA{family}{family}} object used.
\item[\code{linear.predictors}] the linear fit on link scale.
\item[\code{deviance}] up to a constant, minus twice the maximized
log-likelihood.  Where sensible, the constant is chosen so that a
saturated model has deviance zero.
\item[\code{aic}] A version of Akaike's \emph{An Information Criterion},
minus twice the maximized log-likelihood plus twice the number of
parameters, computed by the \code{aic} component of the family.
For binomial and Poison families the dispersion is
fixed at one and the number of parameters is the number of
coefficients. For gaussian, Gamma and inverse gaussian families the
dispersion is estimated from the residual deviance, and the number
of parameters is the number of coefficients plus one.  For a
gaussian family the MLE of the dispersion is used so this is a valid
value of AIC, but for Gamma and inverse gaussian families it is not.
For families fitted by quasi-likelihood the value is \code{NA}.
\item[\code{null.deviance}] The deviance for the null model, comparable with
\code{deviance}. The null model will include the offset, and an
intercept if there is one in the model.  Note that this will be
incorrect if the link function depends on the data other than
through the fitted mean: specify a zero offset to force a correct
calculation.
\item[\code{iter}] the number of iterations of IWLS used.
\item[\code{weights}] the \emph{working} weights, that is the weights
in the final iteration of the IWLS fit.
\item[\code{prior.weights}] the weights initially supplied, a vector of
\code{1}s if none were.
\item[\code{df.residual}] the residual degrees of freedom.
\item[\code{df.null}] the residual degrees of freedom for the null model.
\item[\code{y}] if requested (the default) the \code{y} vector
used. (It is a vector even for a binomial model.)
\item[\code{x}] if requested, the model matrix.
\item[\code{model}] if requested (the default), the model frame.
\item[\code{converged}] logical. Was the IWLS algorithm judged to have converged?
\item[\code{boundary}] logical. Is the fitted value on the boundary of the
attainable values?
\item[\code{call}] the matched call.
\item[\code{formula}] the formula supplied.
\item[\code{terms}] the \code{\LinkA{terms}{terms}} object used.
\item[\code{data}] the \code{data argument}.
\item[\code{offset}] the offset vector used.
\item[\code{control}] the value of the \code{control} argument used.
\item[\code{method}] the name of the fitter function used, currently always
\code{"glm.fit"}.
\item[\code{contrasts}] (where relevant) the contrasts used.
\item[\code{xlevels}] (where relevant) a record of the levels of the factors
used in fitting.
\item[\code{na.action}] (where relevant) information returned by
\code{\LinkA{model.frame}{model.frame}} on the special handling of \code{NA}s.

\end{ldescription}
In addition, non-empty fits will have components \code{qr}, \code{R}
and \code{effects} relating to the final weighted linear fit.

Objects of class \code{"glm"} are normally of class \code{c("glm",
    "lm")}, that is inherit from class \code{"lm"}, and well-designed
methods for class \code{"lm"} will be applied to the weighted linear
model at the final iteration of IWLS.  However, care is needed, as
extractor functions for class \code{"glm"} such as
\code{\LinkA{residuals}{residuals}} and \code{weights} do \bold{not} just pick out
the component of the fit with the same name.

If a \code{\LinkA{binomial}{binomial}} \code{glm} model was specified by giving a
two-column response, the weights returned by \code{prior.weights} are
the total numbers of cases (factored by the supplied case weights) and
the component \code{y} of the result is the proportion of successes.
\end{Value}
%
\begin{Section}{Fitting functions}
The argument \code{method} serves two purposes.  One is to allow the
model frame to be recreated with no fitting.  The other is to allow
the default fitting function \code{glm.fit} to be replaced by a
function which takes the same arguments and uses a different fitting
algorithm.  If \code{glm.fit} is supplied as a character string it is
used to search for a function of that name, starting in the
\pkg{stats} namespace.

The class of the object return by the fitter (if any) will be
prepended to the class returned by \code{glm}.
\end{Section}
%
\begin{Author}\relax
The original \R{} implementation of \code{glm} was written by Simon
Davies working for Ross Ihaka at the University of Auckland, but has
since been extensively re-written by members of the R Core team.

The design was inspired by the S function of the same name described
in Hastie \& Pregibon (1992).
\end{Author}
%
\begin{References}\relax
Dobson, A. J. (1990)
\emph{An Introduction to Generalized Linear Models.}
London: Chapman and Hall.

Hastie, T. J. and Pregibon, D. (1992)
\emph{Generalized linear models.}
Chapter 6 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.

McCullagh P. and Nelder, J. A. (1989)
\emph{Generalized Linear Models.}
London: Chapman and Hall.

Venables, W. N. and Ripley, B. D. (2002)
\emph{Modern Applied Statistics with S.}
New York: Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{anova.glm}{anova.glm}}, \code{\LinkA{summary.glm}{summary.glm}}, etc. for
\code{glm} methods,
and the generic functions \code{\LinkA{anova}{anova}}, \code{\LinkA{summary}{summary}},
\code{\LinkA{effects}{effects}}, \code{\LinkA{fitted.values}{fitted.values}},
and \code{\LinkA{residuals}{residuals}}.

\code{\LinkA{lm}{lm}} for non-generalized \emph{linear} models (which SAS
calls GLMs, for `general' linear models).

\code{\LinkA{loglin}{loglin}} and \code{\LinkA{loglm}{loglm}} (package
\Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}) for fitting log-linear models (which binomial and
Poisson GLMs are) to contingency tables.

\code{bigglm} in package \Rhref{http://CRAN.R-project.org/package=biglm}{\pkg{biglm}} for an alternative
way to fit GLMs to large datasets (especially those with many cases).

\code{\LinkA{esoph}{esoph}}, \code{\LinkA{infert}{infert}} and
\code{\LinkA{predict.glm}{predict.glm}} have examples of fitting binomial glms.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Dobson (1990) Page 93: Randomized Controlled Trial :
counts <- c(18,17,15,20,10,20,25,13,12)
outcome <- gl(3,1,9)
treatment <- gl(3,3)
print(d.AD <- data.frame(treatment, outcome, counts))
glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
anova(glm.D93)
summary(glm.D93)

## an example with offsets from Venables & Ripley (2002, p.189)
utils::data(anorexia, package="MASS")

anorex.1 <- glm(Postwt ~ Prewt + Treat + offset(Prewt),
                family = gaussian, data = anorexia)
summary(anorex.1)

# A Gamma example, from McCullagh & Nelder (1989, pp. 300-2)
clotting <- data.frame(
    u = c(5,10,15,20,30,40,60,80,100),
    lot1 = c(118,58,42,35,27,25,21,19,18),
    lot2 = c(69,35,26,21,18,16,13,12,12))
summary(glm(lot1 ~ log(u), data=clotting, family=Gamma))
summary(glm(lot2 ~ log(u), data=clotting, family=Gamma))

## Not run: 
## for an example of the use of a terms object as a formula
demo(glm.vr)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{glm.control}{Auxiliary for Controlling GLM Fitting}{glm.control}
\keyword{optimize}{glm.control}
\keyword{models}{glm.control}
%
\begin{Description}\relax
Auxiliary function for \code{\LinkA{glm}{glm}} fitting.
Typically only used internally by \code{\LinkA{glm.fit}{glm.fit}}, but may be
used to construct a \code{control} argument to either function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
glm.control(epsilon = 1e-8, maxit = 25, trace = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{epsilon}] positive convergence tolerance \eqn{\epsilon}{};
the iterations converge when
\eqn{|dev - dev_{old}|/(|dev| + 0.1) < \epsilon}{}.
\item[\code{maxit}] integer giving the maximal number of IWLS iterations.
\item[\code{trace}] logical indicating if output should be produced for each
iteration.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \code{control} argument of \code{\LinkA{glm}{glm}} is by default passed
to the \code{control} argument of \code{\LinkA{glm.fit}{glm.fit}}, which uses
its elements as arguments to \code{glm.control}: the latter provides
defaults and sanity checking.

If \code{epsilon} is small (less than \eqn{10^{-10}}{}) it is
also used as the tolerance for the detection of collinearity in the
least squares solution.

When \code{trace} is true, calls to \code{\LinkA{cat}{cat}} produce the
output for each IWLS iteration.  Hence, \code{\LinkA{options}{options}(digits = *)}
can be used to increase the precision, see the example.
\end{Details}
%
\begin{Value}
A list with components named as the arguments.
\end{Value}
%
\begin{References}\relax
Hastie, T. J. and Pregibon, D. (1992)
\emph{Generalized linear models.}
Chapter 6 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{glm.fit}{glm.fit}}, the fitting procedure used by \code{\LinkA{glm}{glm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

### A variation on  example(glm) :

## Annette Dobson's example ...
counts <- c(18,17,15,20,10,20,25,13,12)
outcome <- gl(3,1,9)
treatment <- gl(3,3)
oo <- options(digits = 12) # to see more when tracing :
glm.D93X <- glm(counts ~ outcome + treatment, family=poisson(),
                trace = TRUE, epsilon = 1e-14)
options(oo)
coef(glm.D93X) # the last two are closer to 0 than in ?glm's  glm.D93
\end{ExampleCode}
\end{Examples}
\HeaderA{glm.summaries}{Accessing Generalized Linear Model Fits}{glm.summaries}
\aliasA{family.glm}{glm.summaries}{family.glm}
\aliasA{residuals.glm}{glm.summaries}{residuals.glm}
\keyword{models}{glm.summaries}
\keyword{regression}{glm.summaries}
%
\begin{Description}\relax
These functions are all \code{\LinkA{methods}{methods}} for class \code{glm} or
\code{summary.glm} objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'glm'
family(object, ...)

## S3 method for class 'glm'
residuals(object, type = c("deviance", "pearson", "working",
                           "response", "partial"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object of class \code{glm}, typically the result of
a call to \code{\LinkA{glm}{glm}}.
\item[\code{type}] the type of residuals which should be returned.
The alternatives are: \code{"deviance"} (default), \code{"pearson"},
\code{"working"}, \code{"response"}, and \code{"partial"}.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The references define the types of residuals: Davison \& Snell is a
good reference for the usages of each.

The partial residuals are a matrix of working residuals, with each
column formed by omitting a term from the model.

How \code{residuals} treats cases with missing values in the original
fit is determined by the \code{na.action} argument of that fit.
If \code{na.action = na.omit} omitted cases will not appear in the
residuals, whereas if \code{na.action = na.exclude} they will appear,
with residual value \code{NA}.  See also \code{\LinkA{naresid}{naresid}}.

For fits done with \code{y = FALSE} the response values are computed
from other components.
\end{Details}
%
\begin{References}\relax
Davison, A. C. and Snell, E. J. (1991)
\emph{Residuals and diagnostics.}  In: Statistical Theory
and Modelling. In Honour of Sir David Cox, FRS, eds.
Hinkley, D. V., Reid, N. and Snell, E. J., Chapman \& Hall.

Hastie, T. J. and Pregibon, D. (1992)
\emph{Generalized linear models.}
Chapter 6 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.

McCullagh P. and Nelder, J. A. (1989)
\emph{Generalized Linear Models.}
London: Chapman and Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{glm}{glm}} for computing \code{glm.obj}, \code{\LinkA{anova.glm}{anova.glm}};
the corresponding \emph{generic} functions, \code{\LinkA{summary.glm}{summary.glm}},
\code{\LinkA{coef}{coef}}, \code{\LinkA{deviance}{deviance}},
\code{\LinkA{df.residual}{df.residual}},
\code{\LinkA{effects}{effects}}, \code{\LinkA{fitted}{fitted}},
\code{\LinkA{residuals}{residuals}}.

\LinkA{influence.measures}{influence.measures} for deletion diagnostics, including
standardized (\code{\LinkA{rstandard}{rstandard}})
and studentized (\code{\LinkA{rstudent}{rstudent}}) residuals.
\end{SeeAlso}
\HeaderA{hclust}{Hierarchical Clustering}{hclust}
\aliasA{plclust}{hclust}{plclust}
\aliasA{plot.hclust}{hclust}{plot.hclust}
\aliasA{print.hclust}{hclust}{print.hclust}
\keyword{multivariate}{hclust}
\keyword{cluster}{hclust}
%
\begin{Description}\relax
Hierarchical cluster analysis on a set of dissimilarities and
methods for analyzing it.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
hclust(d, method = "complete", members=NULL)

## S3 method for class 'hclust'
plot(x, labels = NULL, hang = 0.1,
     axes = TRUE, frame.plot = FALSE, ann = TRUE,
     main = "Cluster Dendrogram",
     sub = NULL, xlab = NULL, ylab = "Height", ...)

plclust(tree, hang = 0.1, unit = FALSE, level = FALSE, hmin = 0,
        square = TRUE, labels = NULL, plot. = TRUE,
        axes = TRUE, frame.plot = FALSE, ann = TRUE,
        main = "", sub = NULL, xlab = NULL, ylab = "Height")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{d}] a dissimilarity structure as produced by \code{dist}.

\item[\code{method}] the agglomeration method to be used. This should
be (an unambiguous abbreviation of) one of
\code{"ward"}, \code{"single"}, \code{"complete"},
\code{"average"}, \code{"mcquitty"}, \code{"median"} or
\code{"centroid"}.

\item[\code{members}] \code{NULL} or a vector with length size of
\code{d}. See the `Details' section.

\item[\code{x,tree}] an object of the type produced by \code{hclust}.

\item[\code{hang}] The fraction of the plot height by which labels should hang
below the rest of the plot.
A negative value will cause the labels to hang down from 0.

\item[\code{labels}] A character vector of labels for the leaves of the
tree. By default the row names or row numbers of the original data are
used. If \code{labels=FALSE} no labels at all are plotted.

\item[\code{axes, frame.plot, ann}] logical flags as in \code{\LinkA{plot.default}{plot.default}}.
\item[\code{main, sub, xlab, ylab}] character strings for
\code{\LinkA{title}{title}}.  \code{sub} and \code{xlab} have a non-NULL
default when there's a \code{tree\$call}.
\item[\code{...}] Further graphical arguments.

\item[\code{unit}] logical.  If true, the splits are plotted at
equally-spaced heights rather than at the height in the object.

\item[\code{hmin}] numeric.  All heights less than \code{hmin} are regarded
as being \code{hmin}: this can be used to suppress detail at the
bottom of the tree.
\item[\code{level, square, plot.}] as yet unimplemented arguments of
\code{plclust} for S-PLUS compatibility.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This function performs a hierarchical cluster analysis
using a set of dissimilarities for the \eqn{n}{} objects being
clustered.  Initially, each object is assigned to its own
cluster and then the algorithm proceeds iteratively,
at each stage joining the two most similar clusters,
continuing until there is just a single cluster.
At each stage distances between clusters are recomputed
by the Lance--Williams dissimilarity update formula
according to the particular clustering method being used.

A number of different clustering methods are provided.  \emph{Ward's}
minimum variance method aims at finding compact, spherical clusters.
The \emph{complete linkage} method finds similar clusters. The
\emph{single linkage} method (which is closely related to the minimal
spanning tree) adopts a `friends of friends' clustering
strategy.  The other methods can be regarded as aiming for clusters
with characteristics somewhere between the single and complete link
methods.  Note however, that methods \code{"median"} and
\code{"centroid"} are \emph{not} leading to a \emph{monotone distance}
measure, or equivalently the resulting dendrograms can have so called
\emph{inversions} (which are hard to interpret).

If \code{members!=NULL}, then \code{d} is taken to be a
dissimilarity matrix between clusters instead of dissimilarities
between singletons and \code{members} gives the number of observations
per cluster.  This way the hierarchical cluster algorithm can be
`started in the middle of the dendrogram', e.g., in order to
reconstruct the part of the tree above a cut (see examples).
Dissimilarities between clusters can be efficiently computed (i.e.,
without \code{hclust} itself) only for a limited number of
distance/linkage combinations, the simplest one being squared
Euclidean distance and centroid linkage.  In this case the
dissimilarities between the clusters are the squared Euclidean
distances between cluster means.

In hierarchical cluster displays, a decision is needed at each merge to
specify which subtree should go on the left and which on the right.
Since, for \eqn{n}{} observations there are \eqn{n-1}{} merges,
there are \eqn{2^{(n-1)}}{} possible orderings for the leaves
in a cluster tree, or dendrogram.
The algorithm used in \code{hclust} is to order the subtree so that
the tighter cluster is on the left (the last, i.e., most recent,
merge of the left subtree is at a lower value than the last
merge of the right subtree).
Single observations are the tightest clusters possible,
and merges involving two observations place them in order by their
observation sequence number.
\end{Details}
%
\begin{Value}
An object of class \bold{hclust} which describes the
tree produced by the clustering process.
The object is a list with components:
\begin{ldescription}
\item[\code{merge}] an \eqn{n-1}{} by 2 matrix.
Row \eqn{i}{} of \code{merge} describes the merging of clusters
at step \eqn{i}{} of the clustering.
If an element \eqn{j}{} in the row is negative,
then observation \eqn{-j}{} was merged at this stage.
If \eqn{j}{} is positive then the merge
was with the cluster formed at the (earlier) stage \eqn{j}{}
of the algorithm.
Thus negative entries in \code{merge} indicate agglomerations
of singletons, and positive entries indicate agglomerations
of non-singletons.

\item[\code{height}] a set of \eqn{n-1}{} real values (non-decreasing for
ultrametric trees).
The clustering \emph{height}: that is, the value of
the criterion associated with the clustering
\code{method} for the particular agglomeration.

\item[\code{order}] a vector giving the permutation of the original
observations suitable for plotting, in the sense that a cluster
plot using this ordering and matrix \code{merge} will not have
crossings of the branches.

\item[\code{labels}] labels for each of the objects being clustered.

\item[\code{call}] the call which produced the result.

\item[\code{method}] the cluster method that has been used.

\item[\code{dist.method}] the distance that has been used to create \code{d}
(only returned if the distance object has a \code{"method"}
attribute).

\end{ldescription}
There are \code{\LinkA{print}{print}}, \code{\LinkA{plot}{plot}} and \code{identify}
(see \code{\LinkA{identify.hclust}{identify.hclust}}) methods and the
\code{\LinkA{rect.hclust}{rect.hclust}()} function for \code{hclust} objects.
The \code{plclust()} function is basically the same as the plot method,
\code{plot.hclust}, primarily for back compatibility with S-PLUS. Its
extra arguments are not yet implemented.
\end{Value}
%
\begin{Author}\relax
The \code{hclust} function is based on Fortran code
contributed to STATLIB by F. Murtagh.
\end{Author}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole. (S version.)

Everitt, B. (1974).
\emph{Cluster Analysis}.
London: Heinemann Educ. Books.

Hartigan, J. A. (1975).
\emph{Clustering  Algorithms}.
New York: Wiley.

Sneath, P. H. A. and R. R. Sokal (1973).
\emph{Numerical Taxonomy}.
San Francisco: Freeman.

Anderberg, M. R. (1973).
\emph{Cluster Analysis for Applications}.
Academic Press: New York.

Gordon, A. D. (1999).
\emph{Classification}. Second Edition.
London: Chapman and Hall / CRC

Murtagh, F. (1985).
``Multidimensional Clustering Algorithms'', in
\emph{COMPSTAT Lectures 4}.
Wuerzburg: Physica-Verlag
(for algorithmic details of algorithms used).

McQuitty, L.L. (1966).
Similarity Analysis by Reciprocal Pairs for Discrete and Continuous
Data.
\emph{Educational and Psychological Measurement}, \bold{26}, 825--831.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{identify.hclust}{identify.hclust}}, \code{\LinkA{rect.hclust}{rect.hclust}},
\code{\LinkA{cutree}{cutree}}, \code{\LinkA{dendrogram}{dendrogram}}, \code{\LinkA{kmeans}{kmeans}}.

For the Lance--Williams formula and methods that apply it generally,
see \code{\LinkA{agnes}{agnes}} from package \Rhref{http://CRAN.R-project.org/package=cluster}{\pkg{cluster}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

hc <- hclust(dist(USArrests), "ave")
plot(hc)
plot(hc, hang = -1)

## Do the same with centroid clustering and squared Euclidean distance,
## cut the tree into ten clusters and reconstruct the upper part of the
## tree from the cluster centers.
hc <- hclust(dist(USArrests)^2, "cen")
memb <- cutree(hc, k = 10)
cent <- NULL
for(k in 1:10){
  cent <- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
}
hc1 <- hclust(dist(cent)^2, method = "cen", members = table(memb))
opar <- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)
\end{ExampleCode}
\end{Examples}
\HeaderA{heatmap}{ Draw a Heat Map }{heatmap}
\keyword{hplot}{heatmap}
%
\begin{Description}\relax
A heat map is a false color image (basically
\code{\LinkA{image}{image}(t(x))}) with a dendrogram added to the left side
and to the top.  Typically, reordering of the rows and columns
according to some set of values (row or column means) within the
restrictions imposed by the dendrogram is carried out.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
heatmap(x, Rowv=NULL, Colv=if(symm)"Rowv" else NULL,
        distfun = dist, hclustfun = hclust,
        reorderfun = function(d,w) reorder(d,w),
        add.expr, symm = FALSE, revC = identical(Colv, "Rowv"),
        scale=c("row", "column", "none"), na.rm = TRUE,
        margins = c(5, 5), ColSideColors, RowSideColors,
        cexRow = 0.2 + 1/log10(nr), cexCol = 0.2 + 1/log10(nc),
        labRow = NULL, labCol = NULL, main = NULL,
        xlab = NULL, ylab = NULL,
        keep.dendro = FALSE, verbose = getOption("verbose"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric matrix of the values to be plotted. 
\item[\code{Rowv}] determines if and how the \emph{row} dendrogram should be
computed and reordered.  Either a \code{\LinkA{dendrogram}{dendrogram}} or a
vector of values used to reorder the row dendrogram or
\code{\LinkA{NA}{NA}} to suppress any row dendrogram (and reordering) or
by default, \code{\LinkA{NULL}{NULL}}, see `Details' below.
\item[\code{Colv}] determines if and how the \emph{column} dendrogram should be
reordered.  Has the same options as the \code{Rowv} argument above and
\emph{additionally} when \code{x} is a square matrix, \code{Colv =
      "Rowv"} means that columns should be treated identically to the
rows (and so if there is to be no row dendrogram there will not be a
column one either).
\item[\code{distfun}] function used to compute the distance (dissimilarity)
between both rows and columns.  Defaults to \code{\LinkA{dist}{dist}}.
\item[\code{hclustfun}] function used to compute the hierarchical clustering
when \code{Rowv} or \code{Colv} are not dendrograms.  Defaults to
\code{\LinkA{hclust}{hclust}}. Should take as argument a result of \code{distfun}
and return an object to which \code{\LinkA{as.dendrogram}{as.dendrogram}} can be applied.
\item[\code{reorderfun}] function(d,w) of dendrogram and weights for
reordering the row and column dendrograms.  The default uses
\code{\LinkA{reorder.dendrogram}{reorder.dendrogram}}.
\item[\code{add.expr}] expression that will be evaluated after the call to
\code{image}.  Can be used to add components to the plot.
\item[\code{symm}] logical indicating if \code{x} should be treated
\bold{symm}etrically; can only be true when \code{x} is a square matrix.
\item[\code{revC}] logical indicating if the column order should be
\code{\LinkA{rev}{rev}}ersed for plotting, such that e.g., for the
symmetric case, the symmetry axis is as usual.
\item[\code{scale}] character indicating if the values should be centered and
scaled in either the row direction or the column direction, or
none.  The default is \code{"row"} if \code{symm} false, and
\code{"none"} otherwise.
\item[\code{na.rm}] logical indicating whether \code{NA}'s should be removed.
\item[\code{margins}] numeric vector of length 2 containing the margins
(see \code{\LinkA{par}{par}(mar= *)}) for column and row names, respectively.
\item[\code{ColSideColors}] (optional) character vector of length \code{ncol(x)}
containing the color names for a horizontal side bar that may be used to
annotate the columns of \code{x}.
\item[\code{RowSideColors}] (optional) character vector of length \code{nrow(x)}
containing the color names for a vertical side bar that may be used to
annotate the rows of \code{x}.
\item[\code{cexRow, cexCol}] positive numbers, used as \code{cex.axis} in
for the row or column axis labeling.  The defaults currently only
use number of rows or columns, respectively.
\item[\code{labRow, labCol}] character vectors with row and column labels to
use; these default to \code{rownames(x)} or \code{colnames(x)},
respectively.
\item[\code{main, xlab, ylab}] main, x- and y-axis titles; defaults to none.
\item[\code{keep.dendro}] logical indicating if the dendrogram(s) should be
kept as part of the result (when \code{Rowv} and/or \code{Colv} are
not NA).
\item[\code{verbose}] logical indicating if information should be printed.
\item[\code{...}] additional arguments passed on to \code{\LinkA{image}{image}},
e.g., \code{col} specifying the colors.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If either \code{Rowv} or \code{Colv} are dendrograms they are honored
(and not reordered).  Otherwise, dendrograms are computed as
\code{dd <- as.dendrogram(hclustfun(distfun(X)))} where \code{X} is
either \code{x} or \code{t(x)}.

If either is a vector (of `weights') then the appropriate
dendrogram is reordered according to the supplied values subject to
the constraints imposed by the dendrogram, by \code{\LinkA{reorder}{reorder}(dd,
    Rowv)}, in the row case.
If either is missing, as by default, then the ordering of the
corresponding dendrogram is by the mean value of the rows/columns,
i.e., in the case of rows, \code{Rowv <- rowMeans(x, na.rm=na.rm)}.
If either is \code{\LinkA{NA}{NA}}, \emph{no reordering} will be done for
the corresponding side.

By default (\code{scale = "row"}) the rows are scaled to have mean
zero and standard deviation one.  There is some empirical evidence
from genomic plotting that this is useful.

The default colors are not pretty.  Consider using enhancements such
as the \Rhref{http://CRAN.R-project.org/package=RColorBrewer}{\pkg{RColorBrewer}} package.
\end{Details}
%
\begin{Value}
Invisibly, a list with components
\begin{ldescription}
\item[\code{rowInd}] \bold{r}ow index permutation vector as returned by
\code{\LinkA{order.dendrogram}{order.dendrogram}}.
\item[\code{colInd}] \bold{c}olumn index permutation vector.
\item[\code{Rowv}] the row dendrogram; only if input \code{Rowv} was not NA
and \code{keep.dendro} is true.
\item[\code{Colv}] the column dendrogram; only if input \code{Colv} was not NA
and \code{keep.dendro} is true.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Unless \code{Rowv = NA} (or \code{Colw = NA}), the original rows and
columns are reordered \emph{in any case} to match the dendrogram,
e.g., the rows by \code{\LinkA{order.dendrogram}{order.dendrogram}(Rowv)} where
\code{Rowv} is the (possibly \code{\LinkA{reorder}{reorder}()}ed) row
dendrogram.

\code{heatmap()} uses \code{\LinkA{layout}{layout}} and draws the
\code{\LinkA{image}{image}} in the lower right corner of a 2x2 layout.
Consequentially, it can \bold{not} be used in a multi column/row
layout, i.e., when \code{\LinkA{par}{par}(mfrow= *)} or \code{(mfcol= *)}
has been called.
\end{Note}
%
\begin{Author}\relax
Andy Liaw, original; R. Gentleman, M. Maechler, W. Huber, revisions.
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{image}{image}}, \code{\LinkA{hclust}{hclust}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics); require(grDevices)
x  <- as.matrix(mtcars)
rc <- rainbow(nrow(x), start=0, end=.3)
cc <- rainbow(ncol(x), start=0, end=.3)
hv <- heatmap(x, col = cm.colors(256), scale="column",
              RowSideColors = rc, ColSideColors = cc, margins=c(5,10),
              xlab = "specification variables", ylab= "Car Models",
              main = "heatmap(<Mtcars data>, ..., scale = \"column\")")
utils::str(hv) # the two re-ordering index vectors

## no column dendrogram (nor reordering) at all:
heatmap(x, Colv = NA, col = cm.colors(256), scale="column",
        RowSideColors = rc, margins=c(5,10),
        xlab = "specification variables", ylab= "Car Models",
        main = "heatmap(<Mtcars data>, ..., scale = \"column\")")

## "no nothing"
heatmap(x, Rowv = NA, Colv = NA, scale="column",
        main = "heatmap(*, NA, NA) ~= image(t(x))")


round(Ca <- cor(attitude), 2)
symnum(Ca) # simple graphic
heatmap(Ca,             symm = TRUE, margins=c(6,6))# with reorder()
heatmap(Ca, Rowv=FALSE, symm = TRUE, margins=c(6,6))# _NO_ reorder()
## slightly artificial with color bar, without and with ordering:
cc <- rainbow(nrow(Ca))
heatmap(Ca, Rowv=FALSE, symm = TRUE, RowSideColors=cc, ColSideColors=cc,
	margins=c(6,6))
heatmap(Ca,		symm = TRUE, RowSideColors=cc, ColSideColors=cc,
	margins=c(6,6))

## For variable clustering, rather use distance based on cor():
symnum( cU <- cor(USJudgeRatings) )

hU <- heatmap(cU, Rowv = FALSE, symm = TRUE, col = topo.colors(16),
             distfun = function(c) as.dist(1 - c), keep.dendro = TRUE)
## The Correlation matrix with same reordering:
round(100 * cU[hU[[1]], hU[[2]]])
## The column dendrogram:
utils::str(hU$Colv)
\end{ExampleCode}
\end{Examples}
\HeaderA{HoltWinters}{Holt-Winters Filtering}{HoltWinters}
\aliasA{print.HoltWinters}{HoltWinters}{print.HoltWinters}
\aliasA{residuals.HoltWinters}{HoltWinters}{residuals.HoltWinters}
\keyword{ts}{HoltWinters}
%
\begin{Description}\relax
Computes Holt-Winters Filtering of a given time series.
Unknown parameters are determined by minimizing the squared
prediction error.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
HoltWinters(x, alpha = NULL, beta = NULL, gamma = NULL,
            seasonal = c("additive", "multiplicative"),
            start.periods = 2, l.start = NULL, b.start = NULL,
            s.start = NULL,
            optim.start = c(alpha = 0.3, beta = 0.1, gamma = 0.1),
            optim.control = list())
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] An object of class \code{ts}
\item[\code{alpha}] \eqn{alpha}{} parameter of Holt-Winters Filter.
\item[\code{beta}] \eqn{beta}{} parameter of Holt-Winters Filter. If set to
\code{FALSE}, the function will do exponential smoothing.
\item[\code{gamma}] \eqn{gamma}{} parameter used for the seasonal component.
If set to \code{FALSE}, an non-seasonal model is fitted.
\item[\code{seasonal}] Character string to select an \code{"additive"}
(the default) or \code{"multiplicative"} seasonal model. The first
few characters are sufficient. (Only takes effect if
\code{gamma} is non-zero).
\item[\code{start.periods}] Start periods used in the autodetection of start
values. Must be at least 2.
\item[\code{l.start}] Start value for level (a[0]).
\item[\code{b.start}] Start value for trend (b[0]).
\item[\code{s.start}] Vector of start values for the seasonal component
(\eqn{s_1[0] \ldots s_p[0]}{})
\item[\code{optim.start}] Vector with named components \code{alpha},
\code{beta}, and \code{gamma} containing the starting values for the
optimizer. Only the values needed must be specified.  Ignored in the
one-parameter case.
\item[\code{optim.control}] Optional list with additional control parameters
passed to \code{optim} if this is used.  Ignored in the
one-parameter case.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The additive Holt-Winters prediction function (for time series with
period length p) is
\deqn{\hat Y[t+h] = a[t] + h b[t] + s[t - p + 1 + (h - 1) \bmod p],}{}
where \eqn{a[t]}{}, \eqn{b[t]}{} and \eqn{s[t]}{} are given by
\deqn{a[t] = \alpha (Y[t] - s[t-p])  + (1-\alpha) (a[t-1] + b[t-1])}{}
\deqn{b[t] = \beta (a[t] -a[t-1]) + (1-\beta)  b[t-1]}{}
\deqn{s[t] = \gamma (Y[t] - a[t])   + (1-\gamma) s[t-p]}{}

The multiplicative Holt-Winters prediction function (for time series
with period length p) is
\deqn{\hat Y[t+h] = (a[t] + h b[t]) \times s[t - p + 1 + (h - 1) \bmod p].}{}
where \eqn{a[t]}{}, \eqn{b[t]}{} and \eqn{s[t]}{} are given by
\deqn{a[t] = \alpha (Y[t] / s[t-p])  + (1-\alpha) (a[t-1] + b[t-1])}{}
\deqn{b[t] = \beta (a[t] - a[t-1]) + (1-\beta) b[t-1]}{}
\deqn{s[t] = \gamma (Y[t] / a[t])   + (1-\gamma) s[t-p]}{}
The data in \code{x} are required to be non-zero for a multiplicative
model, but it makes most sense if they are all positive.

The function tries to find the optimal values of \eqn{\alpha}{} and/or
\eqn{\beta}{} and/or \eqn{\gamma}{} by minimizing the squared one-step
prediction error if they are \code{NULL} (the default). \code{optimize}
will be used for the single-parameter case, and \code{optim} otherwise.

For seasonal models, start values for \code{a}, \code{b} and \code{s}
are inferred by performing a simple decomposition in trend and
seasonal component using moving averages (see function
\code{\LinkA{decompose}{decompose}}) on the \code{start.periods} first periods (a simple
linear regression on the trend component is used for starting level
and trend). For level/trend-models (no seasonal component), start
values for \code{a} and \code{b} are \code{x[2]} and \code{x[2] -
  x[1]}, respectively. For level-only models (ordinary exponential
smoothing), the start value for \code{a} is \code{x[1]}.
\end{Details}
%
\begin{Value}
An object of class \code{"HoltWinters"}, a list with components:
\begin{ldescription}
\item[\code{fitted}] A multiple time series with one column for the
filtered series as well as for the level, trend and seasonal
components, estimated contemporaneously (that is at time t and not
at the end of the series).
\item[\code{x}] The original series
\item[\code{alpha}] alpha used for filtering
\item[\code{beta}] beta used for filtering
\item[\code{gamma}] gamma used for filtering
\item[\code{coefficients}] A vector with named components \code{a, b, s1, ..., sp}
containing the estimated values for the level, trend and seasonal
components
\item[\code{seasonal}] The specified \code{seasonal} parameter
\item[\code{SSE}] The final sum of squared errors achieved in optimizing
\item[\code{call}] The call used
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
David Meyer \email{David.Meyer@wu.ac.at}
\end{Author}
%
\begin{References}\relax
C. C. Holt (1957)
Forecasting trends and seasonals by exponentially weighted
moving averages,
\emph{ONR Research Memorandum, Carnegie Institute of Technology} \bold{52}.

P. R. Winters (1960)
Forecasting sales by exponentially weighted moving averages,
\emph{Management Science} \bold{6}, 324--342.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{predict.HoltWinters}{predict.HoltWinters}}, \code{\LinkA{optim}{optim}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(graphics)

## Seasonal Holt-Winters
(m <- HoltWinters(co2))
plot(m)
plot(fitted(m))

(m <- HoltWinters(AirPassengers, seasonal = "mult"))
plot(m)

## Non-Seasonal Holt-Winters
x <- uspop + rnorm(uspop, sd = 5)
m <- HoltWinters(x, gamma = FALSE)
plot(m)

## Exponential Smoothing
m2 <- HoltWinters(x, gamma = FALSE, beta = FALSE)
lines(fitted(m2)[,1], col = 3)

\end{ExampleCode}
\end{Examples}
\HeaderA{Hypergeometric}{The Hypergeometric Distribution}{Hypergeometric}
\aliasA{dhyper}{Hypergeometric}{dhyper}
\aliasA{phyper}{Hypergeometric}{phyper}
\aliasA{qhyper}{Hypergeometric}{qhyper}
\aliasA{rhyper}{Hypergeometric}{rhyper}
\keyword{distribution}{Hypergeometric}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the hypergeometric distribution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dhyper(x, m, n, k, log = FALSE)
phyper(q, m, n, k, lower.tail = TRUE, log.p = FALSE)
qhyper(p, m, n, k, lower.tail = TRUE, log.p = FALSE)
rhyper(nn, m, n, k)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles representing the number of white balls
drawn without replacement from an urn which contains both black and
white balls.
\item[\code{m}] the number of white balls in the urn.
\item[\code{n}] the number of black balls in the urn.
\item[\code{k}] the number of balls drawn from the urn.
\item[\code{p}] probability, it must be between 0 and 1.
\item[\code{nn}] number of observations. If \code{length(nn) > 1}, the length
is taken to be the number required.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The hypergeometric distribution is used for sampling \emph{without}
replacement.  The density of this distribution with parameters
\code{m}, \code{n} and \code{k} (named \eqn{Np}{}, \eqn{N-Np}{}, and
\eqn{n}{}, respectively in the reference below) is given by
\deqn{
    p(x) = \left. {m \choose x}{n \choose k-x} \right/ {m+n \choose k}%
  }{}
for \eqn{x = 0, \ldots, k}{}.

The quantile is defined as the smallest value \eqn{x}{} such that
\eqn{F(x) \ge p}{}, where \eqn{F}{} is the distribution function.
\end{Details}
%
\begin{Value}
\code{dhyper} gives the density,
\code{phyper} gives the distribution function,
\code{qhyper} gives the quantile function, and
\code{rhyper} generates random deviates.

Invalid arguments will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Source}\relax
\code{dhyper} computes via binomial probabilities, using code
contributed by Catherine Loader (see \code{\LinkA{dbinom}{dbinom}}).

\code{phyper} is based on calculating \code{dhyper} and
\code{phyper(...)/dhyper(...)} (as a summation), based on ideas of Ian
Smith and Morten Welinder.

\code{qhyper} is based on inversion.

\code{rhyper} is based on a corrected version of

Kachitvichyanukul, V. and Schmeiser, B. (1985).
Computer generation of hypergeometric random variates.
\emph{Journal of Statistical Computation and Simulation},
\bold{22}, 127--145.
\end{Source}
%
\begin{References}\relax
Johnson, N. L., Kotz, S., and Kemp, A. W. (1992)
\emph{Univariate Discrete Distributions},
Second Edition. New York: Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
m <- 10; n <- 7; k <- 8
x <- 0:(k+1)
rbind(phyper(x, m, n, k), dhyper(x, m, n, k))
all(phyper(x, m, n, k) == cumsum(dhyper(x, m, n, k)))# FALSE
## but error is very small:
signif(phyper(x, m, n, k) - cumsum(dhyper(x, m, n, k)), digits=3)
\end{ExampleCode}
\end{Examples}
\HeaderA{identify.hclust}{Identify Clusters in a Dendrogram}{identify.hclust}
\keyword{cluster}{identify.hclust}
\keyword{iplot}{identify.hclust}
%
\begin{Description}\relax
\code{identify.hclust} reads the position of the graphics pointer when the
(first) mouse button is pressed.  It then cuts the tree at the
vertical position of the pointer and highlights the cluster containing
the horizontal position of the pointer.  Optionally a function is
applied to the index of data points contained in the cluster.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'hclust'
identify(x, FUN = NULL, N = 20, MAXCLUSTER = 20, DEV.FUN = NULL,
          ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object of the type produced by \code{hclust}.
\item[\code{FUN}] (optional) function to be applied to the index numbers of
the data points in a cluster (see `Details' below).
\item[\code{N}] the maximum number of clusters to be identified.
\item[\code{MAXCLUSTER}] the maximum number of clusters that can be produced
by a cut (limits the effective vertical range of the pointer). 
\item[\code{DEV.FUN}] (optional) integer scalar. If specified, the
corresponding graphics device is made active before \code{FUN} is
applied.
\item[\code{...}] further arguments to \code{FUN}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
By default clusters can be identified using the mouse and an
\code{\LinkA{invisible}{invisible}} list of indices of the respective data points
is returned.

If \code{FUN} is not \code{NULL}, then the index vector of data points
is passed to this function as first argument, see the examples
below.  The active graphics device for \code{FUN} can be specified using
\code{DEV.FUN}.

The identification process is terminated by pressing any mouse
button other than the first, see also \code{\LinkA{identify}{identify}}.
\end{Details}
%
\begin{Value}
Either a list of data point index vectors or a list of return values
of \code{FUN}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{hclust}{hclust}},
\code{\LinkA{rect.hclust}{rect.hclust}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
require(graphics)

hca <- hclust(dist(USArrests))
plot(hca)
(x <- identify(hca)) ##  Terminate with 2nd mouse button !!

hci <- hclust(dist(iris[,1:4]))
plot(hci)
identify(hci, function(k) print(table(iris[k,5])))

# open a new device (one for dendrogram, one for bars):
get(getOption("device"))() # << make that narrow (& small)
                           # and *beside* 1st one
nD <- dev.cur()            # to be for the barplot
dev.set(dev.prev())# old one for dendrogram
plot(hci)
## select subtrees in dendrogram and "see" the species distribution:
identify(hci, function(k) barplot(table(iris[k,5]),col=2:4), DEV.FUN = nD)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{influence.measures}{Regression Deletion Diagnostics}{influence.measures}
\aliasA{cooks.distance}{influence.measures}{cooks.distance}
\methaliasA{cooks.distance.glm}{influence.measures}{cooks.distance.glm}
\methaliasA{cooks.distance.lm}{influence.measures}{cooks.distance.lm}
\aliasA{covratio}{influence.measures}{covratio}
\aliasA{dfbeta}{influence.measures}{dfbeta}
\methaliasA{dfbeta.lm}{influence.measures}{dfbeta.lm}
\aliasA{dfbetas}{influence.measures}{dfbetas}
\methaliasA{dfbetas.lm}{influence.measures}{dfbetas.lm}
\aliasA{dffits}{influence.measures}{dffits}
\aliasA{hat}{influence.measures}{hat}
\aliasA{hatvalues}{influence.measures}{hatvalues}
\methaliasA{hatvalues.lm}{influence.measures}{hatvalues.lm}
\aliasA{print.infl}{influence.measures}{print.infl}
\aliasA{rstandard}{influence.measures}{rstandard}
\methaliasA{rstandard.glm}{influence.measures}{rstandard.glm}
\methaliasA{rstandard.lm}{influence.measures}{rstandard.lm}
\aliasA{rstudent}{influence.measures}{rstudent}
\methaliasA{rstudent.glm}{influence.measures}{rstudent.glm}
\methaliasA{rstudent.lm}{influence.measures}{rstudent.lm}
\aliasA{summary.infl}{influence.measures}{summary.infl}
\keyword{regression}{influence.measures}
%
\begin{Description}\relax
This suite of functions can be used to compute some of the regression
(leave-one-out deletion) diagnostics for linear and generalized linear
models discussed in Belsley, Kuh and Welsch (1980), Cook and Weisberg (1982),
etc.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
influence.measures(model)

rstandard(model, ...)
## S3 method for class 'lm'
rstandard(model, infl = lm.influence(model, do.coef = FALSE),
          sd = sqrt(deviance(model)/df.residual(model)), ...)
## S3 method for class 'glm'
rstandard(model, infl=influence(model, do.coef=FALSE),
          type=c("deviance","pearson"), ...)

rstudent(model, ...)
## S3 method for class 'lm'
rstudent(model, infl = lm.influence(model, do.coef = FALSE),
         res = infl$wt.res, ...)
## S3 method for class 'glm'
rstudent(model, infl = influence(model, do.coef = FALSE), ...)

dffits(model, infl = , res = )

dfbeta(model, ...)
## S3 method for class 'lm'
dfbeta(model, infl = lm.influence(model, do.coef = TRUE), ...)

dfbetas(model, ...)
## S3 method for class 'lm'
dfbetas(model, infl = lm.influence(model, do.coef = TRUE), ...)

covratio(model, infl = lm.influence(model, do.coef = FALSE),
         res = weighted.residuals(model))

cooks.distance(model, ...)
## S3 method for class 'lm'
cooks.distance(model, infl = lm.influence(model, do.coef = FALSE),
               res = weighted.residuals(model),
               sd = sqrt(deviance(model)/df.residual(model)),
               hat = infl$hat, ...)
## S3 method for class 'glm'
cooks.distance(model, infl = influence(model, do.coef = FALSE),
               res = infl$pear.res,
               dispersion = summary(model)$dispersion,
               hat = infl$hat, ...)

hatvalues(model, ...)
## S3 method for class 'lm'
hatvalues(model, infl = lm.influence(model, do.coef = FALSE), ...)

hat(x, intercept = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] an \R{} object, typically returned by \code{\LinkA{lm}{lm}} or
\code{\LinkA{glm}{glm}}.
\item[\code{infl}] influence structure as returned by
\code{\LinkA{lm.influence}{lm.influence}} or \code{\LinkA{influence}{influence}} (the latter
only for the \code{glm} method of \code{rstudent} and
\code{cooks.distance}).
\item[\code{res}] (possibly weighted) residuals, with proper default.
\item[\code{sd}] standard deviation to use, see default.
\item[\code{dispersion}] dispersion (for \code{\LinkA{glm}{glm}} objects) to use,
see default.
\item[\code{hat}] hat values \eqn{H_{ii}}{}, see default.
\item[\code{type}] type of residuals for \code{glm} method for \code{rstandard.}

\item[\code{x}] the \eqn{X}{} or design matrix.
\item[\code{intercept}] should an intercept column be prepended to \code{x}?
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The primary high-level function is \code{influence.measures} which produces a
class \code{"infl"} object tabular display showing the DFBETAS for
each model variable, DFFITS, covariance ratios, Cook's distances and
the diagonal elements of the hat matrix.  Cases which are influential
with respect to any of these measures are marked with an asterisk.

The functions \code{dfbetas}, \code{dffits},
\code{covratio} and \code{cooks.distance} provide direct access to the
corresponding diagnostic quantities.  Functions \code{rstandard} and
\code{rstudent} give the standardized and Studentized residuals
respectively. (These re-normalize the residuals to have unit variance,
using an overall and leave-one-out measure of the error variance
respectively.)

Values for generalized linear models are approximations, as described
in Williams (1987) (except that Cook's distances are scaled as
\eqn{F}{} rather than as chi-square values).  The approximations can be
poor when some cases have large influence.

The optional \code{infl}, \code{res} and \code{sd} arguments are there
to encourage the use of these direct access functions, in situations
where, e.g., the underlying basic influence measures (from
\code{\LinkA{lm.influence}{lm.influence}} or the generic \code{\LinkA{influence}{influence}}) are
already available.

Note that cases with \code{weights == 0} are \emph{dropped} from all
these functions, but that if a linear model has been fitted with
\code{na.action = na.exclude}, suitable values are filled in for the
cases excluded during fitting.

The function \code{hat()} exists mainly for S (version 2)
compatibility; we recommend using \code{hatvalues()} instead.
\end{Details}
%
\begin{Note}\relax
For \code{hatvalues}, \code{dfbeta}, and \code{dfbetas}, the method
for linear models also works for generalized linear models.
\end{Note}
%
\begin{Author}\relax
Several R core team members and John Fox, originally in his \file{car}
package.
\end{Author}
%
\begin{References}\relax
Belsley, D. A., Kuh, E. and Welsch, R. E. (1980)
\emph{Regression Diagnostics}.
New York: Wiley.

Cook, R. D. and Weisberg, S. (1982)
\emph{Residuals and Influence in Regression}.
London: Chapman and Hall.

Williams, D. A. (1987)
Generalized linear model diagnostics using the deviance and single
case deletions. \emph{Applied Statistics} \bold{36}, 181--191.

Fox, J. (1997)
\emph{Applied Regression, Linear Models, and Related Methods}. Sage.

Fox, J. (2002)
\emph{An R and S-Plus Companion to Applied Regression}.
Sage Publ.; \url{http://www.socsci.mcmaster.ca/jfox/Books/Companion/}.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{influence}{influence}} (containing \code{\LinkA{lm.influence}{lm.influence}}).

`\LinkA{plotmath}{plotmath}' for the use of \code{hat} in plot annotation.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(graphics)

## Analysis of the life-cycle savings data
## given in Belsley, Kuh and Welsch.
lm.SR <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)

inflm.SR <- influence.measures(lm.SR)
which(apply(inflm.SR$is.inf, 1, any))
# which observations 'are' influential
summary(inflm.SR) # only these
inflm.SR          # all
plot(rstudent(lm.SR) ~ hatvalues(lm.SR)) # recommended by some

## The 'infl' argument is not needed, but avoids recomputation:
rs <- rstandard(lm.SR)
iflSR <- influence(lm.SR)
identical(rs, rstandard(lm.SR, infl = iflSR))
## to "see" the larger values:
1000 * round(dfbetas(lm.SR, infl = iflSR), 3)

## Huber's data [Atkinson 1985]
xh <- c(-4:0, 10)
yh <- c(2.48, .73, -.04, -1.44, -1.32, 0)
summary(lmH <- lm(yh ~ xh))
(im <- influence.measures(lmH))
plot(xh,yh, main = "Huber's data: L.S. line and influential obs.")
abline(lmH); points(xh[im$is.inf], yh[im$is.inf], pch=20, col=2)

## Irwin's data [Williams 1987]
xi <- 1:5
yi <- c(0,2,14,19,30) # number of mice responding to dose xi
mi <- rep(40, 5)      # number of mice exposed
summary(lmI <- glm(cbind(yi, mi -yi) ~ xi, family = binomial))
signif(cooks.distance(lmI), 3)# ~= Ci in Table 3, p.184
(imI <- influence.measures(lmI))
stopifnot(all.equal(imI$infmat[,"cook.d"],
          cooks.distance(lmI)))
\end{ExampleCode}
\end{Examples}
\HeaderA{integrate}{Integration of One-Dimensional Functions}{integrate}
\aliasA{print.integrate}{integrate}{print.integrate}
\keyword{math}{integrate}
\keyword{utilities}{integrate}
%
\begin{Description}\relax
Adaptive quadrature of functions of one variable over a finite or
infinite interval.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
integrate(f, lower, upper, ..., subdivisions=100,
          rel.tol = .Machine$double.eps^0.25, abs.tol = rel.tol,
          stop.on.error = TRUE, keep.xy = FALSE, aux = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{f}] an \R{} function taking a numeric first argument and returning
a numeric vector of the same length.  Returning a non-finite element will
generate an error.
\item[\code{lower, upper}] the limits of integration.  Can be infinite.
\item[\code{...}] additional arguments to be passed to \code{f}.
\item[\code{subdivisions}] the maximum number of subintervals.
\item[\code{rel.tol}] relative accuracy requested.
\item[\code{abs.tol}] absolute accuracy requested.
\item[\code{stop.on.error}] logical. If true (the default) an error stops the
function.  If false some errors will give a result with a warning in
the \code{message} component.
\item[\code{keep.xy}] unused.  For compatibility with S.
\item[\code{aux}] unused.  For compatibility with S.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Note that arguments after \code{...} must be matched exactly.

If one or both limits are infinite, the infinite range is mapped onto
a finite interval.

For a finite interval, globally adaptive interval subdivision is used
in connection with extrapolation by Wynn's Epsilon algorithm, with the
basic step being Gauss--Kronrod quadrature.

\code{rel.tol} cannot be less than \code{max(50*.Machine\$double.eps,
    0.5e-28)} if \code{abs.tol <= 0}.
\end{Details}
%
\begin{Value}
A list of class \code{"integrate"} with components
\begin{ldescription}
\item[\code{value}] the final estimate of the integral.
\item[\code{abs.error}] estimate of the modulus of the absolute error.
\item[\code{subdivisions}] the number of subintervals produced in the
subdivision process.
\item[\code{message}] \code{"OK"} or a character string giving the error message.
\item[\code{call}] the matched call.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Like all numerical integration routines, these evaluate the function
on a finite set of points.  If the function is approximately constant
(in particular, zero) over nearly all its range it is possible that
the result and error estimate may be seriously wrong.

When integrating over infinite intervals do so explicitly, rather than
just using a large number as the endpoint.  This increases the chance
of a correct answer -- any function whose integral over an infinite
interval is finite must be near zero for most of that interval.

For values at a finite set of points to be a fair reflection of the
behaviour of the function elsewhere, the function needs to be
well-behaved, for example differentiable except perhaps for a small
number of jumps or integrable singularities.

\code{f} must accept a vector of inputs and produce a vector of function
evaluations at those points.  The \code{\LinkA{Vectorize}{Vectorize}} function
may be helpful to convert \code{f} to this form.
\end{Note}
%
\begin{Source}\relax
Based on QUADPACK routines \code{dqags} and \code{dqagi} by
R. Piessens and E. deDoncker-Kapenga, available from Netlib.
\end{Source}
%
\begin{References}\relax
R. Piessens, E. deDoncker-Kapenga, C. Uberhuber, D. Kahaner (1983)
\emph{Quadpack: a Subroutine Package for Automatic Integration};
Springer Verlag.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
integrate(dnorm, -1.96, 1.96)
integrate(dnorm, -Inf, Inf)

## a slowly-convergent integral
integrand <- function(x) {1/((x+1)*sqrt(x))}
integrate(integrand, lower = 0, upper = Inf)

## don't do this if you really want the integral from 0 to Inf
integrate(integrand, lower = 0, upper = 10)
integrate(integrand, lower = 0, upper = 100000)
integrate(integrand, lower = 0, upper = 1000000, stop.on.error = FALSE)

## some functions do not handle vector input properly
f <- function(x) 2.0
try(integrate(f, 0, 1)) 
integrate(Vectorize(f), 0, 1)  ## correct
integrate(function(x) rep(2.0, length(x)), 0, 1)  ## correct

## integrate can fail if misused
integrate(dnorm,0,2)
integrate(dnorm,0,20)
integrate(dnorm,0,200)
integrate(dnorm,0,2000)
integrate(dnorm,0,20000) ## fails on many systems
integrate(dnorm,0,Inf)   ## works
\end{ExampleCode}
\end{Examples}
\HeaderA{interaction.plot}{Two-way Interaction Plot}{interaction.plot}
\keyword{hplot}{interaction.plot}
%
\begin{Description}\relax
Plots the mean (or other summary) of the response for two-way
combinations of factors, thereby illustrating possible interactions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
interaction.plot(x.factor, trace.factor, response, fun = mean,
                 type = c("l", "p", "b", "o", "c"), legend = TRUE,
                 trace.label = deparse(substitute(trace.factor)),
                 fixed = FALSE,
                 xlab = deparse(substitute(x.factor)),
                 ylab = ylabel,
                 ylim = range(cells, na.rm=TRUE),
                 lty = nc:1, col = 1, pch = c(1:9, 0, letters),
                 xpd = NULL, leg.bg = par("bg"), leg.bty = "n",
                 xtick = FALSE, xaxt = par("xaxt"), axes = TRUE,
                 ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x.factor}] a factor whose levels will form the x axis.
\item[\code{trace.factor}] another factor whose levels will form the traces.
\item[\code{response}] a numeric variable giving the response
\item[\code{fun}] the function to compute the summary. Should return a single
real value.
\item[\code{type}] the type of plot (see \code{\LinkA{plot.default}{plot.default}}): lines
or points or both.
\item[\code{legend}] logical. Should a legend be included?
\item[\code{trace.label}] overall label for the legend.
\item[\code{fixed}] logical.  Should the legend be in the order of the levels
of \code{trace.factor} or in the order of the traces at their right-hand ends?
\item[\code{xlab,ylab}] the x and y label of the plot each with a sensible default.
\item[\code{ylim}] numeric of length 2 giving the y limits for the plot.
\item[\code{lty}] line type for the lines drawn, with sensible default.
\item[\code{col}] the color to be used for plotting.
\item[\code{pch}] a vector of plotting symbols or characters, with sensible
default.
\item[\code{xpd}] determines clipping behaviour for the \code{\LinkA{legend}{legend}}
used, see \code{\LinkA{par}{par}(xpd)}.  Per default, the legend is
\emph{not} clipped at the figure border.
\item[\code{leg.bg, leg.bty}] arguments passed to \code{\LinkA{legend}{legend}()}.
\item[\code{xtick}] logical. Should tick marks be used on the x axis?
\item[\code{xaxt, axes, ...}] graphics parameters to be passed to the plotting routines.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
By default the levels of \code{x.factor} are plotted on the x axis in
their given order, with extra space left at the right for the legend
(if specified). If \code{x.factor} is an ordered factor and the levels
are numeric, these numeric values are used for the x axis.

The response and hence its summary can contain missing values. If so,
the missing values and the line segments joining them are omitted from
the plot (and this can be somewhat disconcerting).

The graphics parameters \code{xlab}, \code{ylab}, \code{ylim},
\code{lty}, \code{col} and \code{pch} are given suitable defaults
(and \code{xlim} and \code{xaxs} are set and cannot be overridden).
The defaults are to cycle through the line types, use the foreground
colour, and to use the symbols 1:9, 0, and the capital letters to plot
the traces.
\end{Details}
%
\begin{Note}\relax
Some of the argument names and the precise behaviour are chosen for
S-compatibility.
\end{Note}
%
\begin{References}\relax
Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
\emph{Analysis of variance; designed experiments.}
Chapter 5 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

with(ToothGrowth, {
interaction.plot(dose, supp, len, fixed=TRUE)
dose <- ordered(dose)
interaction.plot(dose, supp, len, fixed=TRUE, col = 2:3, leg.bty = "o")
interaction.plot(dose, supp, len, fixed=TRUE, col = 2:3, type = "p")
})

with(OrchardSprays, {
  interaction.plot(treatment, rowpos, decrease)
  interaction.plot(rowpos, treatment, decrease, cex.axis=0.8)
  ## order the rows by their mean effect
  rowpos <- factor(rowpos,
                   levels = sort.list(tapply(decrease, rowpos, mean)))
  interaction.plot(rowpos, treatment, decrease, col = 2:9, lty = 1)
})

with(esoph, {
  interaction.plot(agegp, alcgp, ncases/ncontrols, main = "'esoph' Data")
  interaction.plot(agegp, tobgp, ncases/ncontrols, trace.label="tobacco",
                   fixed=TRUE, xaxt = "n")
})
## deal with NAs:
esoph[66,] # second to last age group: 65-74
esophNA <- esoph; esophNA$ncases[66] <- NA
with(esophNA, {
  interaction.plot(agegp, alcgp, ncases/ncontrols, col= 2:5)
                                # doesn't show *last* group either
  interaction.plot(agegp, alcgp, ncases/ncontrols, col= 2:5, type = "b")
  ## alternative take non-NA's  {"cheating"}
  interaction.plot(agegp, alcgp, ncases/ncontrols, col= 2:5,
                   fun = function(x) mean(x, na.rm=TRUE),
                   sub = "function(x) mean(x, na.rm=TRUE)")
})
rm(esophNA) # to clear up
\end{ExampleCode}
\end{Examples}
\HeaderA{IQR}{The Interquartile Range}{IQR}
\keyword{univar}{IQR}
\keyword{robust}{IQR}
\keyword{distribution}{IQR}
%
\begin{Description}\relax
computes interquartile range of the \code{x} values.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
IQR(x, na.rm = FALSE, type = 7)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector.
\item[\code{na.rm}] logical. Should missing values be removed?
\item[\code{type}] an integer selecting one of the many quantile algorithms,
see \code{\LinkA{quantile}{quantile}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Note that this function computes the quartiles using the
\code{\LinkA{quantile}{quantile}} function rather than following
Tukey's recommendations,
i.e., \code{IQR(x) = quantile(x,3/4) - quantile(x,1/4)}.

For normally \eqn{N(m,1)}{} distributed \eqn{X}{}, the expected value of
\code{IQR(X)} is \code{2*qnorm(3/4) = 1.3490}, i.e., for a normal-consistent
estimate of the standard deviation, use \code{IQR(x) / 1.349}.
\end{Details}
%
\begin{References}\relax
Tukey, J. W. (1977).
\emph{Exploratory Data Analysis.}
Reading: Addison-Wesley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{fivenum}{fivenum}}, \code{\LinkA{mad}{mad}} which is more robust,
\code{\LinkA{range}{range}}, \code{\LinkA{quantile}{quantile}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
IQR(rivers)
\end{ExampleCode}
\end{Examples}
\HeaderA{is.empty.model}{Test if a Model's Formula is Empty}{is.empty.model}
\keyword{models}{is.empty.model}
%
\begin{Description}\relax
\R{}'s formula notation allows models with no intercept and no
predictors. These require special handling internally.
\code{is.empty.model()} checks whether an object describes an empty
model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
is.empty.model(x)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A \code{terms} object or an object with a \code{terms} method.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{TRUE} if the model is empty
\end{Value}
%
\begin{SeeAlso}\relax
 \code{\LinkA{lm}{lm}},\code{\LinkA{glm}{glm}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
y <- rnorm(20)
is.empty.model(y ~ 0)
is.empty.model(y ~ -1)
is.empty.model(lm(y ~ 0))
\end{ExampleCode}
\end{Examples}
\HeaderA{isoreg}{Isotonic / Monotone Regression}{isoreg}
\keyword{regression}{isoreg}
\keyword{smooth}{isoreg}
%
\begin{Description}\relax
Compute the isotonic (monotonely increasing nonparametric) least
squares regression which is piecewise constant.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
isoreg(x, y = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] 
coordinate vectors of the regression points.  Alternatively a single
plotting structure can be specified: see \code{\LinkA{xy.coords}{xy.coords}}.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The algorithm determines the convex minorant \eqn{m(x)}{} of the
\emph{cumulative} data (i.e., \code{cumsum(y)}) which is piecewise
linear and the result is \eqn{m'(x)}{}, a step function with level
changes at locations where the convex \eqn{m(x)}{} touches the
cumulative data polygon and changes slope.\\{}
\code{\LinkA{as.stepfun}{as.stepfun}()} returns a \code{\LinkA{stepfun}{stepfun}}
object which can be more parsimonious.
\end{Details}
%
\begin{Value}
\code{isoreg()} returns an object of class \code{isoreg} which is
basically a list with components
\begin{ldescription}
\item[\code{x}] original (constructed) abscissa values \code{x}.
\item[\code{y}] corresponding y values.
\item[\code{yf}] fitted values corresponding to \emph{ordered} x values.
\item[\code{yc}] cumulative y values corresponding to \emph{ordered} x values.
\item[\code{iKnots}] integer vector giving indices where the fitted curve jumps,
i.e., where the convex minorant has kinks.
\item[\code{isOrd}] logical indicating if original x values were ordered
increasingly already.
\item[\code{ord}] \code{if(!isOrd)}: integer permutation \code{\LinkA{order}{order}(x)} of
\emph{original} \code{x}.
\item[\code{call}] the \code{\LinkA{call}{call}} to \code{isoreg()} used.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The code should be improved to accept \emph{weights} additionally and
solve the corresponding weighted least squares problem.\\{}
`Patches are welcome!'
\end{Note}
%
\begin{References}\relax
Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972)
\emph{Statistical inference under order restrictions}; Wiley, London.

Robertson, T., Wright,F. T. and Dykstra, R. L. (1988)
\emph{Order Restricted Statistical Inference}; Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
the plotting method \code{\LinkA{plot.isoreg}{plot.isoreg}} with more examples;
\code{\LinkA{isoMDS}{isoMDS}()} from the \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} package internally
uses isotonic regression.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

(ir <- isoreg(c(1,0,4,3,3,5,4,2,0)))
plot(ir, plot.type = "row")

(ir3 <- isoreg(y3 <- c(1,0,4,3,3,5,4,2, 3)))# last "3", not "0"
(fi3 <- as.stepfun(ir3))
(ir4 <- isoreg(1:10, y4 <- c(5, 9, 1:2, 5:8, 3, 8)))
cat(sprintf("R^2 = %.2f\n",
            1 - sum(residuals(ir4)^2) / ((10-1)*var(y4))))

## If you are interested in the knots alone :
with(ir4, cbind(iKnots, yf[iKnots]))

## Example of unordered x[] with ties:
x <- sample((0:30)/8)
y <- exp(x)
x. <- round(x) # ties!
plot(m <- isoreg(x., y))
stopifnot(all.equal(with(m, yf[iKnots]),
                    as.vector(tapply(y, x., mean))))
\end{ExampleCode}
\end{Examples}
\HeaderA{KalmanLike}{Kalman Filtering}{KalmanLike}
\aliasA{KalmanForecast}{KalmanLike}{KalmanForecast}
\aliasA{KalmanRun}{KalmanLike}{KalmanRun}
\aliasA{KalmanSmooth}{KalmanLike}{KalmanSmooth}
\aliasA{makeARIMA}{KalmanLike}{makeARIMA}
\keyword{ts}{KalmanLike}
%
\begin{Description}\relax
Use Kalman Filtering to find the (Gaussian) log-likelihood, or for
forecasting or smoothing.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
KalmanLike(y, mod, nit = 0, fast=TRUE)
KalmanRun(y, mod, nit = 0, fast=TRUE)
KalmanSmooth(y, mod, nit = 0)
KalmanForecast(n.ahead = 10, mod, fast=TRUE)
makeARIMA(phi, theta, Delta, kappa = 1e6)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{y}] a univariate time series.
\item[\code{mod}] A list describing the state-space model: see `Details'.
\item[\code{nit}] The time at which the initialization is computed.
\code{nit = 0} implies that the initialization is for a one-step
prediction, so \code{Pn} should not be computed at the first step.
\item[\code{n.ahead}] The number of steps ahead for which prediction is
required.
\item[\code{phi, theta}] numeric vectors of length \eqn{\ge 0}{} giving AR
and MA parameters.
\item[\code{Delta}] vector of differencing coefficients, so an ARMA model is
fitted to \code{y[t] - Delta[1]*y[t-1] - ...}.
\item[\code{kappa}] the prior variance (as a multiple of the innovations
variance) for the past observations in a differenced model.
\item[\code{fast}] If \code{TRUE} the \code{mod} object may be modified.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These functions work with a general univariate state-space model
with state vector \samp{a}, transitions \samp{a <- T a + R e},
\eqn{e \sim {\cal N}(0, \kappa Q)}{} and observation
equation \samp{y = Z'a + eta},
\eqn{(eta\equiv\eta), \eta \sim {\cal N}(0, \kappa h)}{}.
The likelihood is a profile likelihood after estimation of
\eqn{\kappa}{}.

The model is specified as a list with at least components
\begin{description}

\item[\code{T}] the transition matrix
\item[\code{Z}] the observation coefficients
\item[\code{h}] the observation variance
\item[\code{V}] \samp{RQR'}
\item[\code{a}] the current state estimate
\item[\code{P}] the current estimate of the state uncertainty matrix
\item[\code{Pn}] the estimate at time \eqn{t-1}{} of the state
uncertainty matrix

\end{description}


\code{KalmanSmooth} is the workhorse function for \code{\LinkA{tsSmooth}{tsSmooth}}.

\code{makeARIMA} constructs the state-space model for an ARIMA model.
\end{Details}
%
\begin{Value}
For \code{KalmanLike}, a list with components \code{Lik} (the
log-likelihood less some constants) and \code{s2}, the estimate of
\eqn{\kappa}{}.

For \code{KalmanRun}, a list with components \code{values}, a vector
of length 2 giving the output of \code{KalmanLike}, \code{resid} (the
residuals) and \code{states}, the contemporaneous state estimates,
a matrix with one row for each time.

For \code{KalmanSmooth}, a list with two components.
Component \code{smooth} is a \code{n} by \code{p} matrix of state
estimates based on all the observations, with one row for each time.
Component \code{var} is a \code{n} by \code{p} by \code{p} array of
variance matrices.

For \code{KalmanForecast}, a list with components \code{pred}, the
predictions, and \code{var}, the unscaled variances of the prediction
errors (to be multiplied by \code{s2}).

For \code{makeARIMA}, a model list including components for
its arguments.
\end{Value}
%
\begin{Section}{Warning}
These functions are designed to be called from other functions which
check the validity of the arguments passed, so very little checking is
done.

In particular, \code{KalmanLike} alters the objects passed as
the elements \code{a}, \code{P} and \code{Pn} of \code{mod}, so these
should not be shared. Use \code{fast=FALSE} to prevent this.
\end{Section}
%
\begin{References}\relax
Durbin, J. and Koopman, S. J. (2001) \emph{Time Series Analysis by
State Space Methods.}  Oxford University Press.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{arima}{arima}}, \code{\LinkA{StructTS}{StructTS}}. \code{\LinkA{tsSmooth}{tsSmooth}}.

\end{SeeAlso}
\HeaderA{kernapply}{Apply Smoothing Kernel}{kernapply}
\methaliasA{kernapply.default}{kernapply}{kernapply.default}
\methaliasA{kernapply.ts}{kernapply}{kernapply.ts}
\methaliasA{kernapply.tskernel}{kernapply}{kernapply.tskernel}
\methaliasA{kernapply.vector}{kernapply}{kernapply.vector}
\keyword{ts}{kernapply}
%
\begin{Description}\relax
\code{kernapply} computes the convolution between an input sequence 
and a specific kernel.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernapply(x, ...)

## Default S3 method:
kernapply(x, k, circular = FALSE, ...)
## S3 method for class 'ts'
kernapply(x, k, circular = FALSE, ...)
## S3 method for class 'vector'
kernapply(x, k, circular = FALSE, ...)

## S3 method for class 'tskernel'
kernapply(x, k, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an input vector, matrix, time series or kernel to be smoothed.
\item[\code{k}] smoothing \code{"tskernel"} object.
\item[\code{circular}] a logical indicating whether the input sequence to be
smoothed is treated as circular, i.e., periodic.
\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A smoothed version of the input sequence.
\end{Value}
%
\begin{Note}\relax
This uses \code{\LinkA{fft}{fft}} to perform the convolution, so is fastest
when \code{NROW(x)} is a power of 2 or some other highly composite
integer.
\end{Note}
%
\begin{Author}\relax
A. Trapletti
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{kernel}{kernel}}, \code{\LinkA{convolve}{convolve}}, \code{\LinkA{filter}{filter}},
\code{\LinkA{spectrum}{spectrum}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## see 'kernel' for examples
\end{ExampleCode}
\end{Examples}
\HeaderA{kernel}{Smoothing Kernel Objects}{kernel}
\aliasA{bandwidth.kernel}{kernel}{bandwidth.kernel}
\aliasA{df.kernel}{kernel}{df.kernel}
\aliasA{is.tskernel}{kernel}{is.tskernel}
\aliasA{plot.tskernel}{kernel}{plot.tskernel}
\keyword{ts}{kernel}
%
\begin{Description}\relax
The \code{"tskernel"} class is designed to represent discrete
symmetric normalized smoothing kernels.  These kernels can be used to
smooth vectors, matrices, or time series objects.

There are \code{\LinkA{print}{print}}, \code{\LinkA{plot}{plot}} and \code{\LinkA{[}{[}}
methods for these kernel objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kernel(coef, m = 2, r, name)

df.kernel(k)
bandwidth.kernel(k)
is.tskernel(k)

## S3 method for class 'tskernel'
plot(x, type = "h", xlab = "k", ylab = "W[k]",
     main = attr(x,"name"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{coef}] the upper half of the smoothing kernel coefficients
(including coefficient zero) \emph{or} the name of a kernel
(currently \code{"daniell"}, \code{"dirichlet"}, \code{"fejer"} or
\code{"modified.daniell"}.
\item[\code{m}] the kernel dimension(s) if \code{coef} is a name.  When \code{m}
has length larger than one, it means the convolution of
kernels of dimension \code{m[j]}, for \code{j in 1:length(m)}.
Currently this is supported only for the named "*daniell" kernels.
\item[\code{name}] the name the kernel will be called.
\item[\code{r}] the kernel order for a Fejer kernel.
\item[\code{k, x}] a \code{"tskernel"} object.
\item[\code{type, xlab, ylab, main, ...}] arguments passed to
\code{\LinkA{plot.default}{plot.default}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{kernel} is used to construct a general kernel or named specific
kernels.  The modified Daniell kernel halves the end coefficients (as
used by S-PLUS).

The \code{\LinkA{[}{[}} method allows natural indexing of kernel objects
with indices in \code{(-m) : m}.  The normalization is such that for
\code{k <- kernel(*)}, \code{sum(k[ -k\$m : k\$m ])} is one.

\code{df.kernel} returns the `equivalent degrees of freedom' of
a smoothing kernel as defined in Brockwell and Davis (1991), page
362, and \code{bandwidth.kernel} returns the equivalent bandwidth as
defined in Bloomfield (1976), p. 201, with a continuity correction.
\end{Details}
%
\begin{Value}
\code{kernel()} returns an object of class \code{"tskernel"} which is
basically a list with the two components \code{coef} and the kernel
dimension \code{m}.  An additional attribute is \code{"name"}.
\end{Value}
%
\begin{Author}\relax
A. Trapletti; modifications by B.D. Ripley
\end{Author}
%
\begin{References}\relax
Bloomfield, P. (1976)
\emph{Fourier Analysis of Time Series: An Introduction.}
Wiley.

Brockwell, P.J. and Davis, R.A. (1991)
\emph{Time Series: Theory and Methods.}
Second edition. Springer, pp. 350--365.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{kernapply}{kernapply}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Demonstrate a simple trading strategy for the
## financial time series German stock index DAX.
x <- EuStockMarkets[,1]
k1 <- kernel("daniell", 50)  # a long moving average
k2 <- kernel("daniell", 10)  # and a short one
plot(k1)
plot(k2)
x1 <- kernapply(x, k1)
x2 <- kernapply(x, k2)
plot(x)
lines(x1, col = "red")    # go long if the short crosses the long upwards
lines(x2, col = "green")  # and go short otherwise

## More interesting kernels
kd <- kernel("daniell", c(3,3))
kd # note the unusual indexing
kd[-2:2]
plot(kernel("fejer", 100, r=6))
plot(kernel("modified.daniell", c(7,5,3)))

# Reproduce example 10.4.3 from Brockwell and Davis (1991)
spectrum(sunspot.year, kernel=kernel("daniell", c(11,7,3)), log="no")
\end{ExampleCode}
\end{Examples}
\HeaderA{kmeans}{K-Means Clustering}{kmeans}
\aliasA{fitted.kmeans}{kmeans}{fitted.kmeans}
\aliasA{print.kmeans}{kmeans}{print.kmeans}
\keyword{multivariate}{kmeans}
\keyword{cluster}{kmeans}
%
\begin{Description}\relax
Perform k-means clustering on a data matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kmeans(x, centers, iter.max = 10, nstart = 1,
       algorithm = c("Hartigan-Wong", "Lloyd", "Forgy",
                     "MacQueen"))
## S3 method for class 'kmeans'
fitted(object, method = c("centers", "classes"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric matrix of data, or an object that can be coerced to
such a matrix (such as a numeric vector or a data frame with all
numeric columns).
\item[\code{centers}] either the number of clusters, say \eqn{k}{}, or a set of
initial (distinct) cluster centres.  If a number, a random set of
(distinct) rows in \code{x} is chosen as the initial centres.
\item[\code{iter.max}] the maximum number of iterations allowed.
\item[\code{nstart}] if \code{centers} is a number, how many random sets
should be chosen?
\item[\code{algorithm}] character: may be abbreviated.
\item[\code{object}] an \R{} object of class \code{"kmeans"}, typically the
result \code{ob} of \code{ob <- kmeans(..)}.
\item[\code{method}] character: may be abbreviated. \code{"centers"} causes
\code{fitted} to return cluster centers (one for each input point) and
\code{"classes"} causes \code{fitted} to return a vector of class
assignments.
\item[\code{...}] not used.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The data given by \code{x} is clustered by the \eqn{k}{}-means method,
which aims to partition the points into \eqn{k}{} groups such that the
sum of squares from points to the assigned cluster centres is minimized.
At the minimum, all cluster centres are at the mean of their Voronoi
sets (the set of data points which are nearest to the cluster centre).

The algorithm of Hartigan and Wong (1979) is used by default.  Note
that some authors use \eqn{k}{}-means to refer to a specific algorithm
rather than the general method: most commonly the algorithm given by
MacQueen (1967) but sometimes that given by Lloyd (1957) and Forgy
(1965). The Hartigan--Wong algorithm generally does a better job than
either of those, but trying several random starts (\code{nstart}\eqn{>
  1}{}) is often recommended.
For ease of programmatic exploration, \eqn{k=1}{} is allowed, notably
returning the center and \code{withinss}.

Except for the Lloyd--Forgy method, \eqn{k}{} clusters will always be
returned if a number is specified.
If an initial matrix of centres is supplied, it is possible that
no point will be closest to one or more centres, which is currently
an error for the Hartigan--Wong method.
\end{Details}
%
\begin{Value}
\code{kmeans} returns an object of class \code{"kmeans"} which has a
\code{print} and a \code{fitted} method.  It is a list with components:
\begin{ldescription}
\item[\code{cluster}] 
A vector of integers (from \code{1:k}) indicating the cluster to
which each point is allocated.

\item[\code{centers}] A matrix of cluster centres.
\item[\code{totss}] The total sum of squares.
\item[\code{withinss}] Vector of within-cluster sum of squares, one component per cluster.
\item[\code{tot.withinss}] Total within-cluster sum of squares, i.e., \code{sum(withinss)}.
\item[\code{betweenss}] The between-cluster sum of squares, i.e. \code{totss-tot.withinss}.
\item[\code{size}] The number of points in each cluster.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Forgy, E. W. (1965) Cluster analysis of multivariate data:
efficiency vs interpretability of classifications.
\emph{Biometrics} \bold{21}, 768--769.

Hartigan, J. A. and Wong, M. A. (1979).
A K-means clustering algorithm.
\emph{Applied Statistics} \bold{28}, 100--108.

Lloyd, S. P. (1957, 1982)  Least squares quantization in PCM.
Technical Note, Bell Laboratories.  Published in 1982 in
\emph{IEEE Transactions on Information Theory} \bold{28}, 128--137.

MacQueen, J. (1967)  Some methods for classification and analysis of
multivariate observations. In \emph{Proceedings of the Fifth Berkeley
Symposium on  Mathematical Statistics and  Probability},
eds L. M. Le Cam \& J. Neyman,
\bold{1}, pp. 281--297. Berkeley, CA: University of California Press.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

# a 2-dimensional example
x <- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")
(cl <- kmeans(x, 2))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex=2)

# sum of squares
ss <- function(x) sum(scale(x, scale = FALSE)^2)

## cluster centers "fitted" to each obs.:
fitted.x <- fitted(cl);  head(fitted.x)
resid.x <- x - fitted(cl)

## Equalities : ----------------------------------
cbind(cl[c("betweenss", "tot.withinss", "totss")], # the same two columns
         c(ss(fitted.x), ss(resid.x),    ss(x)))
stopifnot(all.equal(cl$ totss,        ss(x)),
	  all.equal(cl$ tot.withinss, ss(resid.x)),
	  ## these three are the same:
	  all.equal(cl$ betweenss,    ss(fitted.x)),
	  all.equal(cl$ betweenss, cl$totss - cl$tot.withinss),
	  ## and hence also
	  all.equal(ss(x), ss(fitted.x) + ss(resid.x))
	  )

kmeans(x,1)$withinss # trivial one-cluster, (its W.SS == ss(x))

## random starts do help here with too many clusters
## (and are often recommended anyway!):
(cl <- kmeans(x, 5, nstart = 25))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:5, pch = 8)
\end{ExampleCode}
\end{Examples}
\HeaderA{kruskal.test}{Kruskal-Wallis Rank Sum Test}{kruskal.test}
\methaliasA{kruskal.test.default}{kruskal.test}{kruskal.test.default}
\methaliasA{kruskal.test.formula}{kruskal.test}{kruskal.test.formula}
\keyword{htest}{kruskal.test}
%
\begin{Description}\relax
Performs a Kruskal-Wallis rank sum test.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
kruskal.test(x, ...)

## Default S3 method:
kruskal.test(x, g, ...)

## S3 method for class 'formula'
kruskal.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector of data values, or a list of numeric data
vectors.
\item[\code{g}] a vector or factor object giving the group for the
corresponding elements of \code{x}.  Ignored if \code{x} is a
list.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
gives the data values and \code{rhs} the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{kruskal.test} performs a Kruskal-Wallis rank sum test of the
null that the location parameters of the distribution of \code{x}
are the same in each group (sample).  The alternative is that they
differ in at least one.

If \code{x} is a list, its elements are taken as the samples to be
compared, and hence have to be numeric data vectors.  In this case,
\code{g} is ignored, and one can simply use \code{kruskal.test(x)}
to perform the test.  If the samples are not yet contained in a
list, use \code{kruskal.test(list(x, ...))}.

Otherwise, \code{x} must be a numeric data vector, and \code{g} must
be a vector or factor object of the same length as \code{x} giving
the group for the corresponding elements of \code{x}.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the Kruskal-Wallis rank sum statistic.
\item[\code{parameter}] the degrees of freedom of the approximate
chi-squared distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] the character string \code{"Kruskal-Wallis rank sum test"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Myles Hollander and Douglas A. Wolfe (1973),
\emph{Nonparametric Statistical Methods.}
New York: John Wiley \& Sons.
Pages 115--120.
\end{References}
%
\begin{SeeAlso}\relax
The Wilcoxon rank sum test (\code{\LinkA{wilcox.test}{wilcox.test}}) as the special
case for two samples;
\code{\LinkA{lm}{lm}} together with \code{\LinkA{anova}{anova}} for performing
one-way location analysis under normality assumptions; with Student's
t test (\code{\LinkA{t.test}{t.test}}) as the special case for two samples.

\code{\LinkA{wilcox\_test}{wilcox.Rul.test}} in package
\Rhref{http://CRAN.R-project.org/package=coin}{\pkg{coin}} for exact, asymptotic and Monte Carlo
\emph{conditional} p-values, including in the presence of ties.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Hollander & Wolfe (1973), 116.
## Mucociliary efficiency from the rate of removal of dust in normal
##  subjects, subjects with obstructive airway disease, and subjects
##  with asbestosis.
x <- c(2.9, 3.0, 2.5, 2.6, 3.2) # normal subjects
y <- c(3.8, 2.7, 4.0, 2.4)      # with obstructive airway disease
z <- c(2.8, 3.4, 3.7, 2.2, 2.0) # with asbestosis
kruskal.test(list(x, y, z))
## Equivalently,
x <- c(x, y, z)
g <- factor(rep(1:3, c(5, 4, 5)),
            labels = c("Normal subjects",
                       "Subjects with obstructive airway disease",
                       "Subjects with asbestosis"))
kruskal.test(x, g)

## Formula interface.
require(graphics)
boxplot(Ozone ~ Month, data = airquality)
kruskal.test(Ozone ~ Month, data = airquality)
\end{ExampleCode}
\end{Examples}
\HeaderA{ks.test}{Kolmogorov-Smirnov Tests}{ks.test}
\keyword{htest}{ks.test}
%
\begin{Description}\relax
Perform a one- or two-sample Kolmogorov-Smirnov test.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ks.test(x, y, ...,
        alternative = c("two.sided", "less", "greater"),
        exact = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector of data values.
\item[\code{y}] either a numeric vector of data values, or a character string
naming a cumulative distribution function or an actual cumulative
distribution function such as \code{pnorm}.  Only continuous CDFs
are valid.
\item[\code{...}] parameters of the distribution specified (as a character
string) by \code{y}.
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"} (default), \code{"less"}, or
\code{"greater"}.  You can specify just the initial letter of the
value, but the argument name must be give in full.
See `Details' for the meanings of the possible values.
\item[\code{exact}] \code{NULL} or a logical indicating whether an exact
p-value should be computed.  See `Details' for the meaning of
\code{NULL}.  Not available in the two-sample case for a one-sided
test or if ties are present.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{y} is numeric, a two-sample test of the null hypothesis
that \code{x} and \code{y} were drawn from the same \emph{continuous}
distribution is performed.

Alternatively, \code{y} can be a character string naming a continuous
(cumulative) distribution function, or such a function.  In this case,
a one-sample test is carried out of the null that the distribution
function which generated \code{x} is distribution \code{y} with
parameters specified by \code{...}.

The presence of ties always generates a warning, since continuous
distributions do not generate them.  If the ties arose from rounding
the tests may be approximately valid, but even modest amounts of
rounding can have a significant effect on the calculated statistic.

Missing values are silently omitted from \code{x} and (in the
two-sample case) \code{y}.

The possible values \code{"two.sided"}, \code{"less"} and
\code{"greater"} of \code{alternative} specify the null hypothesis
that the true distribution function of \code{x} is equal to, not less
than or not greater than the hypothesized distribution function
(one-sample case) or the distribution function of \code{y} (two-sample
case), respectively.  This is a comparison of cumulative distribution
functions, and the test statistic is the maximum difference in value,
with the statistic in the \code{"greater"} alternative being
\eqn{D^+ = \max_u [ F_x(u) - F_y(u) ]}{}.
Thus in the two-sample case \code{alternative = "greater"} includes
distributions for which \code{x} is stochastically \emph{smaller} than
\code{y} (the CDF of \code{x} lies above and hence to the left of that
for \code{y}), in contrast to \code{\LinkA{t.test}{t.test}} or
\code{\LinkA{wilcox.test}{wilcox.test}}.

Exact p-values are not available for the two-sample case if one-sided
or in the presence of ties.  If \code{exact = NULL} (the default), an
exact p-value is computed if the sample size is less than 100 in the
one-sample case \emph{and there are no ties}, and if the product of
the sample sizes is less than 10000 in the two-sample case.
Otherwise, asymptotic distributions are used whose approximations may
be inaccurate in small samples.  In the one-sample two-sided case,
exact p-values are obtained as described in Marsaglia, Tsang \& Wang
(2003) (but not using the optional approximation in the right tail, so
this can be slow for small p-values).  The formula of Birnbaum \&
Tingey (1951) is used for the one-sample one-sided case.

If a single-sample test is used, the parameters specified in
\code{...} must be pre-specified and not estimated from the data.
There is some more refined distribution theory for the KS test with
estimated parameters (see Durbin, 1973), but that is not implemented
in \code{ks.test}.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] a character string indicating what type of test was
performed.
\item[\code{data.name}] a character string giving the name(s) of the data.
\end{ldescription}
\end{Value}
%
\begin{Source}\relax
The two-sided one-sample distribution comes \emph{via}
Marsaglia, Tsang and Wang (2003).
\end{Source}
%
\begin{References}\relax
Z. W. Birnbaum and Fred H. Tingey (1951),
One-sided confidence contours for probability distribution functions.
\emph{The Annals of Mathematical Statistics}, \bold{22}/4, 592--596.

William J. Conover (1971),
\emph{Practical Nonparametric Statistics}.
New York: John Wiley \& Sons.
Pages 295--301 (one-sample Kolmogorov test),
309--314 (two-sample Smirnov test).

Durbin, J. (1973),
\emph{Distribution theory for tests based on the sample distribution
function}.  SIAM.

George Marsaglia, Wai Wan Tsang and Jingbo Wang (2003),
Evaluating Kolmogorov's distribution.
\emph{Journal of Statistical Software}, \bold{8}/18.
\url{http://www.jstatsoft.org/v08/i18/}.

\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{shapiro.test}{shapiro.test}} which performs the Shapiro-Wilk test for
normality.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

x <- rnorm(50)
y <- runif(30)
# Do x and y come from the same distribution?
ks.test(x, y)
# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")

# test if x is stochastically larger than x2
x2 <- rnorm(50, -1)
plot(ecdf(x), xlim=range(c(x, x2)))
plot(ecdf(x2), add=TRUE, lty="dashed")
t.test(x, x2, alternative="g")
wilcox.test(x, x2, alternative="g")
ks.test(x, x2, alternative="l")
\end{ExampleCode}
\end{Examples}
\HeaderA{ksmooth}{Kernel Regression Smoother}{ksmooth}
\keyword{smooth}{ksmooth}
%
\begin{Description}\relax
The Nadaraya--Watson kernel regression estimate.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ksmooth(x, y, kernel = c("box", "normal"), bandwidth = 0.5,
        range.x = range(x),
        n.points = max(100, length(x)), x.points)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] input x values
\item[\code{y}] input y values
\item[\code{kernel}] the kernel to be used.
\item[\code{bandwidth}] the bandwidth. The kernels are scaled so that their
quartiles (viewed as probability densities) are at
\eqn{\pm}{} \code{0.25*bandwidth}.
\item[\code{range.x}] the range of points to be covered in the output.
\item[\code{n.points}] the number of points at which to evaluate the fit.
\item[\code{x.points}] points at which to evaluate the smoothed fit. If
missing, \code{n.points} are chosen uniformly to cover \code{range.x}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with components
\begin{ldescription}
\item[\code{x}] values at which the smoothed fit is evaluated. Guaranteed to
be in increasing order.
\item[\code{y}] fitted values corresponding to \code{x}.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
This function is implemented purely for compatibility with S,
although it is nowhere near as slow as the S function. Better kernel
smoothers are available in other packages.
\end{Note}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

with(cars, {
    plot(speed, dist)
    lines(ksmooth(speed, dist, "normal", bandwidth=2), col=2)
    lines(ksmooth(speed, dist, "normal", bandwidth=5), col=3)
})
\end{ExampleCode}
\end{Examples}
\HeaderA{lag}{Lag a Time Series}{lag}
\methaliasA{lag.default}{lag}{lag.default}
\keyword{ts}{lag}
%
\begin{Description}\relax
Compute a lagged version of a time series, shifting the time base
back by a given number of observations.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lag(x, ...)

## Default S3 method:
lag(x, k = 1, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A vector or matrix or univariate or multivariate time series
\item[\code{k}] The number of lags (in units of observations).
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Vector or matrix arguments \code{x} are coerced to time series.

\code{lag} is a generic function; this page documents its default
method.
\end{Details}
%
\begin{Value}
A time series object.
\end{Value}
%
\begin{Note}\relax
Note the sign of \code{k}: a series lagged by a positive \code{k}
starts \emph{earlier}.
\end{Note}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{diff}{diff}}, \code{\LinkA{deltat}{deltat}} 
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
lag(ldeaths, 12) # starts one year earlier
\end{ExampleCode}
\end{Examples}
\HeaderA{lag.plot}{Time Series Lag Plots}{lag.plot}
\keyword{hplot}{lag.plot}
\keyword{ts}{lag.plot}
%
\begin{Description}\relax
Plot time series against lagged versions of themselves.
Helps visualizing `auto-dependence' even when auto-correlations
vanish. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lag.plot(x, lags = 1, layout = NULL, set.lags = 1:lags,
         main = NULL, asp = 1,
         diag = TRUE, diag.col = "gray", type = "p", oma = NULL,
         ask = NULL, do.lines = (n <= 150), labels = do.lines,
         ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] time-series (univariate or multivariate)
\item[\code{lags}] number of lag plots desired, see arg \code{set.lags}.
\item[\code{layout}] the layout of multiple plots, basically the \code{mfrow}
\code{\LinkA{par}{par}()} argument.  The default uses about a square
layout (see \code{\LinkA{n2mfrow}{n2mfrow}} such that all plots are on one page.
\item[\code{set.lags}] vector of positive integers allowing specification of
the set of lags used; defaults to \code{1:lags}.
\item[\code{main}] character with a main header title to be done on the top
of each page.
\item[\code{asp}] Aspect ratio to be fixed, see \code{\LinkA{plot.default}{plot.default}}.
\item[\code{diag}] logical indicating if the x=y diagonal should be drawn.
\item[\code{diag.col}] color to be used for the diagonal \code{if(diag)}.
\item[\code{type}] plot type to be used, but see \code{\LinkA{plot.ts}{plot.ts}} about
its restricted meaning.
\item[\code{oma}] outer margins, see \code{\LinkA{par}{par}}.
\item[\code{ask}] logical or \code{NULL}; if true, the user is asked to
confirm before a new page is started.
\item[\code{do.lines}] logical indicating if lines should be drawn.
\item[\code{labels}] logical indicating if labels should be used.
\item[\code{...}] Further arguments to \code{\LinkA{plot.ts}{plot.ts}}.  Several
graphical parameters are set in this function and so cannot be
changed: these include \code{xlab}, \code{ylab}, \code{mgp},
\code{col.lab} and \code{font.lab}: this also applies to the
arguments \code{xy.labels} and \code{xy.lines}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If just one plot is produced, this is a conventional plot.  If more
than one plot is to be produced, \code{par(mfrow)} and several other
graphics parameters will be set, so it is not (easily) possible to mix
such lag plots with other plots on the same page.

If \code{ask = NULL}, \code{par(ask = TRUE)} will be called if more than
one page of plots is to be produced and the device is interactive.
\end{Details}
%
\begin{Note}\relax
It is more flexible and has different default behaviour than
the S version.  We use \code{main =} instead of \code{head = } for
internal consistency.
\end{Note}
%
\begin{Author}\relax
Martin Maechler
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{plot.ts}{plot.ts}} which is the basic work horse.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

lag.plot(nhtemp, 8, diag.col = "forest green")
lag.plot(nhtemp, 5, main="Average Temperatures in New Haven")
## ask defaults to TRUE when we have more than one page:
lag.plot(nhtemp, 6, layout = c(2,1), asp = NA,
         main = "New Haven Temperatures", col.main = "blue")

## Multivariate (but non-stationary! ...)
lag.plot(freeny.x, lags = 3)
## Not run: 
no lines for long series :
lag.plot(sqrt(sunspots), set = c(1:4, 9:12), pch = ".", col = "gold")

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{line}{Robust Line Fitting}{line}
\aliasA{residuals.tukeyline}{line}{residuals.tukeyline}
\keyword{robust}{line}
\keyword{regression}{line}
%
\begin{Description}\relax
Fit a line robustly as recommended in \emph{Exploratory Data Analysis}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
line(x, y)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x,y}] the arguments can be any way of specifying x-y pairs.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An object of class \code{"tukeyline"}.

Methods are available for the generic functions \code{coef},
\code{residuals}, \code{fitted}, and \code{print}.
\end{Value}
%
\begin{References}\relax
Tukey, J. W. (1977).
\emph{Exploratory Data Analysis},
Reading Massachusetts: Addison-Wesley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{lm}{lm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

plot(cars)
(z <- line(cars))
abline(coef(z))
## Tukey-Anscombe Plot :
plot(residuals(z) ~ fitted(z), main = deparse(z$call))
\end{ExampleCode}
\end{Examples}
\HeaderA{lm}{Fitting Linear Models}{lm}
\aliasA{print.lm}{lm}{print.lm}
\keyword{regression}{lm}
%
\begin{Description}\relax
\code{lm} is used to fit linear models.
It can be used to carry out regression,
single stratum analysis of variance and
analysis of covariance (although \code{\LinkA{aov}{aov}} may provide a more
convenient interface for these).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] an object of class \code{"\LinkA{formula}{formula}"} (or one that
can be coerced to that class): a symbolic description of the
model to be fitted.  The details of model specification are given
under `Details'.

\item[\code{data}] an optional data frame, list or environment (or object
coercible by \code{\LinkA{as.data.frame}{as.data.frame}} to a data frame) containing
the variables in the model.  If not found in \code{data}, the
variables are taken from \code{environment(formula)},
typically the environment from which \code{lm} is called.

\item[\code{subset}] an optional vector specifying a subset of observations
to be used in the fitting process.

\item[\code{weights}] an optional vector of weights to be used in the fitting
process.  Should be \code{NULL} or a numeric vector.
If non-NULL, weighted least squares is used with weights
\code{weights} (that is, minimizing \code{sum(w*e\textasciicircum{}2)}); otherwise
ordinary least squares is used.  See also `Details',

\item[\code{na.action}] a function which indicates what should happen
when the data contain \code{NA}s.  The default is set by
the \code{na.action} setting of \code{\LinkA{options}{options}}, and is
\code{\LinkA{na.fail}{na.fail}} if that is unset.  The `factory-fresh'
default is \code{\LinkA{na.omit}{na.omit}}.  Another possible value is
\code{NULL}, no action.  Value \code{\LinkA{na.exclude}{na.exclude}} can be useful.

\item[\code{method}] the method to be used; for fitting, currently only
\code{method = "qr"} is supported; \code{method = "model.frame"} returns
the model frame (the same as with \code{model = TRUE}, see below).

\item[\code{model, x, y, qr}] logicals.  If \code{TRUE} the corresponding
components of the fit (the model frame, the model matrix, the
response, the QR decomposition) are returned.


\item[\code{singular.ok}] logical. If \code{FALSE} (the default in S but
not in \R{}) a singular fit is an error.

\item[\code{contrasts}] an optional list. See the \code{contrasts.arg}
of \code{\LinkA{model.matrix.default}{model.matrix.default}}.

\item[\code{offset}] this can be used to specify an \emph{a priori} known
component to be included in the linear predictor during fitting.
This should be \code{NULL} or a numeric vector of length equal to
the number of cases.  One or more \code{\LinkA{offset}{offset}} terms can be
included in the formula instead or as well, and if more than one are
specified their sum is used.  See \code{\LinkA{model.offset}{model.offset}}.

\item[\code{...}] additional arguments to be passed to the low level
regression fitting functions (see below).
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Models for \code{lm} are specified symbolically.  A typical model has
the form \code{response \textasciitilde{} terms} where \code{response} is the (numeric)
response vector and \code{terms} is a series of terms which specifies a
linear predictor for \code{response}.  A terms specification of the form
\code{first + second} indicates all the terms in \code{first} together
with all the terms in \code{second} with duplicates removed.  A
specification of the form \code{first:second} indicates the set of
terms obtained by taking the interactions of all terms in \code{first}
with all terms in \code{second}.  The specification \code{first*second}
indicates the \emph{cross} of \code{first} and \code{second}.  This is
the same as \code{first + second + first:second}.

If the formula includes an \code{\LinkA{offset}{offset}}, this is evaluated and
subtracted from the response.

If \code{response} is a matrix a linear model is fitted separately by
least-squares to each column of the matrix.

See \code{\LinkA{model.matrix}{model.matrix}} for some further details.  The terms in
the formula will be re-ordered so that main effects come first,
followed by the interactions, all second-order, all third-order and so
on: to avoid this pass a \code{terms} object as the formula (see
\code{\LinkA{aov}{aov}} and \code{demo(glm.vr)} for an example).

A formula has an implied intercept term.  To remove this use either
\code{y \textasciitilde{} x - 1} or \code{y \textasciitilde{} 0 + x}.  See \code{\LinkA{formula}{formula}} for
more details of allowed formulae.

Non-\code{NULL} \code{weights} can be used to indicate that different
observations have different variances (with the values in
\code{weights} being inversely proportional to the variances); or
equivalently, when the elements of \code{weights} are positive
integers \eqn{w_i}{}, that each response \eqn{y_i}{} is the mean of
\eqn{w_i}{} unit-weight observations (including the case that there are
\eqn{w_i}{} observations equal to \eqn{y_i}{} and the data have been
summarized).

\code{lm} calls the lower level functions \code{\LinkA{lm.fit}{lm.fit}}, etc,
see below, for the actual numerical computations.  For programming
only, you may consider doing likewise.

All of \code{weights}, \code{subset} and \code{offset} are evaluated
in the same way as variables in \code{formula}, that is first in
\code{data} and then in the environment of \code{formula}.
\end{Details}
%
\begin{Value}
\code{lm} returns an object of \code{\LinkA{class}{class}} \code{"lm"} or for
multiple responses of class \code{c("mlm", "lm")}.

The functions \code{summary} and \code{\LinkA{anova}{anova}} are used to
obtain and print a summary and analysis of variance table of the
results.  The generic accessor functions \code{coefficients},
\code{effects}, \code{fitted.values} and \code{residuals} extract
various useful features of the value returned by \code{lm}.

An object of class \code{"lm"} is a list containing at least the
following components:

\begin{ldescription}
\item[\code{coefficients}] a named vector of coefficients
\item[\code{residuals}] the residuals, that is response minus fitted values.
\item[\code{fitted.values}] the fitted mean values.
\item[\code{rank}] the numeric rank of the fitted linear model.
\item[\code{weights}] (only for weighted fits) the specified weights.
\item[\code{df.residual}] the residual degrees of freedom.
\item[\code{call}] the matched call.
\item[\code{terms}] the \code{\LinkA{terms}{terms}} object used.
\item[\code{contrasts}] (only where relevant) the contrasts used.
\item[\code{xlevels}] (only where relevant) a record of the levels of the
factors used in fitting.
\item[\code{offset}] the offset used (missing if none were used).
\item[\code{y}] if requested, the response used.
\item[\code{x}] if requested, the model matrix used.
\item[\code{model}] if requested (the default), the model frame used.
\item[\code{na.action}] (where relevant) information returned by
\code{\LinkA{model.frame}{model.frame}} on the special handling of \code{NA}s.

\end{ldescription}
In addition, non-null fits will have components \code{assign},
\code{effects} and (unless not requested) \code{qr} relating to the linear
fit, for use by extractor functions such as \code{summary} and
\code{\LinkA{effects}{effects}}.
\end{Value}
%
\begin{Section}{Using time series}
Considerable care is needed when using \code{lm} with time series.

Unless \code{na.action = NULL}, the time series attributes are
stripped from the variables before the regression is done.  (This is
necessary as omitting \code{NA}s would invalidate the time series
attributes, and if \code{NA}s are omitted in the middle of the series
the result would no longer be a regular time series.)

Even if the time series attributes are retained, they are not used to
line up series, so that the time shift of a lagged or differenced
regressor would be ignored.  It is good practice to prepare a
\code{data} argument by \code{\LinkA{ts.intersect}{ts.intersect}(..., dframe = TRUE)},
then apply a suitable \code{na.action} to that data frame and call
\code{lm} with \code{na.action = NULL} so that residuals and fitted
values are time series.
\end{Section}
%
\begin{Note}\relax
Offsets specified by \code{offset} will not be included in predictions
by \code{\LinkA{predict.lm}{predict.lm}}, whereas those specified by an offset term
in the formula will be.
\end{Note}
%
\begin{Author}\relax
The design was inspired by the S function of the same name described
in Chambers (1992).  The implementation of model formula by Ross Ihaka
was based on Wilkinson \& Rogers (1973).
\end{Author}
%
\begin{References}\relax
Chambers, J. M. (1992)
\emph{Linear models.}
Chapter 4 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.

Wilkinson, G. N. and Rogers, C. E. (1973)
Symbolic descriptions of factorial models for analysis of variance.
\emph{Applied Statistics}, \bold{22}, 392--9.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{summary.lm}{summary.lm}} for summaries and \code{\LinkA{anova.lm}{anova.lm}} for
the ANOVA table; \code{\LinkA{aov}{aov}} for a different interface.

The generic functions \code{\LinkA{coef}{coef}}, \code{\LinkA{effects}{effects}},
\code{\LinkA{residuals}{residuals}}, \code{\LinkA{fitted}{fitted}}, \code{\LinkA{vcov}{vcov}}.

\code{\LinkA{predict.lm}{predict.lm}} (via \code{\LinkA{predict}{predict}}) for prediction,
including confidence and prediction intervals;
\code{\LinkA{confint}{confint}} for confidence intervals of \emph{parameters}.

\code{\LinkA{lm.influence}{lm.influence}} for regression diagnostics, and
\code{\LinkA{glm}{glm}} for \bold{generalized} linear models.

The underlying low level functions,
\code{\LinkA{lm.fit}{lm.fit}} for plain, and \code{\LinkA{lm.wfit}{lm.wfit}} for weighted
regression fitting.

More \code{lm()} examples are available e.g., in
\code{\LinkA{anscombe}{anscombe}}, \code{\LinkA{attitude}{attitude}}, \code{\LinkA{freeny}{freeny}},
\code{\LinkA{LifeCycleSavings}{LifeCycleSavings}}, \code{\LinkA{longley}{longley}},
\code{\LinkA{stackloss}{stackloss}}, \code{\LinkA{swiss}{swiss}}.

\code{biglm} in package \Rhref{http://CRAN.R-project.org/package=biglm}{\pkg{biglm}} for an alternative
way to fit linear models to large datasets (especially those with many
cases).
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2,10,20, labels=c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D90 <- lm(weight ~ group - 1) # omitting intercept

anova(lm.D9)
summary(lm.D90)

opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(lm.D9, las = 1)      # Residuals, Fitted, ...
par(opar)

### less simple examples in "See Also" above
\end{ExampleCode}
\end{Examples}
\HeaderA{lm.fit}{Fitter Functions for Linear Models}{lm.fit}
\aliasA{lm.wfit}{lm.fit}{lm.wfit}
\keyword{regression}{lm.fit}
\keyword{array}{lm.fit}
%
\begin{Description}\relax
These are the basic computing engines called by \code{\LinkA{lm}{lm}} used
to fit linear models.  These should usually \emph{not} be used
directly unless by experienced users.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lm.fit (x, y,    offset = NULL, method = "qr", tol = 1e-7,
       singular.ok = TRUE, ...)

lm.wfit(x, y, w, offset = NULL, method = "qr", tol = 1e-7,
        singular.ok = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] design matrix of dimension \code{n * p}.
\item[\code{y}] vector of observations of length \code{n}, or a matrix with
\code{n} rows.
\item[\code{w}] vector of weights (length \code{n}) to be used in the fitting
process for the \code{wfit} functions.  Weighted least squares is
used with weights \code{w}, i.e., \code{sum(w * e\textasciicircum{}2)} is minimized.
\item[\code{offset}] numeric of length \code{n}).  This can be used to
specify an \emph{a priori} known component to be included in the
linear predictor during fitting.

\item[\code{method}] currently, only \code{method="qr"} is supported.

\item[\code{tol}] tolerance for the \code{\LinkA{qr}{qr}} decomposition.  Default
is 1e-7.

\item[\code{singular.ok}] logical. If \code{FALSE}, a singular model is an
error.

\item[\code{...}] currently disregarded.
\end{ldescription}
\end{Arguments}
%
\begin{Value}


a list with components
\begin{ldescription}
\item[\code{coefficients}] \code{p} vector
\item[\code{residuals}] \code{n} vector or matrix
\item[\code{fitted.values}] \code{n} vector or matrix
\item[\code{effects}] (not null fits)\code{n} vector of orthogonal single-df
effects.  The first \code{rank} of them correspond to non-aliased
coefficients, and are named accordingly.
\item[\code{weights}] \code{n} vector --- \emph{only} for the \code{*wfit*}
functions.
\item[\code{rank}] integer, giving the rank
\item[\code{df.residual}] degrees of freedom of residuals
\item[\code{qr}] (not null fits) the QR decomposition, see \code{\LinkA{qr}{qr}}.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{lm}{lm}} which you should use for linear least squares regression,
unless you know better.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(utils)

set.seed(129)
n <- 7 ; p <- 2
X <- matrix(rnorm(n * p), n,p) # no intercept!
y <- rnorm(n)
w <- rnorm(n)^2

str(lmw <- lm.wfit(x=X, y=y, w=w))

str(lm. <- lm.fit (x=X, y=y))


\end{ExampleCode}
\end{Examples}
\HeaderA{lm.influence}{Regression Diagnostics}{lm.influence}
\aliasA{influence}{lm.influence}{influence}
\methaliasA{influence.glm}{lm.influence}{influence.glm}
\methaliasA{influence.lm}{lm.influence}{influence.lm}
\keyword{regression}{lm.influence}
%
\begin{Description}\relax
This function provides the basic quantities which are
used in forming a wide variety of diagnostics for
checking the quality of regression fits.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
influence(model, ...)
## S3 method for class 'lm'
influence(model, do.coef = TRUE, ...)
## S3 method for class 'glm'
influence(model, do.coef = TRUE, ...)

lm.influence(model, do.coef = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] an object as returned by \code{\LinkA{lm}{lm}} or \code{\LinkA{glm}{glm}}.
\item[\code{do.coef}] logical indicating if the changed \code{coefficients}
(see below) are desired.  These need \eqn{O(n^2 p)}{} computing time.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \code{\LinkA{influence.measures}{influence.measures}()} and other functions listed in
\bold{See Also} provide a more user oriented way of computing a
variety of regression diagnostics.  These all build on
\code{lm.influence}.  Note that for GLMs (other than the Gaussian
family with identity link) these are based on one-step approximations
which may be inadequate if a case has high influence.

An attempt is made to ensure that computed hat values that are
probably one are treated as one, and the corresponding rows in
\code{sigma} and \code{coefficients} are \code{NaN}.  (Dropping such a
case would normally result in a variable being dropped, so it is not
possible to give simple drop-one diagnostics.)

\code{\LinkA{naresid}{naresid}} is applied to the results and so will fill in
with \code{NA}s it the fit had \code{na.action = na.exclude}.
\end{Details}
%
\begin{Value}
A list containing the following components of the same length or
number of rows \eqn{n}{}, which is the number of non-zero weights.
Cases omitted in the fit are omitted unless a \code{\LinkA{na.action}{na.action}}
method was used (such as \code{\LinkA{na.exclude}{na.exclude}}) which restores them.

\begin{ldescription}
\item[\code{hat}] a vector containing the diagonal of the `hat' matrix.
\item[\code{coefficients}] (unless \code{do.coef} is false) a matrix whose
i-th row contains the change in the estimated coefficients which
results when the i-th case is dropped from the regression.  Note
that aliased coefficients are not included in the matrix.
\item[\code{sigma}] a vector whose i-th element contains the estimate
of the residual standard deviation obtained when the i-th
case is dropped from the regression.  (The approximations needed for
GLMs can result in this being \code{NaN}.)
\item[\code{wt.res}] a vector of \emph{weighted} (or for class \code{glm}
rather \emph{deviance}) residuals.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The \code{coefficients} returned by the \R{} version
of \code{lm.influence} differ from those computed by S.
Rather than returning the coefficients which result
from dropping each case, we return the changes in the coefficients.
This is more directly useful in many diagnostic measures.\\{}
Since these need \eqn{O(n^2 p)}{} computing time, they can be omitted by
\code{do.coef = FALSE}.

Note that cases with \code{weights == 0} are \emph{dropped} (contrary
to the situation in S).

If a model has been fitted with \code{na.action=na.exclude} (see
\code{\LinkA{na.exclude}{na.exclude}}), cases excluded in the fit \emph{are}
considered here.
\end{Note}
%
\begin{References}\relax
See the list in the documentation for \code{\LinkA{influence.measures}{influence.measures}}.

Chambers, J. M. (1992)
\emph{Linear models.}
Chapter 4 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{summary.lm}{summary.lm}} for \code{\LinkA{summary}{summary}} and related methods;\\{}
\code{\LinkA{influence.measures}{influence.measures}},\\{}
\code{\LinkA{hat}{hat}} for the hat matrix diagonals,\\{}
\code{\LinkA{dfbetas}{dfbetas}},
\code{\LinkA{dffits}{dffits}},
\code{\LinkA{covratio}{covratio}},
\code{\LinkA{cooks.distance}{cooks.distance}},
\code{\LinkA{lm}{lm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Analysis of the life-cycle savings data
## given in Belsley, Kuh and Welsch.
summary(lm.SR <- lm(sr ~ pop15 + pop75 + dpi + ddpi,
                    data = LifeCycleSavings),
        corr = TRUE)
utils::str(lmI <- lm.influence(lm.SR))

## For more "user level" examples, use example(influence.measures)
\end{ExampleCode}
\end{Examples}
\HeaderA{lm.summaries}{Accessing Linear Model Fits}{lm.summaries}
\aliasA{family.lm}{lm.summaries}{family.lm}
\aliasA{formula.lm}{lm.summaries}{formula.lm}
\aliasA{labels.lm}{lm.summaries}{labels.lm}
\aliasA{residuals.lm}{lm.summaries}{residuals.lm}
\keyword{regression}{lm.summaries}
\keyword{models}{lm.summaries}
%
\begin{Description}\relax
All these functions are \code{\LinkA{methods}{methods}} for class \code{"lm"}  objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'lm'
family(object, ...)

## S3 method for class 'lm'
formula(x, ...)

## S3 method for class 'lm'
residuals(object,
          type = c("working", "response", "deviance", "pearson",
                   "partial"),
          ...)

## S3 method for class 'lm'
labels(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object, x}] an object inheriting from class \code{lm}, usually
the result of a call to \code{\LinkA{lm}{lm}} or \code{\LinkA{aov}{aov}}.
\item[\code{...}] further arguments passed to or from other methods.
\item[\code{type}] the type of residuals which should be returned.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The generic accessor functions \code{coef}, \code{effects},
\code{fitted} and \code{residuals} can be used to extract
various useful features of the value returned by \code{lm}.

The working and response residuals are `observed - fitted'.  The
deviance and pearson residuals are weighted residuals, scaled by the
square root of the weights used in fitting.  The partial residuals
are a matrix with each column formed by omitting a term from the
model.  In all these, zero weight cases are never omitted (as opposed
to the standardized \code{\LinkA{rstudent}{rstudent}} residuals, and the
\code{\LinkA{weighted.residuals}{weighted.residuals}}).

How \code{residuals} treats cases with missing values in the original
fit is determined by the \code{na.action} argument of that fit.
If \code{na.action = na.omit} omitted cases will not appear in the
residuals, whereas if \code{na.action = na.exclude} they will appear,
with residual value \code{NA}.  See also \code{\LinkA{naresid}{naresid}}.

The \code{"lm"} method for generic \code{\LinkA{labels}{labels}} returns the
term labels for estimable terms, that is the names of the terms with
an least one estimable coefficient.
\end{Details}
%
\begin{References}\relax
Chambers, J. M. (1992)
\emph{Linear models.}
Chapter 4 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
The model fitting function \code{\LinkA{lm}{lm}}, \code{\LinkA{anova.lm}{anova.lm}}.

\code{\LinkA{coef}{coef}}, \code{\LinkA{deviance}{deviance}},
\code{\LinkA{df.residual}{df.residual}},
\code{\LinkA{effects}{effects}}, \code{\LinkA{fitted}{fitted}},
\code{\LinkA{glm}{glm}} for \bold{generalized} linear models,
\code{\LinkA{influence}{influence}} (etc on that page) for regression diagnostics,
\code{\LinkA{weighted.residuals}{weighted.residuals}},
\code{\LinkA{residuals}{residuals}}, \code{\LinkA{residuals.glm}{residuals.glm}},
\code{\LinkA{summary.lm}{summary.lm}}, \code{\LinkA{weights}{weights}}.

\LinkA{influence.measures}{influence.measures} for deletion diagnostics, including
standardized (\code{\LinkA{rstandard}{rstandard}})
and studentized (\code{\LinkA{rstudent}{rstudent}}) residuals.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

##-- Continuing the  lm(.) example:
coef(lm.D90)# the bare coefficients

## The 2 basic regression diagnostic plots [plot.lm(.) is preferred]
plot(resid(lm.D90), fitted(lm.D90))# Tukey-Anscombe's
abline(h=0, lty=2, col = 'gray')

qqnorm(residuals(lm.D90))
\end{ExampleCode}
\end{Examples}
\HeaderA{loadings}{Print Loadings in Factor Analysis}{loadings}
\aliasA{print.factanal}{loadings}{print.factanal}
\aliasA{print.loadings}{loadings}{print.loadings}
\keyword{multivariate}{loadings}
\keyword{print}{loadings}
%
\begin{Description}\relax
Extract or print loadings in factor analysis (or principal
components analysis).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
loadings(x)

## S3 method for class 'loadings'
print(x, digits = 3, cutoff = 0.1, sort = FALSE, ...)

## S3 method for class 'factanal'
print(x, digits = 3, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object of class \code{"\LinkA{factanal}{factanal}"} or
\code{"\LinkA{princomp}{princomp}"} or the \code{loadings} component of such an
object.
\item[\code{digits}] number of decimal places to use in printing uniquenesses
and loadings.
\item[\code{cutoff}] loadings smaller than this (in absolute value) are suppressed.
\item[\code{sort}] logical. If true, the variables are sorted by their
importance on each factor.  Each variable with any loading larger
than 0.5 (in modulus) is assigned to the factor with the largest
loading, and the variables are printed in the order of the factor
they are assigned to, then those unassigned.
\item[\code{...}] further arguments for other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
`Loadings' is a term from \emph{factor analysis}, but because
factor analysis and principal component analysis (PCA) are often
conflated in the social science literature, it was used for PCA by
SPSS and hence by \code{\LinkA{princomp}{princomp}} in S-PLUS to help SPSS users.

Small loadings are conventionally not printed (replaced by spaces), to
draw the eye to the pattern of the larger loadings.

The \code{print} method for class \code{"\LinkA{factanal}{factanal}"} calls the
\code{"loadings"} method to print the loadings, and so passes down
arguments such as \code{cutoff} and \code{sort}.
\end{Details}
%
\begin{SeeAlso}\relax
\code{\LinkA{factanal}{factanal}}, \code{\LinkA{princomp}{princomp}}
\end{SeeAlso}
\HeaderA{loess}{Local Polynomial Regression Fitting}{loess}
\keyword{smooth}{loess}
\keyword{loess}{loess}
%
\begin{Description}\relax
Fit a polynomial surface determined by one or more numerical
predictors, using local fitting.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
loess(formula, data, weights, subset, na.action, model = FALSE,
      span = 0.75, enp.target, degree = 2,
      parametric = FALSE, drop.square = FALSE, normalize = TRUE,
      family = c("gaussian", "symmetric"),
      method = c("loess", "model.frame"),
      control = loess.control(...), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a \LinkA{formula}{formula} specifying the numeric response and
one to four numeric predictors (best specified via an interaction,
but can also be specified additively).  Will be coerced to a formula
if necessary.
\item[\code{data}] an optional data frame, list or environment (or object
coercible by \code{\LinkA{as.data.frame}{as.data.frame}} to a data frame) containing
the variables in the model.  If not found in \code{data}, the
variables are taken from \code{environment(formula)},
typically the environment from which \code{loess} is called.
\item[\code{weights}] optional weights for each case.
\item[\code{subset}] an optional specification of a subset of the data to be
used.
\item[\code{na.action}] the action to be taken with missing values in the
response or predictors.  The default is given by
\code{getOption("na.action")}.
\item[\code{model}] should the model frame be returned?
\item[\code{span}] the parameter \eqn{\alpha}{} which controls the degree of
smoothing.
\item[\code{enp.target}] an alternative way to specify \code{span}, as the
approximate equivalent number of parameters to be used.
\item[\code{degree}] the degree of the polynomials to be used, normally 1 or
2. (Degree 0 is also allowed, but see the `Note'.)
\item[\code{parametric}] should any terms be fitted globally rather than
locally?  Terms can be specified by name, number or as a logical
vector of the same length as the number of predictors.
\item[\code{drop.square}] for fits with more than one predictor and
\code{degree=2}, should the quadratic term be dropped for particular
predictors?  Terms are specified in the same way as for
\code{parametric}.
\item[\code{normalize}] should the predictors be normalized to a common scale
if there is more than one?  The normalization used is to set the
10\% trimmed standard deviation to one.  Set to false for spatial
coordinate predictors and others know to be a common scale.
\item[\code{family}] if \code{"gaussian"} fitting is by least-squares, and if
\code{"symmetric"} a re-descending M estimator is used with Tukey's
biweight function.
\item[\code{method}] fit the model or just extract the model frame.
\item[\code{control}] control parameters: see \code{\LinkA{loess.control}{loess.control}}.
\item[\code{...}] control parameters can also be supplied directly.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Fitting is done locally.  That is, for the fit at point \eqn{x}{}, the
fit is made using points in a neighbourhood of \eqn{x}{}, weighted by
their distance from \eqn{x}{} (with differences in `parametric'
variables being ignored when computing the distance).  The size of the
neighbourhood is controlled by \eqn{\alpha}{} (set by \code{span} or
\code{enp.target}).  For \eqn{\alpha < 1}{}, the
neighbourhood includes proportion \eqn{\alpha}{} of the points,
and these have tricubic weighting (proportional to \eqn{(1 -
    \mathrm{(dist/maxdist)}^3)^3}{}).  For
\eqn{\alpha > 1}{}, all points are used, with the
`maximum distance' assumed to be \eqn{\alpha^{1/p}}{}
times the actual maximum distance for \eqn{p}{} explanatory variables.

For the default family, fitting is by (weighted) least squares.  For
\code{family="symmetric"} a few iterations of an M-estimation
procedure with Tukey's biweight are used.  Be aware that as the initial
value is the least-squares fit, this need not be a very resistant fit.

It can be important to tune the control list to achieve acceptable
speed.  See \code{\LinkA{loess.control}{loess.control}} for details.
\end{Details}
%
\begin{Value}
An object of class \code{"loess"}.
\end{Value}
%
\begin{Note}\relax
As this is based on \code{cloess}, it is similar to but not identical to
the \code{loess} function of S.  In particular, conditioning is not
implemented.

The memory usage of this implementation of \code{loess} is roughly
quadratic in the number of points, with 1000 points taking about 10Mb.

\code{degree = 0}, local constant fitting, is allowed in this
implementation but not documented in the reference.  It seems very little
tested, so use with caution.
\end{Note}
%
\begin{Author}\relax
B. D. Ripley, based on the \code{cloess} package of Cleveland,
Grosse and Shyu.
\end{Author}
%
\begin{Source}\relax
The 1998 version of \code{cloess} package of Cleveland,
Grosse and Shyu.  A later version is available as \code{dloess} at
\url{http://www.netlib.org/a}.
\end{Source}
%
\begin{References}\relax
W. S. Cleveland, E. Grosse and W. M. Shyu (1992) Local regression
models. Chapter 8 of \emph{Statistical Models in S} eds J.M. Chambers
and T.J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{loess.control}{loess.control}},
\code{\LinkA{predict.loess}{predict.loess}}.

\code{\LinkA{lowess}{lowess}}, the ancestor of \code{loess} (with
different defaults!).
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
cars.lo <- loess(dist ~ speed, cars)
predict(cars.lo, data.frame(speed = seq(5, 30, 1)), se = TRUE)
# to allow extrapolation
cars.lo2 <- loess(dist ~ speed, cars,
  control = loess.control(surface = "direct"))
predict(cars.lo2, data.frame(speed = seq(5, 30, 1)), se = TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{loess.control}{Set Parameters for Loess}{loess.control}
\keyword{smooth}{loess.control}
\keyword{loess}{loess.control}
%
\begin{Description}\relax
Set control parameters for \code{loess} fits.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
loess.control(surface = c("interpolate", "direct"),
              statistics = c("approximate", "exact"),
              trace.hat = c("exact", "approximate"),
              cell = 0.2, iterations = 4, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{surface}] should the fitted surface be computed exactly or via
interpolation from a kd tree?
\item[\code{statistics}] should the statistics be computed exactly or
approximately? Exact computation can be very slow.
\item[\code{trace.hat}] should the trace of the smoother matrix be computed
exactly or approximately? It is recommended to use the approximation
for more than about 1000 data points.
\item[\code{cell}] if interpolation is used this controls the accuracy of the
approximation via the maximum number of points in a  cell in the kd
tree. Cells with more than \code{floor(n*span*cell)} points are subdivided.
\item[\code{iterations}] the number of iterations used in robust fitting.
\item[\code{...}] further arguments which are ignored.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with components
\begin{ldescription}
\item[\code{surface}] 
\item[\code{statistics}] 
\item[\code{trace.hat}] 
\item[\code{cell}] 
\item[\code{iterations}] 
\end{ldescription}
with meanings as explained under `Arguments'.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{loess}{loess}}
\end{SeeAlso}
\HeaderA{Logistic}{The Logistic Distribution}{Logistic}
\aliasA{dlogis}{Logistic}{dlogis}
\aliasA{plogis}{Logistic}{plogis}
\aliasA{qlogis}{Logistic}{qlogis}
\aliasA{rlogis}{Logistic}{rlogis}
\keyword{distribution}{Logistic}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the logistic distribution with parameters
\code{location} and \code{scale}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dlogis(x, location = 0, scale = 1, log = FALSE)
plogis(q, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
qlogis(p, location = 0, scale = 1, lower.tail = TRUE, log.p = FALSE)
rlogis(n, location = 0, scale = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{location, scale}] location and scale parameters.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{location} or \code{scale} are omitted, they assume the
default values of \code{0} and \code{1} respectively.

The Logistic distribution with \code{location} \eqn{= \mu}{} and
\code{scale} \eqn{= \sigma}{} has distribution function
\deqn{
    F(x) = \frac{1}{1 + e^{-(x-\mu)/\sigma}}%
  }{}  and density
\deqn{
    f(x)= \frac{1}{\sigma}\frac{e^{(x-\mu)/\sigma}}{(1 + e^{(x-\mu)/\sigma})^2}%
  }{}

It is a long-tailed distribution with mean \eqn{\mu}{} and variance
\eqn{\pi^2/3 \sigma^2}{}.
\end{Details}
%
\begin{Value}
\code{dlogis} gives the density,
\code{plogis} gives the distribution function,
\code{qlogis} gives the quantile function, and
\code{rlogis} generates random deviates.
\end{Value}
%
\begin{Note}\relax
\code{qlogis(p)} is the same as the well known `\emph{logit}'
function, \eqn{logit(p) = \log p/(1-p)}{},
and \code{plogis(x)} has consequently been called the `inverse logit'.

The distribution function is a rescaled hyperbolic tangent,
\code{plogis(x) == (1+ \LinkA{tanh}{tanh}(x/2))/2}, and it is called a
\emph{sigmoid function} in contexts such as neural networks.
\end{Note}
%
\begin{Source}\relax
\code{[dpq]logis} are calculated directly from the definitions.

\code{rlogis} uses inversion.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 2, chapter 23.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
var(rlogis(4000, 0, scale = 5))# approximately (+/- 3)
pi^2/3 * 5^2
\end{ExampleCode}
\end{Examples}
\HeaderA{logLik}{Extract Log-Likelihood}{logLik}
\methaliasA{logLik.lm}{logLik}{logLik.lm}
\aliasA{print.logLik}{logLik}{print.logLik}
\aliasA{str.logLik}{logLik}{str.logLik}
\keyword{models}{logLik}
%
\begin{Description}\relax
This function is generic; method functions can be written to handle
specific classes of objects.  Classes which have methods for this
function include: \code{"glm"}, \code{"lm"}, \code{"nls"} and
\code{"Arima"}.  Packages contain methods for other classes, such as
\code{"fitdistr"}, \code{"negbin"} and \code{"polr"} in package
\Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}, \code{"multinom"} in package \Rhref{http://CRAN.R-project.org/package=nnet}{\pkg{nnet}} and
\code{"gls"}, \code{"gnls"} \code{"lme"} and others in package
\Rhref{http://CRAN.R-project.org/package=nlme}{\pkg{nlme}}. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
logLik(object, ...)

## S3 method for class 'lm'
logLik(object, REML = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] any object from which a log-likelihood value, or a
contribution to a log-likelihood value, can be extracted.
\item[\code{...}] some methods for this generic function require additional
arguments.
\item[\code{REML}] an optional logical value.  If \code{TRUE} the restricted
log-likelihood is returned, else, if \code{FALSE}, the
log-likelihood is returned.  Defaults to \code{FALSE}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For a \code{"glm"} fit the \code{\LinkA{family}{family}} does not have to
specify how to calculate the log-likelihood, so this is based on using
the family's \code{aic()} function to compute the AIC.  For the
\code{\LinkA{gaussian}{gaussian}}, \code{\LinkA{Gamma}{Gamma}} and
\code{\LinkA{inverse.gaussian}{inverse.gaussian}} families it assumed that the dispersion
of the GLM is estimated and has been counted as a parameter in the AIC
value, and for all other families it is assumed that the dispersion is
known.  Note that this procedure does not give the maximized
likelihood for \code{"glm"} fits from the Gamma and inverse gaussian
families, as the estimate of dispersion used is not the MLE.

For \code{"lm"} fits it is assumed that the scale has been estimated
(by maximum likelihood or REML), and all the constants in the
log-likelihood are included.  That method is only applicable to
single-response fits.
\end{Details}
%
\begin{Value}
Returns an object of class \code{logLik}.  This is a number with at
least one attribute, \code{"df"} (\bold{d}egrees of \bold{f}reedom),
giving the number of (estimated) parameters in the model.

There is a simple \code{print} method for \code{"logLik"} objects.

There may be other attributes depending on the method used: see the
appropriate documentation.  One that is used by several methods is
\code{"nobs"}, the number of observations used in estimation (after
the restrictions if \code{REML = TRUE}).
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{References}\relax
For \code{logLik.lm}:

Harville, D.A. (1974).
Bayesian inference for variance components using only error contrasts.
\emph{Biometrika}, \bold{61}, 383--385.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{logLik.gls}{logLik.gls}}, \code{\LinkA{logLik.lme}{logLik.lme}}, in
package \Rhref{http://CRAN.R-project.org/package=nlme}{\pkg{nlme}}, etc.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:5
lmx <- lm(x ~ 1)
logLik(lmx) # using print.logLik() method
utils::str(logLik(lmx))

## lm method
(fm1 <- lm(rating ~ ., data = attitude))
logLik(fm1)
logLik(fm1, REML = TRUE)

utils::data(Orthodont, package="nlme")
fm1 <- lm(distance ~ Sex * age, Orthodont)
logLik(fm1)
logLik(fm1, REML = TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{loglin}{Fitting Log-Linear Models}{loglin}
\keyword{category}{loglin}
\keyword{models}{loglin}
%
\begin{Description}\relax
\code{loglin} is used to fit log-linear models to multidimensional
contingency tables by Iterative Proportional Fitting.  
\end{Description}
%
\begin{Usage}
\begin{verbatim}
loglin(table, margin, start = rep(1, length(table)), fit = FALSE,
       eps = 0.1, iter = 20, param = FALSE, print = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{table}] a contingency table to be fit, typically the output from
\code{table}.
\item[\code{margin}] a list of vectors with the marginal totals to be fit.

(Hierarchical) log-linear models can be specified in terms of these
marginal totals which give the `maximal' factor subsets contained
in the model.  For example, in a three-factor model,
\code{list(c(1, 2), c(1, 3))} specifies a model which contains
parameters for the grand mean, each factor, and the 1-2 and 1-3
interactions, respectively (but no 2-3 or 1-2-3 interaction), i.e.,
a model where factors 2 and 3 are independent conditional on factor
1 (sometimes represented as `[12][13]').

The names of factors (i.e., \code{names(dimnames(table))}) may be
used rather than numeric indices.

\item[\code{start}] a starting estimate for the fitted table.  This optional
argument is important for incomplete tables with structural zeros
in \code{table} which should be preserved in the fit.  In this
case, the corresponding entries in \code{start} should be zero and
the others can be taken as one.
\item[\code{fit}] a logical indicating whether the fitted values should be
returned.
\item[\code{eps}] maximum deviation allowed between observed and fitted
margins.
\item[\code{iter}] maximum number of iterations.
\item[\code{param}] a logical indicating whether the parameter values should
be returned.
\item[\code{print}] a logical.  If \code{TRUE}, the number of iterations and
the final deviation are printed.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The Iterative Proportional Fitting algorithm as presented in
Haberman (1972) is used for fitting the model.  At most \code{iter}
iterations are performed, convergence is taken to occur when the
maximum deviation between observed and fitted margins is less than
\code{eps}.  All internal computations are done in double precision;
there is no limit on the number of factors (the dimension of the
table) in the model.

Assuming that there are no structural zeros, both the Likelihood
Ratio Test and Pearson test statistics have an asymptotic chi-squared
distribution with \code{df} degrees of freedom.

Note that the IPF steps are applied to the factors in the order given
in \code{margin}.  Hence if the model is decomposable and the order
given in \code{margin} is a running intersection property ordering
then IPF will converge in one iteration.

Package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} contains \code{loglm}, a front-end to
\code{loglin} which allows the log-linear model to be specified and
fitted in a formula-based manner similar to that of other fitting
functions such as \code{lm} or \code{glm}.
\end{Details}
%
\begin{Value}
A list with the following components.
\begin{ldescription}
\item[\code{lrt}] the Likelihood Ratio Test statistic.
\item[\code{pearson}] the Pearson test statistic (X-squared).
\item[\code{df}] the degrees of freedom for the fitted model.  There is no
adjustment for structural zeros.
\item[\code{margin}] list of the margins that were fit.  Basically the same
as the input \code{margin}, but with numbers replaced by names
where possible.
\item[\code{fit}] An array like \code{table} containing the fitted values.
Only returned if \code{fit} is \code{TRUE}.
\item[\code{param}] A list containing the estimated parameters of the
model.  The `standard' constraints of zero marginal sums
(e.g., zero row and column sums for a two factor parameter) are
employed.  Only returned if \code{param} is \code{TRUE}.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Kurt Hornik
\end{Author}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Haberman, S. J. (1972)
Log-linear fit for contingency tables---Algorithm AS51.
\emph{Applied Statistics}, \bold{21}, 218--225.

Agresti, A. (1990)
\emph{Categorical data analysis}.
New York: Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{table}{table}}.

\code{\LinkA{loglm}{loglm}} in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} for a
user-friendly wrapper.

\code{\LinkA{glm}{glm}} for another way to fit log-linear models.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Model of joint independence of sex from hair and eye color.
fm <- loglin(HairEyeColor, list(c(1, 2), c(1, 3), c(2, 3)))
fm
1 - pchisq(fm$lrt, fm$df)
## Model with no three-factor interactions fits well.
\end{ExampleCode}
\end{Examples}
\HeaderA{Lognormal}{The Log Normal Distribution}{Lognormal}
\aliasA{dlnorm}{Lognormal}{dlnorm}
\aliasA{plnorm}{Lognormal}{plnorm}
\aliasA{qlnorm}{Lognormal}{qlnorm}
\aliasA{rlnorm}{Lognormal}{rlnorm}
\keyword{distribution}{Lognormal}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the log normal distribution whose logarithm has mean
equal to \code{meanlog} and standard  deviation equal to \code{sdlog}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dlnorm(x, meanlog = 0, sdlog = 1, log = FALSE)
plnorm(q, meanlog = 0, sdlog = 1, lower.tail = TRUE, log.p = FALSE)
qlnorm(p, meanlog = 0, sdlog = 1, lower.tail = TRUE, log.p = FALSE)
rlnorm(n, meanlog = 0, sdlog = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{meanlog, sdlog}] mean and standard deviation of the distribution
on the log scale with default values of \code{0} and \code{1} respectively.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The log normal distribution has density
\deqn{
    f(x) = \frac{1}{\sqrt{2\pi}\sigma x} e^{-(\log(x) - \mu)^2/2 \sigma^2}%
  }{}
where \eqn{\mu}{} and \eqn{\sigma}{} are the mean and standard
deviation of the logarithm.
The mean is \eqn{E(X) = exp(\mu + 1/2 \sigma^2)}{},
the median is \eqn{med(X) = exp(\mu)}{}, and the variance
\eqn{Var(X) = exp(2\mu + \sigma^2)(exp(\sigma^2) - 1)}{}
and hence the coefficient of variation is
\eqn{\sqrt{exp(\sigma^2) - 1}}{} which is
approximately \eqn{\sigma}{} when that is small (e.g., \eqn{\sigma < 1/2}{}).
\end{Details}
%
\begin{Value}
\code{dlnorm} gives the density,
\code{plnorm} gives the distribution function,
\code{qlnorm} gives the quantile function, and
\code{rlnorm} generates random deviates.
\end{Value}
%
\begin{Note}\relax
The cumulative hazard \eqn{H(t) = - \log(1 - F(t))}{}
is \code{-plnorm(t, r, lower = FALSE, log = TRUE)}.
\end{Note}
%
\begin{Source}\relax
\code{dlnorm} is calculated from the definition (in `Details').
\code{[pqr]lnorm} are based on the relationship to the normal.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 1, chapter 14.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dnorm}{dnorm}} for the normal distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
dlnorm(1) == dnorm(0)
\end{ExampleCode}
\end{Examples}
\HeaderA{lowess}{Scatter Plot Smoothing}{lowess}
\keyword{smooth}{lowess}
%
\begin{Description}\relax
This function performs the computations for the
\emph{LOWESS} smoother which uses locally-weighted polynomial
regression (see the references).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lowess(x, y = NULL, f = 2/3, iter = 3,
       delta = 0.01 * diff(range(xy$x[o])))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] vectors giving the coordinates of the points in the scatter plot.
Alternatively a single plotting structure can be specified -- see
\code{\LinkA{xy.coords}{xy.coords}}.
\item[\code{f}] the smoother span. This gives the proportion of points in
the plot which influence the smooth at each value.
Larger values give more smoothness.
\item[\code{iter}] the number of `robustifying' iterations which should be
performed.
Using smaller values of \code{iter} will make \code{lowess} run faster.
\item[\code{delta}] See `Details'.  Defaults to 1/100th of the range
of \code{x}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{lowess} is defined by a complex algorithm, the Ratfor original
of which (by W. S. Cleveland) can be found in the \R{} sources as file
\file{src/appl/lowess.doc}.  Normally a local linear polynomial fit is
used, but under some circumstances (see the file) a local constant fit
can be used.  `Local' is defined by the distance to the
\code{floor(f*n)}th nearest neighbour, and tricubic weighting is used
for \code{x} which fall within the neighbourhood.

The initial fit is done using weighted least squares.  If
\code{iter > 0}, further weighted fits are done using the product of
the weights from the proximity of the \code{x} values and case weights
derived from the residuals at the previous iteration.  Specifically,
the case weight is Tukey's biweight, with cutoff 6 times the MAD of the
residuals.  (The current \R{} implementation differs from the original
in stopping iteration if the MAD is effectively zero since the
algorithm is highly unstable in that case.)

\code{delta} is used to speed up computation: instead of computing the
local polynomial fit at each data point it is not computed for points
within \code{delta} of the last computed point, and linear
interpolation is used to fill in the fitted values for the skipped
points.
\end{Details}
%
\begin{Value}
\code{lowess} returns a list containing components
\code{x} and \code{y} which give the coordinates of the smooth.
The smooth can be added to a plot of the original
points with the function \code{lines}: see the examples.
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Cleveland, W. S. (1979)
Robust locally weighted regression and smoothing scatterplots.
\emph{J. Amer. Statist. Assoc.} \bold{74}, 829--836.

Cleveland, W. S. (1981)
LOWESS: A program for smoothing scatterplots by robust locally weighted
regression. \emph{The American Statistician}, \bold{35}, 54.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{loess}{loess}}, a newer
formula based version of \code{lowess} (with different defaults!).
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

plot(cars, main = "lowess(cars)")
lines(lowess(cars), col = 2)
lines(lowess(cars, f=.2), col = 3)
legend(5, 120, c(paste("f = ", c("2/3", ".2"))), lty = 1, col = 2:3)
\end{ExampleCode}
\end{Examples}
\HeaderA{ls.diag}{Compute Diagnostics for \code{lsfit} Regression Results}{ls.diag}
\keyword{regression}{ls.diag}
%
\begin{Description}\relax
Computes basic statistics, including standard errors, t- and p-values
for the regression coefficients.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ls.diag(ls.out)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{ls.out}] Typically the result of \code{\LinkA{lsfit}{lsfit}()}
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A \code{list} with the following numeric components.
\begin{ldescription}
\item[\code{std.dev}] The standard deviation of the errors, an estimate of
\eqn{\sigma}{}.
\item[\code{hat}] diagonal entries \eqn{h_{ii}}{} of the hat matrix \eqn{H}{}
\item[\code{std.res}] standardized residuals
\item[\code{stud.res}] studentized residuals
\item[\code{cooks}] Cook's distances
\item[\code{dfits}] DFITS statistics
\item[\code{correlation}] correlation matrix
\item[\code{std.err}] standard errors of the regression coefficients
\item[\code{cov.scaled}] Scaled covariance matrix of the coefficients
\item[\code{cov.unscaled}] Unscaled covariance matrix of the coefficients
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Belsley, D. A., Kuh, E. and Welsch, R. E. (1980)
\emph{Regression Diagnostics.}
New York: Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{hat}{hat}} for the hat matrix diagonals,
\code{\LinkA{ls.print}{ls.print}},
\code{\LinkA{lm.influence}{lm.influence}}, \code{\LinkA{summary.lm}{summary.lm}},
\code{\LinkA{anova}{anova}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}


##-- Using the same data as the lm(.) example:
lsD9 <- lsfit(x = as.numeric(gl(2, 10, 20)), y = weight)
dlsD9 <- ls.diag(lsD9)
utils::str(dlsD9, give.attr=FALSE)
abs(1 - sum(dlsD9$hat) / 2) < 10*.Machine$double.eps # sum(h.ii) = p
plot(dlsD9$hat, dlsD9$stud.res, xlim=c(0,0.11))
abline(h = 0, lty = 2, col = "lightgray")
\end{ExampleCode}
\end{Examples}
\HeaderA{ls.print}{Print \code{lsfit} Regression Results}{ls.print}
\keyword{regression}{ls.print}
%
\begin{Description}\relax
Computes basic statistics, including standard errors, t- and p-values
for the regression coefficients and prints them if \code{print.it} is
\code{TRUE}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ls.print(ls.out, digits = 4, print.it = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{ls.out}] Typically the result of \code{\LinkA{lsfit}{lsfit}()}
\item[\code{digits}] The number of significant digits used for printing
\item[\code{print.it}] a logical indicating whether the result should also be
printed
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with the components
\begin{ldescription}
\item[\code{summary}] The ANOVA table of the regression
\item[\code{coef.table}] matrix with regression coefficients, standard
errors, t- and p-values
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Usually you would use \code{summary(lm(...))} and
\code{anova(lm(...))} to obtain similar output.
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{ls.diag}{ls.diag}}, \code{\LinkA{lsfit}{lsfit}}, also for examples;
\code{\LinkA{lm}{lm}}, \code{\LinkA{lm.influence}{lm.influence}} which usually are
preferable.
\end{SeeAlso}
\HeaderA{lsfit}{Find the Least Squares Fit}{lsfit}
\keyword{regression}{lsfit}
%
\begin{Description}\relax
The least squares estimate of \bold{\eqn{\beta}{}} in the model
\deqn{\bold{Y} = \bold{X \beta} + \bold{\epsilon}}{}
is found.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lsfit(x, y, wt = NULL, intercept = TRUE, tolerance = 1e-07,
      yname = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a matrix whose rows correspond to cases and whose columns
correspond to variables.
\item[\code{y}] the responses, possibly a matrix if you want to fit multiple
left hand sides.
\item[\code{wt}] an optional vector of weights for performing weighted least squares.
\item[\code{intercept}] whether or not an intercept term should be used.
\item[\code{tolerance}] the tolerance to be used in the matrix decomposition.
\item[\code{yname}] names to be used for the response variables.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If weights are specified then a weighted least squares is performed
with the weight given to the \emph{j}th case specified by the \emph{j}th
entry in \code{wt}.

If any observation has a missing value in any field, that observation
is removed before the analysis is carried out.
This can be quite inefficient if there is a lot of missing data.

The implementation is via a modification of the LINPACK subroutines
which allow for multiple left-hand sides.
\end{Details}
%
\begin{Value}
A list with the following named components:
\begin{ldescription}
\item[\code{coef}] the least squares estimates of the coefficients in
the model (\bold{\eqn{\beta}{}} as stated above).
\item[\code{residuals}] residuals from the fit.
\item[\code{intercept}] indicates whether an intercept was fitted.
\item[\code{qr}] the QR decomposition of the design matrix.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{lm}{lm}} which usually is preferable;
\code{\LinkA{ls.print}{ls.print}}, \code{\LinkA{ls.diag}{ls.diag}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

##-- Using the same data as the lm(.) example:
lsD9 <- lsfit(x = unclass(gl(2,10)), y = weight)
ls.print(lsD9)
\end{ExampleCode}
\end{Examples}
\HeaderA{mad}{Median Absolute Deviation}{mad}
\keyword{univar}{mad}
\keyword{robust}{mad}
%
\begin{Description}\relax
Compute the median absolute deviation, i.e., the (lo-/hi-) median of
the absolute deviations from the median, and (by default) adjust by a
factor for asymptotically normal consistency.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mad(x, center = median(x), constant = 1.4826, na.rm = FALSE,
    low = FALSE, high = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector.
\item[\code{center}] Optionally, the centre: defaults to the median.
\item[\code{constant}] scale factor.
\item[\code{na.rm}] if \code{TRUE} then \code{NA} values are stripped
from \code{x} before computation takes place.
\item[\code{low}] if \code{TRUE}, compute the `lo-median', i.e., for even
sample size, do not average the two middle values, but take the
smaller one.
\item[\code{high}] if \code{TRUE}, compute the `hi-median', i.e., take the
larger of the two middle values for even sample size.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The actual value calculated is \code{constant * cMedian(abs(x - center))}
with the default value of \code{center} being \code{median(x)}, and
\code{cMedian} being the usual, the `low' or `high' median, see
the arguments description for \code{low} and \code{high} above.

The default \code{constant = 1.4826} (approximately
\eqn{1/\Phi^{-1}(\frac 3 4)}{} = \code{1/qnorm(3/4)})
ensures consistency, i.e.,
\deqn{E[mad(X_1,\dots,X_n)] = \sigma}{}
for \eqn{X_i}{} distributed as \eqn{N(\mu, \sigma^2)}{}
and large \eqn{n}{}.

If \code{na.rm} is \code{TRUE} then \code{NA}
values are stripped from \code{x} before computation takes place.
If this is not done then an \code{NA} value in
\code{x} will cause \code{mad} to return \code{NA}.
\end{Details}
%
\begin{SeeAlso}\relax
\code{\LinkA{IQR}{IQR}} which is simpler but less robust,
\code{\LinkA{median}{median}}, \code{\LinkA{var}{var}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
mad(c(1:9))
print(mad(c(1:9),     constant=1)) ==
      mad(c(1:8,100), constant=1)       # = 2 ; TRUE
x <- c(1,2,3, 5,7,8)
sort(abs(x - median(x)))
c(mad(x, constant=1),
  mad(x, constant=1, low = TRUE),
  mad(x, constant=1, high = TRUE))
\end{ExampleCode}
\end{Examples}
\HeaderA{mahalanobis}{Mahalanobis Distance}{mahalanobis}
\keyword{multivariate}{mahalanobis}
%
\begin{Description}\relax
Returns the squared Mahalanobis distance of all rows in \code{x} and the
vector \eqn{\mu}{} = \code{center} with respect to
\eqn{\Sigma}{} = \code{cov}.
This is (for vector \code{x}) defined as
\deqn{D^2 = (x - \mu)' \Sigma^{-1} (x - \mu)}{}
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mahalanobis(x, center, cov, inverted=FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] vector or matrix of data with, say, \eqn{p}{} columns.
\item[\code{center}] mean vector of the distribution or second data vector of
length \eqn{p}{}.
\item[\code{cov}] covariance matrix (\eqn{p \times p}{}) of the distribution.
\item[\code{inverted}] logical.  If \code{TRUE}, \code{cov} is supposed to
contain the \emph{inverse} of the covariance matrix.
\item[\code{...}] passed to \code{\LinkA{solve}{solve}} for computing the inverse of
the covariance matrix (if \code{inverted} is false).
\end{ldescription}
\end{Arguments}
%
\begin{SeeAlso}\relax
\code{\LinkA{cov}{cov}}, \code{\LinkA{var}{var}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

ma <- cbind(1:6, 1:3)
(S <-  var(ma))
mahalanobis(c(0,0), 1:2, S)

x <- matrix(rnorm(100*3), ncol = 3)
stopifnot(mahalanobis(x, 0, diag(ncol(x))) == rowSums(x*x))
        ##- Here, D^2 = usual squared Euclidean distances

Sx <- cov(x)
D2 <- mahalanobis(x, colMeans(x), Sx)
plot(density(D2, bw=.5),
     main="Squared Mahalanobis distances, n=100, p=3") ; rug(D2)
qqplot(qchisq(ppoints(100), df=3), D2,
       main = expression("Q-Q plot of Mahalanobis" * ~D^2 *
                         " vs. quantiles of" * ~ chi[3]^2))
abline(0, 1, col = 'gray')
\end{ExampleCode}
\end{Examples}
\HeaderA{make.link}{Create a Link for GLM Families}{make.link}
\keyword{models}{make.link}
%
\begin{Description}\relax
This function is used with the \code{\LinkA{family}{family}} functions in
\code{\LinkA{glm}{glm}()}.
Given the name of a link, it returns a link function, an inverse link
function, the derivative \eqn{d\mu / d\eta}{} and a function
for domain checking.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
make.link(link)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{link}] character; one of \code{"logit"},
\code{"probit"}, \code{"cauchit"}, \code{"cloglog"}, \code{"identity"},
\code{"log"},  \code{"sqrt"},  \code{"1/mu\textasciicircum{}2"}, \code{"inverse"}.

\end{ldescription}
\end{Arguments}
%
\begin{Value}
A object of class \code{"link-glm"}, a list with components
\begin{ldescription}
\item[\code{linkfun}] Link function \code{function(mu)}
\item[\code{linkinv}] Inverse link function \code{function(eta)}
\item[\code{mu.eta}] Derivative \code{function(eta)} \eqn{d\mu / d\eta}{}
\item[\code{valideta}] \code{function(eta)}\{ \code{TRUE} if
\code{eta} is in the domain of \code{linkinv} \}.
\item[\code{name}] a name to be used for the link\end{ldescription}
.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{power}{power}}, \code{\LinkA{glm}{glm}}, \code{\LinkA{family}{family}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
utils::str(make.link("logit"))
\end{ExampleCode}
\end{Examples}
\HeaderA{makepredictcall}{Utility Function for Safe Prediction}{makepredictcall}
\methaliasA{makepredictcall.default}{makepredictcall}{makepredictcall.default}
\aliasA{SafePrediction}{makepredictcall}{SafePrediction}
\keyword{models}{makepredictcall}
%
\begin{Description}\relax
A utility to help \code{\LinkA{model.frame.default}{model.frame.default}} create the right
matrices when predicting from models with terms like \code{poly} or
\code{ns}. 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
makepredictcall(var, call)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{var}] A variable.
\item[\code{call}] The term in the formula, as a call.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function with methods for \code{poly}, \code{bs} and
\code{ns}: the default method handles \code{scale}.  If
\code{model.frame.default} encounters such a term when
creating a model frame, it modifies the \code{predvars} attribute of
the terms supplied by replacing the term with one which will work for
predicting new data.  For example \code{makepredictcall.ns} adds
arguments for the knots and intercept.

To make use of this, have your model-fitting function return the
\code{terms} attribute of the model frame, or copy the \code{predvars}
attribute of the \code{terms} attribute of the model frame to your
\code{terms} object.

To extend this, make sure the term creates variables with a class,
and write a suitable method for that class.
\end{Details}
%
\begin{Value}
A replacement for \code{call} for the \code{predvars} attribute of
the terms.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{model.frame}{model.frame}}, \code{\LinkA{poly}{poly}}, \code{\LinkA{scale}{scale}};
\code{\LinkA{bs}{bs}} and \code{\LinkA{ns}{ns}} in package \pkg{splines}.

\code{\LinkA{cars}{cars}} for an example of prediction from a polynomial fit.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## using poly: this did not work in R < 1.5.0
fm <- lm(weight ~ poly(height, 2), data = women)
plot(women, xlab = "Height (in)", ylab = "Weight (lb)")
ht <- seq(57, 73, len = 200)
lines(ht, predict(fm, data.frame(height=ht)))

## see also example(cars)

## see bs and ns for spline examples.
\end{ExampleCode}
\end{Examples}
\HeaderA{manova}{Multivariate Analysis of Variance}{manova}
\keyword{models}{manova}
%
\begin{Description}\relax
A class for the multivariate analysis of variance.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
manova(...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{...}] Arguments to be passed to \code{\LinkA{aov}{aov}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Class \code{"manova"} differs from class \code{"aov"} in selecting a
different \code{summary} method.  Function \code{manova} calls
\code{\LinkA{aov}{aov}} and then add class \code{"manova"} to the result
object for each stratum.
\end{Details}
%
\begin{Value}
See \code{\LinkA{aov}{aov}} and the comments in `Details' here.
\end{Value}
%
\begin{References}\relax
Krzanowski, W. J. (1988) \emph{Principles of Multivariate Analysis. A
User's Perspective.} Oxford.

Hand, D. J. and Taylor, C. C.  (1987)
\emph{Multivariate Analysis of Variance and Repeated Measures.}
Chapman and Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{aov}{aov}}, \code{\LinkA{summary.manova}{summary.manova}}, the latter containing
examples.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## From Venables and Ripley (2002) p.165.
utils::data(npk, package="MASS")

## Set orthogonal contrasts.
op <- options(contrasts=c("contr.helmert", "contr.poly"))

## Fake a 2nd response variable
npk2 <- within(npk, foo <- rnorm(24))
( npk2.aov <- manova(cbind(yield, foo) ~ block + N*P*K, npk2) )
summary(npk2.aov)

( npk2.aovE <- manova(cbind(yield, foo) ~  N*P*K + Error(block), npk2) )
summary(npk2.aovE)


\end{ExampleCode}
\end{Examples}
\HeaderA{mantelhaen.test}{Cochran-Mantel-Haenszel Chi-Squared Test for Count Data}{mantelhaen.test}
\keyword{htest}{mantelhaen.test}
%
\begin{Description}\relax
Performs a Cochran-Mantel-Haenszel chi-squared test of the null that
two nominal variables are conditionally independent in each stratum,
assuming that there is no three-way interaction.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mantelhaen.test(x, y = NULL, z = NULL,
                alternative = c("two.sided", "less", "greater"),
                correct = TRUE, exact = FALSE, conf.level = 0.95)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] either a 3-dimensional contingency table in array form where
each dimension is at least 2 and the last dimension corresponds to
the strata, or a factor object with at least 2 levels.
\item[\code{y}] a factor object with at least 2 levels; ignored if \code{x}
is an array.
\item[\code{z}] a factor object with at least 2 levels identifying to which
stratum the corresponding elements in \code{x} and \code{y} belong;
ignored if \code{x} is an array.
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"}, \code{"greater"} or \code{"less"}.
You can specify just the initial letter.
Only used in the 2 by 2 by \eqn{K}{} case.
\item[\code{correct}] a logical indicating whether to apply continuity
correction when computing the test statistic.
Only used in the 2 by 2 by \eqn{K}{} case.
\item[\code{exact}] a logical indicating whether the Mantel-Haenszel test or
the exact conditional test (given the strata margins) should be
computed.
Only used in the 2 by 2 by \eqn{K}{} case.
\item[\code{conf.level}] confidence level for the returned confidence
interval.
Only used in the 2 by 2 by \eqn{K}{} case.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{x} is an array, each dimension must be at least 2, and
the entries should be nonnegative integers.  \code{NA}'s are not
allowed.  Otherwise, \code{x}, \code{y} and \code{z} must have the
same length.  Triples containing \code{NA}'s are removed.  All
variables must take at least two different values.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] Only present if no exact test is performed.  In the
classical case of a 2 by 2 by \eqn{K}{} table (i.e., of dichotomous
underlying variables), the Mantel-Haenszel chi-squared statistic;
otherwise, the generalized Cochran-Mantel-Haenszel statistic.
\item[\code{parameter}] the degrees of freedom of the approximate chi-squared
distribution of the test statistic (\eqn{1}{} in the classical case).
Only present if no exact test is performed.
\item[\code{p.value}] the p-value of the test.
\item[\code{conf.int}] a confidence interval for the common odds ratio.
Only present in the 2 by 2 by \eqn{K}{} case.
\item[\code{estimate}] an estimate of the common odds ratio.  If an exact
test is performed, the conditional Maximum Likelihood Estimate is
given; otherwise, the Mantel-Haenszel estimate.
Only present in the 2 by 2 by \eqn{K}{} case.
\item[\code{null.value}] the common odds ratio under the null of
independence, \code{1}.
Only present in the 2 by 2 by \eqn{K}{} case.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
Only present in the 2 by 2 by \eqn{K}{} case.
\item[\code{method}] a character string indicating the method employed, and whether
or not continuity correction was used.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The asymptotic distribution is only valid if there is no three-way
interaction.  In the classical 2 by 2 by \eqn{K}{} case, this is
equivalent to the conditional odds ratios in each stratum being
identical.  Currently, no inference on homogeneity of the odds ratios
is performed.

See also the example below.
\end{Note}
%
\begin{References}\relax
Alan Agresti (1990).
\emph{Categorical data analysis}.
New York: Wiley.
Pages 230--235.

Alan Agresti (2002).
\emph{Categorical data analysis} (second edition).
New York: Wiley.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
## Agresti (1990), pages 231--237, Penicillin and Rabbits
## Investigation of the effectiveness of immediately injected or 1.5
##  hours delayed penicillin in protecting rabbits against a lethal
##  injection with beta-hemolytic streptococci.
Rabbits <-
array(c(0, 0, 6, 5,
        3, 0, 3, 6,
        6, 2, 0, 4,
        5, 6, 1, 0,
        2, 5, 0, 0),
      dim = c(2, 2, 5),
      dimnames = list(
          Delay = c("None", "1.5h"),
          Response = c("Cured", "Died"),
          Penicillin.Level = c("1/8", "1/4", "1/2", "1", "4")))
Rabbits
## Classical Mantel-Haenszel test
mantelhaen.test(Rabbits)
## => p = 0.047, some evidence for higher cure rate of immediate
##               injection
## Exact conditional test
mantelhaen.test(Rabbits, exact = TRUE)
## => p - 0.040
## Exact conditional test for one-sided alternative of a higher
## cure rate for immediate injection
mantelhaen.test(Rabbits, exact = TRUE, alternative = "greater")
## => p = 0.020

## UC Berkeley Student Admissions
mantelhaen.test(UCBAdmissions)
## No evidence for association between admission and gender
## when adjusted for department.  However,
apply(UCBAdmissions, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))
## This suggests that the assumption of homogeneous (conditional)
## odds ratios may be violated.  The traditional approach would be
## using the Woolf test for interaction:
woolf <- function(x) {
  x <- x + 1 / 2
  k <- dim(x)[3]
  or <- apply(x, 3, function(x) (x[1,1]*x[2,2])/(x[1,2]*x[2,1]))
  w <-  apply(x, 3, function(x) 1 / sum(1 / x))
  1 - pchisq(sum(w * (log(or) - weighted.mean(log(or), w)) ^ 2), k - 1)
}
woolf(UCBAdmissions)
## => p = 0.003, indicating that there is significant heterogeneity.
## (And hence the Mantel-Haenszel test cannot be used.)

## Agresti (2002), p. 287f and p. 297.
## Job Satisfaction example.
Satisfaction <-
    as.table(array(c(1, 2, 0, 0, 3, 3, 1, 2,
                     11, 17, 8, 4, 2, 3, 5, 2,
                     1, 0, 0, 0, 1, 3, 0, 1,
                     2, 5, 7, 9, 1, 1, 3, 6),
                   dim = c(4, 4, 2),
                   dimnames =
                   list(Income =
                        c("<5000", "5000-15000",
                          "15000-25000", ">25000"),
                        "Job Satisfaction" =
                        c("V_D", "L_S", "M_S", "V_S"),
                        Gender = c("Female", "Male"))))
## (Satisfaction categories abbreviated for convenience.)
ftable(. ~ Gender + Income, Satisfaction)
## Table 7.8 in Agresti (2002), p. 288.
mantelhaen.test(Satisfaction)
## See Table 7.12 in Agresti (2002), p. 297.
\end{ExampleCode}
\end{Examples}
\HeaderA{mauchly.test}{Mauchly's Test of Sphericity}{mauchly.test}
\methaliasA{mauchly.test.mlm}{mauchly.test}{mauchly.test.mlm}
\methaliasA{mauchly.test.SSD}{mauchly.test}{mauchly.test.SSD}
\keyword{htest}{mauchly.test}
\keyword{models}{mauchly.test}
\keyword{multivariate}{mauchly.test}
%
\begin{Description}\relax
Tests whether a Wishart-distributed covariance matrix (or
transformation thereof) is proportional to a given matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mauchly.test(object, ...)
## S3 method for class 'mlm'
mauchly.test(object,...)
## S3 method for class 'SSD'
mauchly.test(object, Sigma = diag(nrow = p),
   T = Thin.row(proj(M) - proj(X)), M = diag(nrow = p), X = ~0,
   idata = data.frame(index = seq_len(p)), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] object of class \code{SSD} or \code{mlm}.
\item[\code{Sigma}] matrix to be proportional to.
\item[\code{T}] transformation matrix. By default computed from \code{M} and
\code{X}.
\item[\code{M}] formula or matrix describing the outer projection (see below).
\item[\code{X}] formula or matrix describing the inner projection (see below).
\item[\code{idata}] data frame describing intra-block design.
\item[\code{...}] arguments to be passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Mauchly's test test for whether a covariance matrix can be assumed to
be proportional to a given matrix.

This is a generic function with methods for classes \code{"mlm"} and
\code{"\LinkA{SSD}{SSD}"}.

The basic method is for objects of
class \code{SSD} the method for \code{mlm} objects just extracts the
SSD matrix and invokes the corresponding method with the same options
and arguments.   

The \code{T} argument is used to transform the observations prior to
testing. This typically involves transformation to intra-block
differences, but more complicated within-block designs can be
encountered, making more elaborate transformations necessary. A
matrix \code{T} can be given directly or specified as
the difference between two projections onto the spaces spanned by
\code{M} and \code{X}, which in turn can be given as matrices or as
model formulas with respect to \code{idata} (the tests will be
invariant to parametrization of the quotient space \code{M/X}).

The common use of this test is in repeated measurements designs, with
\code{X=\textasciitilde{}1}. This is almost, but not quite the same as testing for
compound symmetry in the untransformed covariance matrix.

Notice that the defaults involve \code{p}, which is calculated
internally as the dimension of the SSD matrix, and a couple of hidden
functions in the \pkg{stats} namespace, namely \code{proj} which
calculates projection matrices from design matrices or model formulas
and \code{Thin.row} which removes linearly dependent rows from a
matrix until it has full row rank. 

\end{Details}
%
\begin{Value}
An object of class \code{"htest"}
\end{Value}
%
\begin{Note}\relax
The p-value differs slightly from that of SAS because a second order term
is included in the asymptotic approximation in \R{}.
\end{Note}
%
\begin{References}\relax
T. W. Anderson (1958).  \emph{An Introduction to Multivariate
Statistical Analysis.} Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{SSD}{SSD}}, \code{\LinkA{anova.mlm}{anova.mlm}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
utils::example(SSD) # Brings in the mlmfit and reacttime objects

### traditional test of intrasubj. contrasts
mauchly.test(mlmfit, X=~1) 

### tests using intra-subject 3x2 design
idata <- data.frame(deg=gl(3,1,6, labels=c(0,4,8)),
                    noise=gl(2,3,6, labels=c("A","P")))
mauchly.test(mlmfit, X = ~ deg + noise, idata = idata)
mauchly.test(mlmfit, M = ~ deg + noise, X = ~ noise, idata=idata)
\end{ExampleCode}
\end{Examples}
\HeaderA{mcnemar.test}{McNemar's Chi-squared Test for Count Data}{mcnemar.test}
\keyword{htest}{mcnemar.test}
%
\begin{Description}\relax
Performs McNemar's chi-squared test for symmetry of rows and columns
in a two-dimensional contingency table.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mcnemar.test(x, y = NULL, correct = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] either a two-dimensional contingency table in matrix form,
or a factor object.
\item[\code{y}] a factor object; ignored if \code{x} is a matrix.
\item[\code{correct}] a logical indicating whether to apply continuity
correction when computing the test statistic.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The null is that the probabilities of being classified into cells
\code{[i,j]} and \code{[j,i]} are the same.

If \code{x} is a matrix, it is taken as a two-dimensional contingency
table, and hence its entries should be nonnegative integers.
Otherwise, both \code{x} and \code{y} must be vectors or factors of the
same length.  Incomplete cases are removed, vectors are coerced into
factors, and the contingency table is computed from these.

Continuity correction is only used in the 2-by-2 case if
\code{correct} is \code{TRUE}.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of McNemar's statistic.
\item[\code{parameter}] the degrees of freedom of the approximate
chi-squared distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] a character string indicating the type of test
performed, and whether continuity correction was used.
\item[\code{data.name}] a character string giving the name(s) of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Alan Agresti (1990).
\emph{Categorical data analysis}.
New York: Wiley.
Pages 350--354.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
## Agresti (1990), p. 350.
## Presidential Approval Ratings.
##  Approval of the President's performance in office in two surveys,
##  one month apart, for a random sample of 1600 voting-age Americans.
Performance <-
matrix(c(794, 86, 150, 570),
       nrow = 2,
       dimnames = list("1st Survey" = c("Approve", "Disapprove"),
                       "2nd Survey" = c("Approve", "Disapprove")))
Performance
mcnemar.test(Performance)
## => significant change (in fact, drop) in approval ratings
\end{ExampleCode}
\end{Examples}
\HeaderA{median}{Median Value}{median}
\methaliasA{median.default}{median}{median.default}
\keyword{univar}{median}
\keyword{robust}{median}
%
\begin{Description}\relax
Compute the sample median.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
median(x, na.rm = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object for which a method has been defined, or a
numeric vector containing the values whose median is to be computed.
\item[\code{na.rm}] a logical value indicating whether \code{NA}
values should be stripped before the computation proceeds.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function for which methods can be written.  However,
the default method makes use of \code{sort} and \code{mean} from
package \pkg{base} both of which are generic, and so the default
method will work for most classes (e.g. \code{"\LinkA{Date}{Date}"}) for
which a median is a reasonable concept. 
\end{Details}
%
\begin{Value}
The default method returns a length-one object of the same type as
\code{x}, except when \code{x} is integer of even length, when the
result will be double.

If there are no values or if \code{na.rm = FALSE} and there are \code{NA}
values the result is \code{NA} of the same type as \code{x} (or more
generally the result of \code{x[FALSE][NA]}).
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{quantile}{quantile}} for general quantiles.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
median(1:4)# = 2.5 [even number]
median(c(1:3,100,1000))# = 3 [odd, robust]
\end{ExampleCode}
\end{Examples}
\HeaderA{medpolish}{Median Polish of a Matrix}{medpolish}
\keyword{robust}{medpolish}
%
\begin{Description}\relax
Fits an additive model using Tukey's \emph{median polish} procedure.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
medpolish(x, eps = 0.01, maxiter = 10, trace.iter = TRUE,
          na.rm = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric matrix.
\item[\code{eps}] real number greater than 0. A tolerance for convergence:
see `Details'.
\item[\code{maxiter}] the maximum number of iterations
\item[\code{trace.iter}] logical. Should progress in convergence be reported?
\item[\code{na.rm}] logical. Should missing values be removed?
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The model fitted is additive (constant + rows + columns). The
algorithm works by alternately removing the row and column medians,
and continues until the proportional reduction in the sum
of absolute residuals is less than \code{eps}
or until there have been \code{maxiter} iterations.
The sum of absolute residuals is printed at
each iteration of the fitting process, if \code{trace.iter} is \code{TRUE}.
If \code{na.rm} is \code{FALSE} the presence of any \code{NA} value in
\code{x} will cause an error, otherwise \code{NA} values are ignored.

\code{medpolish} returns an object of class \code{medpolish} (see below).
There are printing and plotting methods for this
class, which are invoked via by the generics
\code{\LinkA{print}{print}} and \code{\LinkA{plot}{plot}}.
\end{Details}
%
\begin{Value}
An object of class \code{medpolish} with the following named components:
\begin{ldescription}
\item[\code{overall}] the fitted constant term.
\item[\code{row}] the fitted row effects.
\item[\code{col}] the fitted column effects.
\item[\code{residuals}] the residuals.
\item[\code{name}] the name of the dataset.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Tukey, J. W. (1977).
\emph{Exploratory Data Analysis},
Reading Massachusetts: Addison-Wesley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{median}{median}}; \code{\LinkA{aov}{aov}} for a \emph{mean}
instead of \emph{median} decomposition.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Deaths from sport parachuting;  from ABC of EDA, p.224:
deaths <-
    rbind(c(14,15,14),
          c( 7, 4, 7),
          c( 8, 2,10),
          c(15, 9,10),
          c( 0, 2, 0))
dimnames(deaths) <- list(c("1-24", "25-74", "75-199", "200++", "NA"),
                         paste(1973:1975))
deaths
(med.d <- medpolish(deaths))
plot(med.d)
## Check decomposition:
all(deaths ==
    med.d$overall + outer(med.d$row,med.d$col, "+") + med.d$residuals)
\end{ExampleCode}
\end{Examples}
\HeaderA{model.extract}{Extract Components from a Model Frame}{model.extract}
\aliasA{model.offset}{model.extract}{model.offset}
\aliasA{model.response}{model.extract}{model.response}
\aliasA{model.weights}{model.extract}{model.weights}
\keyword{manip}{model.extract}
\keyword{programming}{model.extract}
\keyword{models}{model.extract}
%
\begin{Description}\relax
Returns the response, offset, subset, weights or other
special components of a model frame passed as optional arguments to
\code{\LinkA{model.frame}{model.frame}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
model.extract(frame, component)
model.offset(x)
model.response(data, type = "any")
model.weights(x)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{frame, x, data}] A model frame.
\item[\code{component}] literal character string or name. The name of a
component to extract, such as \code{"weights"}, \code{"subset"}.
\item[\code{type}] One of \code{"any"}, \code{"numeric"}, \code{"double"}.
Using either of latter two coerces the result to have storage mode
\code{"double"}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{model.extract} is provided for compatibility with S, which does
not have the more specific functions.  It is also useful to extract
e.g. the \code{etastart} and \code{mustart} components of a
\code{\LinkA{glm}{glm}} fit.

\code{model.offset} and \code{model.response} are equivalent to
\code{model.extract(, "offset")} and \code{model.extract(, "response")}
respectively.  \code{model.offset} sums any terms specified by
\code{\LinkA{offset}{offset}} terms in the formula or by \code{offset} arguments
in the call producing the model frame: it does check that the offset
is numeric.

\code{model.weights} is slightly different from
\code{model.frame(, "weights")} in not naming the vector it returns.
\end{Details}
%
\begin{Value}
The specified component of the model frame, usually a vector.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{model.frame}{model.frame}}, \code{\LinkA{offset}{offset}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
a <- model.frame(cbind(ncases,ncontrols) ~ agegp+tobgp+alcgp, data=esoph)
model.extract(a, "response")
stopifnot(model.extract(a, "response") == model.response(a))

a <- model.frame(ncases/(ncases+ncontrols) ~ agegp+tobgp+alcgp,
                 data = esoph, weights = ncases+ncontrols)
model.response(a)
model.extract(a, "weights")

a <- model.frame(cbind(ncases,ncontrols) ~ agegp,
                 something = tobgp, data = esoph)
names(a)
stopifnot(model.extract(a, "something") == esoph$tobgp)
\end{ExampleCode}
\end{Examples}
\HeaderA{model.frame}{Extracting the Model Frame from a Formula or Fit}{model.frame}
\aliasA{get\_all\_vars}{model.frame}{get.Rul.all.Rul.vars}
\methaliasA{model.frame.aovlist}{model.frame}{model.frame.aovlist}
\methaliasA{model.frame.default}{model.frame}{model.frame.default}
\methaliasA{model.frame.glm}{model.frame}{model.frame.glm}
\methaliasA{model.frame.lm}{model.frame}{model.frame.lm}
\keyword{models}{model.frame}
%
\begin{Description}\relax
\code{model.frame} (a generic function) and its methods return a
\code{\LinkA{data.frame}{data.frame}} with the variables needed to use
\code{formula} and any \code{...} arguments.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
model.frame(formula, ...)

## Default S3 method:
model.frame(formula, data = NULL,
           subset = NULL, na.action = na.fail,
           drop.unused.levels = FALSE, xlev = NULL, ...)

## S3 method for class 'aovlist'
model.frame(formula, data = NULL, ...)

## S3 method for class 'glm'
model.frame(formula, ...)

## S3 method for class 'lm'
model.frame(formula, ...)

get_all_vars(formula, data, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a model \code{\LinkA{formula}{formula}} or \code{\LinkA{terms}{terms}}
object or an \R{} object.

\item[\code{data}] a data.frame, list or environment (or object
coercible by \code{\LinkA{as.data.frame}{as.data.frame}} to a data.frame),
containing the variables in \code{formula}.  Neither a matrix nor an
array will be accepted.

\item[\code{subset}] a specification of the rows to be used: defaults to all
rows. This can be any valid indexing vector (see
\code{\LinkA{[.data.frame}{[.data.frame}}) for the rows of \code{data} or if that is not
supplied, a data frame made up of the variables used in \code{formula}.

\item[\code{na.action}] how \code{NA}s are treated.  The default is first,
any \code{na.action} attribute of \code{data}, second
a \code{na.action} setting of \code{\LinkA{options}{options}}, and third
\code{\LinkA{na.fail}{na.fail}} if that is unset.  The `factory-fresh'
default is \code{\LinkA{na.omit}{na.omit}}.  Another possible value is \code{NULL}.

\item[\code{drop.unused.levels}] should factors have unused levels dropped?
Defaults to \code{FALSE}.

\item[\code{xlev}] a named list of character vectors giving the full set of levels
to be assumed for each factor.

\item[\code{...}] further arguments such as \code{data}, \code{na.action},
\code{subset}.  Any additional arguments such as \code{offset} and
\code{weights} which reach the default method are used to create
further columns in the model frame, with parenthesised names such as
\code{"(offset)"}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Exactly what happens depends on the class and attributes of the object
\code{formula}.  If this is an object of fitted-model class such as
\code{"lm"}, the method will either return the saved model frame
used when fitting the model (if any, often selected by argument
\code{model = TRUE}) or pass the call used when fitting on to the
default method.  The default method itself can cope with rather
standard model objects such as those of class
\code{"\LinkA{lqs}{lqs}"} from package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} if no other
arguments are supplied.

The rest of this section applies only to the default method.

If either \code{formula} or \code{data} is already a model frame (a
data frame with a \code{"terms"} attribute) and the other is missing,
the model frame is returned.  Unless \code{formula} is a terms object,
\code{as.formula} and then \code{terms} is called on it.  (If you wish
to use the \code{keep.order} argument of \code{terms.formula}, pass a
terms object rather than a formula.)

Row names for the model frame are taken from the \code{data} argument
if present, then from the names of the response in the formula (or
rownames if it is a matrix), if there is one.

All the variables in \code{formula}, \code{subset} and in \code{...}
are looked for first in \code{data} and then in the environment of
\code{formula} (see the help for \code{\LinkA{formula}{formula}()} for further
details) and collected into a data frame.  Then the \code{subset}
expression is evaluated, and it is used as a row index to the data
frame.  Then the \code{na.action} function is applied to the data frame
(and may well add attributes).  The levels of any factors in the data
frame are adjusted according to the \code{drop.unused.levels} and
\code{xlev} arguments: if \code{xlev} specifies a factor and a
character variable is found, it is converted to a factor (as from \R{}
2.10.0).

Unless \code{na.action = NULL}, time-series attributes will be removed
from the variables found (since they will be wrong if \code{NA}s are
removed).

Note that \emph{all} the variables in the formula are included in the
data frame, even those preceded by \code{-}.

Only variables whose type is raw, logical, integer, real, complex or
character can be included in a model frame: this includes classed
variables such as factors (whose underlying type is integer), but
excludes lists.

\code{get\_all\_vars} returns a \code{\LinkA{data.frame}{data.frame}} containing the
variables used in \code{formula} plus those specified \code{...}.
Unlike \code{model.frame.default}, it returns the input variables and
not those resulting from function calls in \code{formula}.
\end{Details}
%
\begin{Value}
A \code{\LinkA{data.frame}{data.frame}} containing the variables used in
\code{formula} plus those specified in \code{...}.  It will have
additional attributes, including \code{"terms"} for an object of class
\code{"\LinkA{terms}{terms.object}"} derived from \code{formula},
and possibly \code{"na.action"} giving information on the handling of
\code{NA}s (which will not be present if no special handling was done,
e.g. by \code{\LinkA{na.pass}{na.pass}}).
\end{Value}
%
\begin{References}\relax
Chambers, J. M. (1992)
\emph{Data for models.}
Chapter 3 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{model.matrix}{model.matrix}} for the `design matrix',
\code{\LinkA{formula}{formula}} for formulas  and
\code{\LinkA{expand.model.frame}{expand.model.frame}} for model.frame manipulation.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
data.class(model.frame(dist ~ speed, data = cars))
\end{ExampleCode}
\end{Examples}
\HeaderA{model.matrix}{Construct Design Matrices}{model.matrix}
\methaliasA{model.matrix.default}{model.matrix}{model.matrix.default}
\methaliasA{model.matrix.lm}{model.matrix}{model.matrix.lm}
\keyword{models}{model.matrix}
%
\begin{Description}\relax
\code{model.matrix} creates a design (or model) matrix.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
model.matrix(object, ...)

## Default S3 method:
model.matrix(object, data = environment(object),
             contrasts.arg = NULL, xlev = NULL, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object of an appropriate class.  For the default
method, a model \LinkA{formula}{formula} or a \code{\LinkA{terms}{terms}} object.
\item[\code{data}] a data frame created with \code{\LinkA{model.frame}{model.frame}}.  If
another sort of object, \code{model.frame} is called first.
\item[\code{contrasts.arg}] A list, whose entries are values (numeric
matrices or character strings naming functions) to be used
as replacement values for the \code{\LinkA{contrasts}{contrasts}}
replacement function and whose names are the names of
columns of \code{data} containing \code{\LinkA{factor}{factor}}s.  
\item[\code{xlev}] to be used as argument of \code{\LinkA{model.frame}{model.frame}} if
\code{data} is such that \code{model.frame} is called.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{model.matrix} creates a design matrix from the description given
in \code{terms(object)}, using the data in \code{data} which must
supply variables with the same names as would be created by a call to
\code{model.frame(object)} or, more precisely, by evaluating
\code{attr(terms(object), "variables")}.  If \code{data} is a data
frame, there may be other columns and the order of columns is not
important.  Any character variables are coerced to factors, with a
warning.  After coercion, all the variables used on the right-hand
side of the formula must be logical, integer, numeric or factor.

If \code{contrasts.arg} is specified for a factor it overrides the
default factor coding for that variable and any \code{"contrasts"}
attribute set by \code{\LinkA{C}{C}} or \code{\LinkA{contrasts}{contrasts}}.

In an interaction term, the variable whose levels vary fastest is the
first one to appear in the formula (and not in the term), so in
\code{\textasciitilde{} a + b + b:a} the interaction will have \code{a} varying
fastest.

By convention, if the response variable also appears on the
right-hand side of the formula it is dropped (with a warning),
although interactions involving the term are retained.
\end{Details}
%
\begin{Value}
The design matrix for a regression-like model with the specified formula
and data.

There is an attribute \code{"assign"}, an integer vector with an entry
for each column in the matrix giving the term in the formula which
gave rise to the column.  Value \code{0} corresponds to the intercept
(if any), and positive values to terms in the order given by the
\code{term.labels} attribute of the \code{terms} structure
corresponding to \code{object}.

If there are any factors in terms in the model, there is an attribute
\code{"contrasts"}, a named list with an entry for each factor.  This
specifies the contrasts that would be used in terms in which the
factor is coded by contrasts (in some terms dummy coding may be used),
either as a character vector naming a function or as a numeric matrix.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. (1992)
\emph{Data for models.}
Chapter 3 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{model.frame}{model.frame}}, \code{\LinkA{model.extract}{model.extract}},
\code{\LinkA{terms}{terms}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ff <- log(Volume) ~ log(Height) + log(Girth)
utils::str(m <- model.frame(ff, trees))
mat <- model.matrix(ff, m)

dd <- data.frame(a = gl(3,4), b = gl(4,1,12)) # balanced 2-way
options("contrasts")
model.matrix(~ a + b, dd)
model.matrix(~ a + b, dd, contrasts = list(a="contr.sum"))
model.matrix(~ a + b, dd, contrasts = list(a="contr.sum", b="contr.poly"))
m.orth <- model.matrix(~a+b, dd, contrasts = list(a="contr.helmert"))
crossprod(m.orth) # m.orth is  ALMOST  orthogonal
\end{ExampleCode}
\end{Examples}
\HeaderA{model.tables}{Compute Tables of Results from an Aov Model Fit}{model.tables}
\methaliasA{model.tables.aov}{model.tables}{model.tables.aov}
\methaliasA{model.tables.aovlist}{model.tables}{model.tables.aovlist}
\keyword{models}{model.tables}
%
\begin{Description}\relax
Computes summary tables for model fits, especially complex \code{aov}
fits.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
model.tables(x, ...)

## S3 method for class 'aov'
model.tables(x, type = "effects", se = FALSE, cterms, ...)

## S3 method for class 'aovlist'
model.tables(x, type = "effects", se = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a model object, usually produced by \code{aov}
\item[\code{type}] type of table: currently only \code{"effects"} and
\code{"means"} are implemented.
\item[\code{se}] should standard errors be computed?
\item[\code{cterms}] A character vector giving the names of the terms for
which tables should be computed. The default is all tables.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
For \code{type = "effects"} give tables of the coefficients for each
term, optionally with standard errors.

For \code{type = "means"} give tables of the mean response for each
combinations of levels of the factors in a term.

The \code{"aov"} method cannot be applied to components of a
\code{"aovlist"} fit.
\end{Details}
%
\begin{Value}
An object of class \code{"tables.aov"}, as list which may contain components
\begin{ldescription}
\item[\code{tables}] A list of tables for each requested term.
\item[\code{n}] The replication information for each term.
\item[\code{se}] Standard error information.
\end{ldescription}
\end{Value}
%
\begin{Section}{Warning}
The implementation is incomplete, and only the simpler cases have been
tested thoroughly.

Weighted \code{aov} fits are not supported.
\end{Section}
%
\begin{SeeAlso}\relax
\code{\LinkA{aov}{aov}}, \code{\LinkA{proj}{proj}},
\code{\LinkA{replications}{replications}}, \code{\LinkA{TukeyHSD}{TukeyHSD}},
\code{\LinkA{se.contrast}{se.contrast}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

## From Venables and Ripley (2002) p.165.
utils::data(npk, package="MASS")

options(contrasts=c("contr.helmert", "contr.treatment"))
npk.aov <- aov(yield ~ block + N*P*K, npk)
model.tables(npk.aov, "means", se = TRUE)

## as a test, not particularly sensible statistically
npk.aovE <- aov(yield ~  N*P*K + Error(block), npk)
model.tables(npk.aovE, se=TRUE)
model.tables(npk.aovE, "means")
\end{ExampleCode}
\end{Examples}
\HeaderA{monthplot}{Plot a Seasonal or other Subseries from a Time Series}{monthplot}
\methaliasA{monthplot.default}{monthplot}{monthplot.default}
\methaliasA{monthplot.stl}{monthplot}{monthplot.stl}
\methaliasA{monthplot.StructTS}{monthplot}{monthplot.StructTS}
\methaliasA{monthplot.ts}{monthplot}{monthplot.ts}
\keyword{hplot}{monthplot}
\keyword{ts}{monthplot}
%
\begin{Description}\relax
These functions plot seasonal (or other) subseries of a time series.
For each season (or other category), a time series is plotted.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
monthplot(x, ...)

## S3 method for class 'stl'
monthplot(x, labels = NULL, ylab = choice, choice = "seasonal",
          ...)

## S3 method for class 'StructTS'
monthplot(x, labels = NULL, ylab = choice, choice = "sea", ...)

## S3 method for class 'ts'
monthplot(x, labels = NULL, times = time(x), phase = cycle(x),
             ylab = deparse(substitute(x)), ...)

## Default S3 method:
monthplot(x, labels = 1L:12L,
          ylab = deparse(substitute(x)),
          times = seq_along(x),
          phase = (times - 1L)%%length(labels) + 1L, base = mean,
          axes = TRUE, type = c("l", "h"), box = TRUE,
          add = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] Time series or related object.
\item[\code{labels}] Labels to use for each `season'.
\item[\code{ylab}] y label.
\item[\code{times}] Time of each observation.
\item[\code{phase}] Indicator for each `season'.
\item[\code{base}] Function to use for reference line for subseries.
\item[\code{choice}] Which series of an \code{stl} or \code{StructTS} object?
\item[\code{...}] Arguments to be passed to the default method or
graphical parameters.
\item[\code{axes}] Should axes be drawn (ignored if \code{add=TRUE})?
\item[\code{type}] Type of plot.  The default is to join the points with
lines, and \code{"h"} is for histogram-like vertical lines.
\item[\code{box}] Should a box be drawn (ignored if \code{add=TRUE}?
\item[\code{add}] Should thus just add on an existing plot.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These functions extract subseries from a time series and plot them
all in one frame.  The \code{\LinkA{ts}{ts}}, \code{\LinkA{stl}{stl}}, and
\code{\LinkA{StructTS}{StructTS}} methods use the internally recorded frequency and
start and finish times to set the scale and the seasons.  The default
method assumes observations come in groups of 12 (though this can be
changed).

If the \code{labels} are not given but the \code{phase} is given, then
the \code{labels} default to the unique values of the \code{phase}.  If
both are given, then the \code{phase} values are assumed to be indices
into the \code{labels} array, i.e., they should be in the range
from 1 to \code{length(labels)}.
\end{Details}
%
\begin{Value}
These functions are executed for their side effect of
drawing a seasonal subseries plot on the current graphical
window.
\end{Value}
%
\begin{Author}\relax
Duncan Murdoch
\end{Author}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ts}{ts}}, \code{\LinkA{stl}{stl}}, \code{\LinkA{StructTS}{StructTS}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## The CO2 data
fit <- stl(log(co2), s.window = 20, t.window = 20)
plot(fit)
op <- par(mfrow = c(2,2))
monthplot(co2, ylab = "data", cex.axis = 0.8)
monthplot(fit, choice = "seasonal", cex.axis = 0.8)
monthplot(fit, choice = "trend", cex.axis = 0.8)
monthplot(fit, choice = "remainder", type = "h", cex.axis = 0.8)
par(op)

## The CO2 data, grouped quarterly
quarter <- (cycle(co2) - 1) %/% 3
monthplot(co2, phase = quarter)

## see also JohnsonJohnson
\end{ExampleCode}
\end{Examples}
\HeaderA{mood.test}{Mood Two-Sample Test of Scale}{mood.test}
\methaliasA{mood.test.default}{mood.test}{mood.test.default}
\methaliasA{mood.test.formula}{mood.test}{mood.test.formula}
\keyword{htest}{mood.test}
%
\begin{Description}\relax
Performs Mood's two-sample test for a difference in scale parameters.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
mood.test(x, ...)

## Default S3 method:
mood.test(x, y,
          alternative = c("two.sided", "less", "greater"), ...)

## S3 method for class 'formula'
mood.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] numeric vectors of data values.
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"} (default), \code{"greater"} or
\code{"less"} all of which can be abbreviated.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
is a numeric variable giving the data values and \code{rhs} a factor
with two levels giving the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods. 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The underlying model is that the two samples are drawn from
\eqn{f(x-l)}{} and \eqn{f((x-l)/s)/s}{}, respectively, where \eqn{l}{} is a
common location parameter and \eqn{s}{} is a scale parameter.

The null hypothesis is \eqn{s = 1}{}.

There are more useful tests for this problem.

In the case of ties, the formulation of Mielke (1967) is employed.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] the character string \code{"Mood two-sample test of scale"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
William J. Conover (1971),
\emph{Practical nonparametric statistics}.
New York: John Wiley \& Sons.
Pages 234f.

Paul W. Mielke, Jr. (1967),
Note on some squared rank tests with existing ties.
\emph{Technometrics}, \bold{9}/2, 312--314.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{fligner.test}{fligner.test}} for a rank-based (nonparametric) k-sample
test for homogeneity of variances;
\code{\LinkA{ansari.test}{ansari.test}} for another rank-based two-sample test for a
difference in scale parameters;
\code{\LinkA{var.test}{var.test}} and \code{\LinkA{bartlett.test}{bartlett.test}} for parametric
tests for the homogeneity in variance.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Same data as for the Ansari-Bradley test:
## Serum iron determination using Hyland control sera
ramsay <- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,
            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)
jung.parekh <- c(107, 108, 106, 98, 105, 103, 110, 105, 104,
            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)
mood.test(ramsay, jung.parekh)
## Compare this to ansari.test(ramsay, jung.parekh)
\end{ExampleCode}
\end{Examples}
\HeaderA{Multinom}{The Multinomial Distribution}{Multinom}
\aliasA{dmultinom}{Multinom}{dmultinom}
\aliasA{Multinomial}{Multinom}{Multinomial}
\aliasA{rmultinom}{Multinom}{rmultinom}
\keyword{distribution}{Multinom}
%
\begin{Description}\relax
Generate multinomially distributed random number vectors and
compute multinomial probabilities.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
rmultinom(n, size, prob)
dmultinom(x, size = NULL, prob, log = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] vector of length \eqn{K}{} of integers in \code{0:size}.

\item[\code{n}] number of random vectors to draw.
\item[\code{size}] integer, say \eqn{N}{}, specifying the total number
of objects that are put into \eqn{K}{} boxes in the typical multinomial
experiment. For \code{dmultinom}, it defaults to \code{sum(x)}.
\item[\code{prob}] numeric non-negative vector of length \eqn{K}{}, specifying
the probability for the \eqn{K}{} classes; is internally normalized to
sum 1.
\item[\code{log}] logical; if TRUE, log probabilities are computed.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{x} is a \$K\$-component vector, \code{dmultinom(x, prob)} is
the probability
\deqn{P(X_1=x_1,\ldots,X_K=x_k) = C \times \prod_{j=1}^K
    \pi_j^{x_j}}{}
where \eqn{C}{} is the `multinomial coefficient'
\eqn{C = N! / (x_1! \cdots x_K!)}{}
and \eqn{N = \sum_{j=1}^K x_j}{}.
\\{}
By definition, each component \eqn{X_j}{} is binomially distributed as
\code{Bin(size, prob[j])} for \eqn{j = 1, \ldots, K}{}.

The \code{rmultinom()} algorithm draws binomials \eqn{X_j}{} from
\eqn{Bin(n_j,P_j)}{} sequentially, where
\eqn{n_1 = N}{} (N := \code{size}),
\eqn{P_1 = \pi_1}{} (\eqn{\pi}{} is \code{prob} scaled to sum 1),
and for \eqn{j \ge 2}{}, recursively,
\eqn{n_j = N - \sum_{k=1}^{j-1} X_k}{}
and
\eqn{P_j = \pi_j / (1 - \sum_{k=1}^{j-1} \pi_k)}{}.
\end{Details}
%
\begin{Value}
For \code{rmultinom()},
an integer \code{K x n} matrix where each column is a random vector
generated according to the desired multinomial law, and hence summing
to \code{size}.  Whereas the \emph{transposed} result would seem more
natural at first, the returned matrix is more efficient because of
columnwise storage.
\end{Value}
%
\begin{Note}\relax
\code{dmultinom} is currently \emph{not vectorized} at all and has
no C interface (API); this may be amended in the future.
\end{Note}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for standard distributions, including
\code{\LinkA{dbinom}{dbinom}} which is a special case conceptually.

\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
rmultinom(10, size = 12, prob=c(0.1,0.2,0.8))

pr <- c(1,3,6,10) # normalization not necessary for generation
rmultinom(10, 20, prob = pr)

## all possible outcomes of Multinom(N = 3, K = 3)
X <- t(as.matrix(expand.grid(0:3, 0:3))); X <- X[, colSums(X) <= 3]
X <- rbind(X, 3:3 - colSums(X)); dimnames(X) <- list(letters[1:3], NULL)
X
round(apply(X, 2, function(x) dmultinom(x, prob = c(1,2,5))), 3)
\end{ExampleCode}
\end{Examples}
\HeaderA{na.action}{NA Action}{na.action}
\methaliasA{na.action.default}{na.action}{na.action.default}
\keyword{NA}{na.action}
\keyword{methods}{na.action}
%
\begin{Description}\relax
Extract information on the NA action used to create an object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
na.action(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] any object whose \code{\LinkA{NA}{NA}} action is given.
\item[\code{...}] further arguments special methods could require.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{na.action} is a generic function, and \code{na.action.default} its
default method.  The latter extracts the \code{"na.action"} component
of a list if present, otherwise the \code{"na.action"} attribute.

When \code{\LinkA{model.frame}{model.frame}} is called, it records any information
on \code{NA} handling in a \code{"na.action"} attribute.  Most
model-fitting functions return this as a component of their result.
\end{Details}
%
\begin{Value}
Information from the action which was applied to \code{object} if
\code{NA}s were handled specially, or \code{NULL}.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S.}
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{options}{options}("na.action")}, \code{\LinkA{na.omit}{na.omit}},
\code{\LinkA{na.fail}{na.fail}}, also for \code{na.exclude}, \code{na.pass}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
na.action(na.omit(c(1, NA)))
\end{ExampleCode}
\end{Examples}
\HeaderA{na.contiguous}{Find Longest Contiguous Stretch of non-NAs}{na.contiguous}
\methaliasA{na.contiguous.default}{na.contiguous}{na.contiguous.default}
\keyword{ts}{na.contiguous}
%
\begin{Description}\relax
Find the longest consecutive stretch of non-missing values in a time
series object.  (In the event of a tie, the first such stretch.)
\end{Description}
%
\begin{Usage}
\begin{verbatim}
na.contiguous(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a univariate or multivariate time series.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A time series without missing values.  The class of \code{object} will
be preserved.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{na.omit}{na.omit}} and \code{\LinkA{na.omit.ts}{na.omit.ts}};
\code{\LinkA{na.fail}{na.fail}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
na.contiguous(presidents)
\end{ExampleCode}
\end{Examples}
\HeaderA{na.fail}{Handle Missing Values in Objects}{na.fail}
\aliasA{na.exclude}{na.fail}{na.exclude}
\methaliasA{na.exclude.data.frame}{na.fail}{na.exclude.data.frame}
\methaliasA{na.exclude.default}{na.fail}{na.exclude.default}
\methaliasA{na.fail.default}{na.fail}{na.fail.default}
\aliasA{na.omit}{na.fail}{na.omit}
\methaliasA{na.omit.data.frame}{na.fail}{na.omit.data.frame}
\methaliasA{na.omit.default}{na.fail}{na.omit.default}
\aliasA{na.pass}{na.fail}{na.pass}
\keyword{NA}{na.fail}
%
\begin{Description}\relax
These generic functions are useful for dealing with \code{\LinkA{NA}{NA}}s
in e.g., data frames.
\code{na.fail} returns the object if it does not contain any
missing values, and signals an error otherwise.
\code{na.omit} returns the object with incomplete cases removed.
\code{na.pass} returns the object unchanged.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
na.fail(object, ...)
na.omit(object, ...)
na.exclude(object, ...)
na.pass(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an \R{} object, typically a data frame
\item[\code{...}] further arguments special methods could require.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
At present these will handle vectors, matrices and data frames
comprising vectors and matrices (only).

If \code{na.omit} removes cases, the row numbers of the cases form the
\code{"na.action"} attribute of the result, of class \code{"omit"}.

\code{na.exclude} differs from \code{na.omit} only in the class of the
\code{"na.action"} attribute of the result, which is
\code{"exclude"}.  This gives different behaviour in functions making
use of \code{\LinkA{naresid}{naresid}} and \code{\LinkA{napredict}{napredict}}: when
\code{na.exclude} is used the residuals and predictions are padded to
the correct length by inserting \code{NA}s for cases omitted by
\code{na.exclude}.
\end{Details}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S.}
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{na.action}{na.action}};
\code{\LinkA{options}{options}} with argument \code{na.action} for setting NA actions;
and \code{\LinkA{lm}{lm}} and \code{\LinkA{glm}{glm}} for functions using these.
\code{\LinkA{na.contiguous}{na.contiguous}} as alternative for time series.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
DF <- data.frame(x = c(1, 2, 3), y = c(0, 10, NA))
na.omit(DF)
m <- as.matrix(DF)
na.omit(m)
stopifnot(all(na.omit(1:3) == 1:3))  # does not affect objects with no NA's
try(na.fail(DF))#> Error: missing values in ...

options("na.action")
\end{ExampleCode}
\end{Examples}
\HeaderA{naprint}{Adjust for Missing Values}{naprint}
\methaliasA{naprint.default}{naprint}{naprint.default}
\methaliasA{naprint.exclude}{naprint}{naprint.exclude}
\methaliasA{naprint.omit}{naprint}{naprint.omit}
\keyword{NA}{naprint}
\keyword{models}{naprint}
%
\begin{Description}\relax
Use missing value information to report the effects of an \code{na.action}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
naprint(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] 
An object produced by an \code{na.action} function.

\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function, and the exact information differs by
method. \code{naprint.omit} reports the number of rows omitted:
\code{naprint.default} reports an empty string.
\end{Details}
%
\begin{Value}
A character string providing information on missing values, for
example the number.
\end{Value}
\HeaderA{naresid}{Adjust for Missing Values}{naresid}
\aliasA{napredict}{naresid}{napredict}
\methaliasA{napredict.default}{naresid}{napredict.default}
\methaliasA{napredict.exclude}{naresid}{napredict.exclude}
\methaliasA{naresid.default}{naresid}{naresid.default}
\methaliasA{naresid.exclude}{naresid}{naresid.exclude}
\keyword{NA}{naresid}
\keyword{models}{naresid}
%
\begin{Description}\relax
Use missing value information to adjust residuals and predictions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
naresid(omit, x, ...)
napredict(omit, x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{omit}] an object produced by an \code{\LinkA{na.action}{na.action}} function,
typically the \code{"na.action"} attribute of the result of
\code{\LinkA{na.omit}{na.omit}} or \code{\LinkA{na.exclude}{na.exclude}}.
\item[\code{x}] a vector, data frame, or matrix to be adjusted based upon the
missing value information.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These are utility functions used to allow \code{\LinkA{predict}{predict}},
\code{\LinkA{fitted}{fitted}} and \code{\LinkA{residuals}{residuals}} methods for modelling
functions to compensate for the removal of \code{NA}s in the fitting
process.  They are used by the default, \code{"lm"}, \code{"glm"} and
\code{"nls"} methods, and by further methods in packages \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}},
\Rhref{http://CRAN.R-project.org/package=rpart}{\pkg{rpart}} and \Rhref{http://CRAN.R-project.org/package=survival}{\pkg{survival}}.  Also used for the scores returned by
\code{\LinkA{factanal}{factanal}}, \code{\LinkA{prcomp}{prcomp}} and \code{\LinkA{princomp}{princomp}}.

The default methods do nothing.  The default method for the \code{na.exclude}
action is to pad the object with \code{NA}s in the correct positions to
have the same number of rows as the original data frame.

Currently \code{naresid} and \code{napredict} are identical, but
future methods need not be.  \code{naresid} is used for residuals, and
\code{napredict} for fitted values, predictions and \code{\LinkA{weights}{weights}}.
\end{Details}
%
\begin{Value}
These return a similar object to \code{x}.
\end{Value}
%
\begin{Note}\relax
In the early 2000s, packages \Rhref{http://CRAN.R-project.org/package=rpart}{\pkg{rpart}} and \pkg{survival5} contained
versions of these functions that had an \code{na.omit} action
equivalent to that now used for \code{na.exclude}.
\end{Note}
\HeaderA{NegBinomial}{The Negative Binomial Distribution}{NegBinomial}
\aliasA{dnbinom}{NegBinomial}{dnbinom}
\aliasA{pnbinom}{NegBinomial}{pnbinom}
\aliasA{qnbinom}{NegBinomial}{qnbinom}
\aliasA{rnbinom}{NegBinomial}{rnbinom}
\keyword{distribution}{NegBinomial}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the negative binomial distribution with parameters
\code{size} and \code{prob}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dnbinom(x, size, prob, mu, log = FALSE)
pnbinom(q, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
qnbinom(p, size, prob, mu, lower.tail = TRUE, log.p = FALSE)
rnbinom(n, size, prob, mu)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] vector of (non-negative integer) quantiles.
\item[\code{q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{size}] target for number of successful trials, or dispersion
parameter (the shape parameter of the gamma mixing distribution).
Must be strictly positive, need not be integer.
\item[\code{prob}] probability of success in each trial. \code{0 < prob <= 1}.
\item[\code{mu}] alternative parametrization via mean: see `Details'.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The negative binomial distribution with \code{size} \eqn{= n}{} and
\code{prob} \eqn{= p}{} has density
\deqn{
    p(x) = \frac{\Gamma(x+n)}{\Gamma(n) x!} p^n (1-p)^x}{}
for \eqn{x = 0, 1, 2, \ldots}{}, \eqn{n > 0}{} and \eqn{0 < p \le 1}{}.

This represents the number of failures which occur in a sequence of
Bernoulli trials before a target number of successes is reached.
The mean is \eqn{n(1-p)/p}{} and variance \eqn{n(1-p)/p^2}{}.

A negative binomial distribution can also arise as a mixture of
Poisson distributions with mean distributed as a gamma distribution
(see\code{\LinkA{pgamma}{pgamma}}) with scale parameter \code{(1 - prob)/prob}
and shape parameter \code{size}.  (This definition allows non-integer
values of \code{size}.)

An alternative parametrization (often used in ecology) is by the
\emph{mean} \code{mu}, and \code{size}, the \emph{dispersion
parameter}, where \code{prob} = \code{size/(size+mu)}.  The variance
is \code{mu + mu\textasciicircum{}2/size} in this parametrization.

If an element of \code{x} is not integer, the result of \code{dnbinom}
is zero, with a warning.

The quantile is defined as the smallest value \eqn{x}{} such that
\eqn{F(x) \ge p}{}, where \eqn{F}{} is the distribution function.
\end{Details}
%
\begin{Value}
\code{dnbinom} gives the density,
\code{pnbinom} gives the distribution function,
\code{qnbinom} gives the quantile function, and
\code{rnbinom} generates random deviates.

Invalid \code{size} or \code{prob} will result in return value
\code{NaN}, with a warning.
\end{Value}
%
\begin{Source}\relax
\code{dnbinom} computes via binomial probabilities, using code
contributed by Catherine Loader (see \code{\LinkA{dbinom}{dbinom}}).

\code{pnbinom} uses \code{\LinkA{pbeta}{pbeta}}.

\code{qnbinom} uses the Cornish--Fisher Expansion to include a skewness
correction to a normal approximation, followed by a search.

\code{rnbinom} uses the derivation as a gamma mixture of Poissons, see

Devroye, L. (1986) \emph{Non-Uniform Random Variate Generation.}
Springer-Verlag, New York. Page 480.
\end{Source}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for standard distributions, including
\code{\LinkA{dbinom}{dbinom}} for the binomial, \code{\LinkA{dpois}{dpois}} for the
Poisson and \code{\LinkA{dgeom}{dgeom}} for the geometric distribution, which
is a special case of the negative binomial.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)
x <- 0:11
dnbinom(x, size = 1, prob = 1/2) * 2^(1 + x) # == 1
126 /  dnbinom(0:8, size  = 2, prob  = 1/2) #- theoretically integer

## Cumulative ('p') = Sum of discrete prob.s ('d');  Relative error :
summary(1 - cumsum(dnbinom(x, size = 2, prob = 1/2)) /
                  pnbinom(x, size  = 2, prob = 1/2))

x <- 0:15
size <- (1:20)/4
persp(x,size, dnb <- outer(x, size, function(x,s) dnbinom(x,s, prob= 0.4)),
      xlab = "x", ylab = "s", zlab="density", theta = 150)
title(tit <- "negative binomial density(x,s, pr = 0.4)  vs.  x & s")

image  (x,size, log10(dnb), main= paste("log [",tit,"]"))
contour(x,size, log10(dnb),add=TRUE)

## Alternative parametrization
x1 <- rnbinom(500, mu = 4, size = 1)
x2 <- rnbinom(500, mu = 4, size = 10)
x3 <- rnbinom(500, mu = 4, size = 100)
h1 <- hist(x1, breaks = 20, plot = FALSE)
h2 <- hist(x2, breaks = h1$breaks, plot = FALSE)
h3 <- hist(x3, breaks = h1$breaks, plot = FALSE)
barplot(rbind(h1$counts, h2$counts, h3$counts),
        beside = TRUE, col = c("red","blue","cyan"),
        names.arg = round(h1$breaks[-length(h1$breaks)]))
\end{ExampleCode}
\end{Examples}
\HeaderA{nextn}{Highly Composite Numbers}{nextn}
\keyword{math}{nextn}
%
\begin{Description}\relax
\code{nextn} returns the smallest integer,
greater than or equal to \code{n}, which can be obtained
as a product of powers of the values contained in \code{factors}.
\code{nextn} is intended to be used to find a suitable length
to zero-pad the argument of \code{fft} to
so that the transform is computed quickly.
The default value for \code{factors} ensures this.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nextn(n, factors = c(2,3,5))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] an integer.
\item[\code{factors}] a vector of positive integer factors.
\end{ldescription}
\end{Arguments}
%
\begin{SeeAlso}\relax
\code{\LinkA{convolve}{convolve}}, \code{\LinkA{fft}{fft}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
nextn(1001) # 1024
table(sapply(599:630, nextn))
\end{ExampleCode}
\end{Examples}
\HeaderA{nlm}{Non-Linear Minimization}{nlm}
\keyword{nonlinear}{nlm}
\keyword{optimize}{nlm}
%
\begin{Description}\relax
This function carries out a minimization of the function \code{f}
using a Newton-type algorithm.  See the references for details.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nlm(f, p, ..., hessian = FALSE, typsize = rep(1, length(p)),
    fscale = 1, print.level = 0, ndigit = 12, gradtol = 1e-6,
    stepmax = max(1000 * sqrt(sum((p/typsize)^2)), 1000),
    steptol = 1e-6, iterlim = 100, check.analyticals = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{f}] the function to be minimized.  If the function value has
an attribute called \code{gradient} or both \code{gradient} and
\code{hessian} attributes, these will be used in the calculation of
updated parameter values.  Otherwise, numerical derivatives are
used. \code{\LinkA{deriv}{deriv}} returns a function with suitable
\code{gradient} attribute.  This should be a function of a vector of
the length of \code{p} followed by any other arguments specified
by the \code{...} argument.
\item[\code{p}] starting parameter values for the minimization.
\item[\code{...}] additional arguments to \code{f}.
\item[\code{hessian}] if \code{TRUE}, the hessian of \code{f}
at the minimum is returned.
\item[\code{typsize}] an estimate of the size of each parameter
at the minimum.
\item[\code{fscale}] an estimate of the size of \code{f} at the minimum.
\item[\code{print.level}] this argument determines the level of printing
which is done during the minimization process.  The default
value of \code{0} means that no printing occurs, a value of \code{1}
means that initial and final details are printed and a value
of 2 means that full tracing information is printed.
\item[\code{ndigit}] the number of significant digits in the function \code{f}.
\item[\code{gradtol}] a positive scalar giving the tolerance at which the
scaled gradient is considered close enough to zero to
terminate the algorithm.  The scaled gradient is a
measure of the relative change in \code{f} in each direction
\code{p[i]} divided by the relative change in \code{p[i]}.
\item[\code{stepmax}] a positive scalar which gives the maximum allowable
scaled step length.  \code{stepmax} is used to prevent steps which
would cause the optimization function to overflow, to prevent the
algorithm from leaving the area of interest in parameter space, or to
detect divergence in the algorithm. \code{stepmax} would be chosen
small enough to prevent the first two of these occurrences, but should
be larger than any anticipated reasonable step.
\item[\code{steptol}] A positive scalar providing the minimum allowable
relative step length.
\item[\code{iterlim}] a positive integer specifying the maximum number of
iterations to be performed before the program is terminated.
\item[\code{check.analyticals}] a logical scalar specifying whether the
analytic gradients and Hessians, if they are supplied, should be
checked against numerical derivatives at the initial parameter
values. This can help detect incorrectly formulated gradients or
Hessians.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Note that arguments after \code{...} must be matched exactly.

If a gradient or hessian is supplied but evaluates to the wrong mode
or length, it will be ignored if \code{check.analyticals = TRUE} (the
default) with a warning.  The hessian is not even checked unless the
gradient is present and passes the sanity checks.

From the three methods available in the original source, we always use
method ``1'' which is line search.

The functions supplied must always return finite (including not
\code{NA} and not \code{NaN}) values.

The parameter vector passed to \code{f} and \code{hessian} has special
semantics and is shared between calls.  The functions should not copy it.
\end{Details}
%
\begin{Value}
A list containing the following components:
\begin{ldescription}
\item[\code{minimum}] the value of the estimated minimum of \code{f}.
\item[\code{estimate}] the point at which the minimum value of
\code{f} is obtained.
\item[\code{gradient}] the gradient at the estimated minimum of \code{f}.
\item[\code{hessian}] the hessian at the estimated minimum of \code{f} (if
requested).
\item[\code{code}] an integer indicating why the optimization process terminated.
\begin{description}

\item[1:] relative gradient is close to zero, current iterate is
probably solution.
\item[2:] successive iterates within tolerance, current iterate
is probably solution.
\item[3:] last global step failed to locate a point lower than
\code{estimate}.  Either \code{estimate} is an approximate local
minimum of the function or \code{steptol} is too small.
\item[4:] iteration limit exceeded.
\item[5:] maximum step size \code{stepmax} exceeded five consecutive
times.  Either the function is unbounded below,
becomes asymptotic to a finite value from above in
some direction or \code{stepmax} is too small.

\end{description}


\item[\code{iterations}] the number of iterations performed.
\end{ldescription}
\end{Value}
%
\begin{Source}\relax
The current code is by Saikat DebRoy and the R Core team, using a C
translation of Fortran code by Richard H. Jones.
\end{Source}
%
\begin{References}\relax
Dennis, J. E. and Schnabel, R. B. (1983) \emph{Numerical Methods for
Unconstrained Optimization and Nonlinear Equations.} Prentice-Hall,
Englewood Cliffs, NJ.

Schnabel, R. B., Koontz, J. E. and Weiss, B. E. (1985) A modular
system of algorithms for unconstrained minimization.
\emph{ACM Trans. Math. Software}, \bold{11}, 419--440.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{optim}{optim}} and \code{\LinkA{nlminb}{nlminb}}.


\code{\LinkA{constrOptim}{constrOptim}} for constrained optimization, 
\code{\LinkA{optimize}{optimize}} for one-dimensional
minimization and \code{\LinkA{uniroot}{uniroot}} for root finding.
\code{\LinkA{deriv}{deriv}} to calculate analytical derivatives.

For nonlinear regression, \code{\LinkA{nls}{nls}} may be better.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
f <- function(x) sum((x-1:length(x))^2)
nlm(f, c(10,10))
nlm(f, c(10,10), print.level = 2)
utils::str(nlm(f, c(5), hessian = TRUE))

f <- function(x, a) sum((x-a)^2)
nlm(f, c(10,10), a=c(3,5))
f <- function(x, a)
{
    res <- sum((x-a)^2)
    attr(res, "gradient") <- 2*(x-a)
    res
}
nlm(f, c(10,10), a=c(3,5))

## more examples, including the use of derivatives.
## Not run: demo(nlm)
\end{ExampleCode}
\end{Examples}
\HeaderA{nlminb}{Optimization using PORT routines }{nlminb}
\keyword{optimize}{nlminb}
%
\begin{Description}\relax
Unconstrained and box-constrained optimization using PORT routines.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nlminb(start, objective, gradient = NULL, hessian = NULL, ...,
       scale = 1, control = list(), lower = -Inf, upper = Inf)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{start}] 
numeric vector, initial values for the parameters to be optimized.

\item[\code{objective}] 
Function to be minimized.  Must return a scalar value.  The first
argument to \code{objective} is the vector of parameters to be
optimized, whose initial values are supplied through \code{start}.
Further arguments (fixed during the course of the optimization) to
\code{objective} may be specified as well (see \code{...}).

\item[\code{gradient}] 
Optional function that takes the same arguments as \code{objective} and
evaluates the gradient of \code{objective} at its first argument.  Must
return a vector as long as \code{start}.

\item[\code{hessian}] 
Optional function that takes the same arguments as \code{objective} and
evaluates the hessian of \code{objective} at its first argument.  Must
return a square matrix of order \code{length(start)}.  Only the
lower triangle is used.

\item[\code{...}] Further arguments to be supplied to \code{objective}.
\item[\code{scale}] See PORT documentation (or leave alone).
\item[\code{control}] A list of control parameters. See below for details.
\item[\code{lower, upper}] 
vectors of lower and upper bounds, replicated to be as long as
\code{start}.  If unspecified, all parameters are assumed to be
unconstrained.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Any names of \code{start} are passed on to \code{objective} and where
applicable, \code{gradient} and \code{hessian}.  The parameter vector
will be coerced to double.

The PORT documentation is at
\url{http://netlib.bell-labs.com/cm/cs/cstr/153.pdf}.

The parameter vector passed to \code{objective}, \code{gradient} and
\code{hessian} has special semantics and is shared between calls.  The
functions should not copy it.

If any of the functions returns \code{NA} or \code{NaN} the internal code
could infinite-loop in \R{} prior to 2.15.2: this is now an error for
the gradient and Hessian, and such values for function evaluation are
replaced by \code{+Inf} with a warning.
\end{Details}
%
\begin{Value}
A list with components:
\begin{ldescription}
\item[\code{par}] The best set of parameters found.
\item[\code{objective}] The value of \code{objective} corresponding to \code{par}.
\item[\code{convergence}] An integer code. \code{0} indicates successful
convergence.

\item[\code{message}] 
A character string giving any additional information returned by the
optimizer, or \code{NULL}. For details, see PORT documentation.

\item[\code{iterations}] Number of iterations performed.
\item[\code{evaluations}] Number of objective function and gradient function evaluations
\end{ldescription}
\end{Value}
%
\begin{Section}{Control parameters}
Possible names in the \code{control} list and their default values
are:
\begin{description}

\item[\code{eval.max}] Maximum number of evaluations of the objective
function allowed.  Defaults to 200.
\item[\code{iter.max}] Maximum number of iterations allowed.
Defaults to 150.
\item[\code{trace}] The value of the objective function and the parameters
is printed every trace'th iteration.  Defaults to 0 which
indicates no trace information is to be printed.
\item[\code{abs.tol}] Absolute tolerance.  As from \R{} 2.12.0, defaults
to 0 so the absolute convergence test is not used.  If the objective
function is known to be non-negative, the previous default of
\code{1e-20} would be more appropriate.
\item[\code{rel.tol}] Relative tolerance.  Defaults to
\code{1e-10}.
\item[\code{x.tol}] X tolerance.  Defaults to \code{1.5e-8}.
\item[\code{xf.tol}] false convergence tolerance.  Defaults to
\code{2.2e-14}.
\item[\code{step.min, step.max}] Minimum and maximum step size.  Both
default to \code{1.}.
\item[sing.tol] singular convergence tolerance; defaults to
\code{rel.tol}.
\item[scale.init] ...
\item[diff.g] an estimated bound on the relative error in the
objective function value.

\end{description}

\end{Section}
%
\begin{Author}\relax
\R{} port: Douglas Bates and Deepayan Sarkar.

Underlying Fortran code by David M. Gay
\end{Author}
%
\begin{Source}\relax
 \url{http://netlib.bell-labs.com/netlib/port/} 
\end{Source}
%
\begin{SeeAlso}\relax
\code{\LinkA{optim}{optim}} and \code{\LinkA{nlm}{nlm}}.

\code{\LinkA{optimize}{optimize}} for one-dimensional minimization and
\code{\LinkA{constrOptim}{constrOptim}} for constrained optimization.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

x <- rnbinom(100, mu = 10, size = 10)
hdev <- function(par)
    -sum(dnbinom(x, mu = par[1], size = par[2], log = TRUE))
nlminb(c(9, 12), hdev)
nlminb(c(20, 20), hdev, lower = 0, upper = Inf)
nlminb(c(20, 20), hdev, lower = 0.001, upper = Inf)

## slightly modified from the S-PLUS help page for nlminb
# this example minimizes a sum of squares with known solution y
sumsq <- function( x, y) {sum((x-y)^2)}
y <- rep(1,5)
x0 <- rnorm(length(y))
nlminb(start = x0, sumsq, y = y)
# now use bounds with a y that has some components outside the bounds
y <- c( 0, 2, 0, -2, 0)
nlminb(start = x0, sumsq, lower = -1, upper = 1, y = y)
# try using the gradient
sumsq.g <- function(x,y) 2*(x-y)
nlminb(start = x0, sumsq, sumsq.g,
       lower = -1, upper = 1, y = y)
# now use the hessian, too
sumsq.h <- function(x,y) diag(2, nrow = length(x))
nlminb(start = x0, sumsq, sumsq.g, sumsq.h,
       lower = -1, upper = 1, y = y)

## Rest lifted from optim help page

fr <- function(x) {   ## Rosenbrock Banana function
    x1 <- x[1]
    x2 <- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr <- function(x) { ## Gradient of 'fr'
    x1 <- x[1]
    x2 <- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}
nlminb(c(-1.2,1), fr)
nlminb(c(-1.2,1), fr, grr)


flb <- function(x)
    { p <- length(x); sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2) }
## 25-dimensional box constrained
## par[24] is *not* at boundary
nlminb(rep(3, 25), flb,
          lower=rep(2, 25),
          upper=rep(4, 25))
## trying to use a too small tolerance:
r <- nlminb(rep(3, 25), flb, control = list(rel.tol=1e-16))
stopifnot(grepl("rel.tol", r$message))
\end{ExampleCode}
\end{Examples}
\HeaderA{nls}{Nonlinear Least Squares}{nls}
\keyword{nonlinear}{nls}
\keyword{regression}{nls}
\keyword{models}{nls}
%
\begin{Description}\relax
Determine the nonlinear (weighted) least-squares estimates of the
parameters of a nonlinear model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nls(formula, data, start, control, algorithm,
    trace, subset, weights, na.action, model,
    lower, upper, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a nonlinear model \LinkA{formula}{formula} including variables and
parameters.  Will be coerced to a formula if necessary.
\item[\code{data}] an optional data frame in which to evaluate the variables in
\code{formula} and \code{weights}.  Can also be a list or an
environment, but not a matrix.
\item[\code{start}] a named list or named numeric vector of starting
estimates.  When \code{start} is missing, a very cheap guess for
\code{start} is tried (if \code{algorithm != "plinear"}).

\item[\code{control}] an optional list of control settings.  See
\code{\LinkA{nls.control}{nls.control}} for the names of the settable control
values and their effect.
\item[\code{algorithm}] character string specifying the algorithm to use.
The default algorithm is a Gauss-Newton algorithm.  Other possible
values are \code{"plinear"} for the Golub-Pereyra algorithm for
partially linear least-squares models and \code{"port"} for the
`nl2sol' algorithm from the Port library -- see the references.
\item[\code{trace}] logical value indicating if a trace of the iteration
progress should be printed.  Default is \code{FALSE}.  If
\code{TRUE} the residual (weighted) sum-of-squares and the
parameter values are printed at the conclusion of each iteration.
When the \code{"plinear"} algorithm is used, the conditional
estimates of the linear parameters are printed after the nonlinear
parameters.  When the \code{"port"} algorithm is used the
objective function value printed is half the residual (weighted)
sum-of-squares.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used in the fitting process.
\item[\code{weights}] an optional numeric vector of (fixed) weights.  When
present, the objective function is weighted least squares.
\item[\code{na.action}] a function which indicates what should happen
when the data contain \code{NA}s.  The default is set by
the \code{na.action} setting of \code{\LinkA{options}{options}}, and is
\code{\LinkA{na.fail}{na.fail}} if that is unset.  The `factory-fresh'
default is \code{\LinkA{na.omit}{na.omit}}.  Value \code{\LinkA{na.exclude}{na.exclude}}
can be useful.
\item[\code{model}] logical.  If true, the model frame is returned as part of
the object. Default is \code{FALSE}.
\item[\code{lower, upper}] vectors of lower and upper bounds, replicated to
be as long as \code{start}.  If unspecified, all parameters are
assumed to be unconstrained.  Bounds can only be used with the
\code{"port"} algorithm.  They are ignored, with a warning, if given
for other algorithms.
\item[\code{...}] Additional optional arguments.  None are used at present.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
An \code{nls} object is a type of fitted model object.  It has methods
for the generic functions \code{\LinkA{anova}{anova}}, \code{\LinkA{coef}{coef}},
\code{\LinkA{confint}{confint}}, \code{\LinkA{deviance}{deviance}},
\code{\LinkA{df.residual}{df.residual}}, \code{\LinkA{fitted}{fitted}},
\code{\LinkA{formula}{formula}}, \code{\LinkA{logLik}{logLik}}, \code{\LinkA{predict}{predict}},
\code{\LinkA{print}{print}}, \code{\LinkA{profile}{profile}}, \code{\LinkA{residuals}{residuals}},
\code{\LinkA{summary}{summary}}, \code{\LinkA{vcov}{vcov}} and \code{\LinkA{weights}{weights}}.

Variables in \code{formula} (and \code{weights} if not missing) are
looked for first in \code{data}, then the environment of
\code{formula} and finally along the search path.  Functions in
\code{formula} are searched for first in the environment of
\code{formula} and then along the search path.

Arguments \code{subset} and \code{na.action} are supported only when
all the variables in the formula taken from \code{data} are of the
same length: other cases give a warning.

Note that the \code{\LinkA{anova}{anova}} method does not check that the
models are nested: this cannot easily be done automatically, so use
with care.
\end{Details}
%
\begin{Value}
A list of
\begin{ldescription}
\item[\code{m}] an \code{nlsModel} object incorporating the model.
\item[\code{data}] the expression that was passed to \code{nls} as the data
argument.  The actual data values are present in the environment of
the \code{m} component.
\item[\code{call}] the matched call with several components, notably
\code{algorithm}.
\item[\code{na.action}] the \code{"na.action"} attribute (if any) of the
model frame.
\item[\code{dataClasses}] the \code{"dataClasses"} attribute (if any) of the
\code{"terms"} attribute of the model frame.
\item[\code{model}] if \code{model = TRUE}, the model frame.
\item[\code{weights}] if \code{weights} is supplied, the weights.
\item[\code{convInfo}] a list with convergence information.
\item[\code{control}] the control \code{list} used, see the \code{control}
argument.
\item[\code{convergence, message}] for an \code{algorithm = "port"} fit only,
a convergence code (\code{0} for convergence) and message.

To use these is \emph{deprecated}, as they are available from
\code{convInfo} now.

\end{ldescription}
\end{Value}
%
\begin{Section}{Warning}
\bold{Do not use \code{nls} on artificial "zero-residual" data.}

The \code{nls} function uses a relative-offset convergence criterion
that compares the numerical imprecision at the current parameter
estimates to the residual sum-of-squares.  This performs well on data of
the form \deqn{y=f(x, \theta) + \epsilon}{} (with
\code{var(eps) > 0}).  It fails to indicate convergence on data of the form
\deqn{y = f(x, \theta)}{} because the criterion amounts to
comparing two components of the round-off error.  If you wish to test
\code{nls} on artificial data please add a noise component, as shown
in the example below.

The \code{algorithm = "port"} code appears unfinished, and does
not even check that the starting value is within the bounds.
Use with caution, especially where bounds are supplied.
\end{Section}
%
\begin{Note}\relax
Setting \code{warnOnly = TRUE} in the \code{control}
argument (see \code{\LinkA{nls.control}{nls.control}}) returns a non-converged
object (since \R{} version 2.5.0) which might be useful for further
convergence analysis, \emph{but \bold{not} for inference}.
\end{Note}
%
\begin{Author}\relax
Douglas M. Bates and Saikat DebRoy: David M. Gay for the Fortran code
used by \code{algorithm = "port"}.
\end{Author}
%
\begin{References}\relax
Bates, D. M. and Watts, D. G. (1988)
\emph{Nonlinear Regression Analysis and Its Applications},
Wiley

Bates, D. M. and Chambers, J. M. (1992)
\emph{Nonlinear models.}
Chapter 10 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.

\url{http://www.netlib.org/port/} for the Port library
documentation.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{summary.nls}{summary.nls}}, \code{\LinkA{predict.nls}{predict.nls}},
\code{\LinkA{profile.nls}{profile.nls}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(graphics)

DNase1 <- subset(DNase, Run == 1)

## using a selfStart model
fm1DNase1 <- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)
summary(fm1DNase1)
## the coefficients only:
coef(fm1DNase1)
## including their SE, etc:
coef(summary(fm1DNase1))

## using conditional linearity
fm2DNase1 <- nls(density ~ 1/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(xmid = 0, scal = 1),
                 algorithm = "plinear")
summary(fm2DNase1)

## without conditional linearity
fm3DNase1 <- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1))
summary(fm3DNase1)

## using Port's nl2sol algorithm
fm4DNase1 <- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1),
                 algorithm = "port")
summary(fm4DNase1)

## weighted nonlinear regression
Treated <- Puromycin[Puromycin$state == "treated", ]
weighted.MM <- function(resp, conc, Vm, K)
{
    ## Purpose: exactly as white book p. 451 -- RHS for nls()
    ##  Weighted version of Michaelis-Menten model
    ## ----------------------------------------------------------
    ## Arguments: 'y', 'x' and the two parameters (see book)
    ## ----------------------------------------------------------
    ## Author: Martin Maechler, Date: 23 Mar 2001

    pred <- (Vm * conc)/(K + conc)
    (resp - pred) / sqrt(pred)
}

Pur.wt <- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
              start = list(Vm = 200, K = 0.1))
summary(Pur.wt)

## Passing arguments using a list that can not be coerced to a data.frame
lisTreat <- with(Treated,
                 list(conc1 = conc[1], conc.1 = conc[-1], rate = rate))

weighted.MM1 <- function(resp, conc1, conc.1, Vm, K)
{
     conc <- c(conc1, conc.1)
     pred <- (Vm * conc)/(K + conc)
    (resp - pred) / sqrt(pred)
}
Pur.wt1 <- nls( ~ weighted.MM1(rate, conc1, conc.1, Vm, K),
               data = lisTreat, start = list(Vm = 200, K = 0.1))
stopifnot(all.equal(coef(Pur.wt), coef(Pur.wt1)))

## Chambers and Hastie (1992) Statistical Models in S  (p. 537):
## If the value of the right side [of formula] has an attribute called
## 'gradient' this should be a matrix with the number of rows equal
## to the length of the response and one column for each parameter.

weighted.MM.grad <- function(resp, conc1, conc.1, Vm, K)
{
  conc <- c(conc1, conc.1)

  K.conc <- K+conc
  dy.dV <- conc/K.conc
  dy.dK <- -Vm*dy.dV/K.conc
  pred <- Vm*dy.dV
  pred.5 <- sqrt(pred)
  dev <- (resp - pred) / pred.5
  Ddev <- -0.5*(resp+pred)/(pred.5*pred)
  attr(dev, "gradient") <- Ddev * cbind(Vm = dy.dV, K = dy.dK)
  dev
}

Pur.wt.grad <- nls( ~ weighted.MM.grad(rate, conc1, conc.1, Vm, K),
                   data = lisTreat, start = list(Vm = 200, K = 0.1))

rbind(coef(Pur.wt), coef(Pur.wt1), coef(Pur.wt.grad))

## In this example, there seems no advantage to providing the gradient.
## In other cases, there might be.


## The two examples below show that you can fit a model to
## artificial data with noise but not to artificial data
## without noise.
x <- 1:10
y <- 2*x + 3                            # perfect fit
yeps <- y + rnorm(length(y), sd = 0.01) # added noise
nls(yeps ~ a + b*x, start = list(a = 0.12345, b = 0.54321))
## Not run: 
## terminates in an error, because convergence cannot be confirmed:
nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321))

## End(Not run)

## the nls() internal cheap guess for starting values can be sufficient:

x <- -(1:100)/10
y <- 100 + 10 * exp(x / 2) + rnorm(x)/10
nlmod <- nls(y ~  Const + A * exp(B * x))

plot(x,y, main = "nls(*), data, true function and fit, n=100")
curve(100 + 10 * exp(x / 2), col=4, add = TRUE)
lines(x, predict(nlmod), col=2)


## The muscle dataset in MASS is from an experiment on muscle
## contraction on 21 animals.  The observed variables are Strip
## (identifier of muscle), Conc (Cacl concentration) and Length
## (resulting length of muscle section).
utils::data(muscle, package = "MASS")

## The non linear model considered is
##       Length = alpha + beta*exp(-Conc/theta) + error
## where theta is constant but alpha and beta may vary with Strip.

with(muscle, table(Strip)) # 2,3 or 4 obs per strip

## We first use the plinear algorithm to fit an overall model,
## ignoring that alpha and beta might vary with Strip.

musc.1 <- nls(Length ~ cbind(1, exp(-Conc/th)), muscle,
              start = list(th=1), algorithm="plinear")
summary(musc.1)

## Then we use nls' indexing feature for parameters in non-linear
## models to use the conventional algorithm to fit a model in which
## alpha and beta vary with Strip.  The starting values are provided
## by the previously fitted model.
## Note that with indexed parameters, the starting values must be
## given in a list (with names):
b <- coef(musc.1)
musc.2 <- nls(Length ~ a[Strip] + b[Strip]*exp(-Conc/th),
              muscle,
              start = list(a=rep(b[2],21), b=rep(b[3],21), th=b[1]))
summary(musc.2)

\end{ExampleCode}
\end{Examples}
\HeaderA{nls.control}{Control the Iterations in nls}{nls.control}
\keyword{nonlinear}{nls.control}
\keyword{regression}{nls.control}
\keyword{models}{nls.control}
%
\begin{Description}\relax
Allow the user to set some characteristics of the \code{nls}
nonlinear least squares algorithm.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nls.control(maxiter = 50, tol = 1e-05, minFactor = 1/1024,
            printEval = FALSE, warnOnly = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{maxiter}] A positive integer specifying the maximum number of
iterations allowed.
\item[\code{tol}] A positive numeric value specifying the tolerance level for
the relative offset convergence criterion.
\item[\code{minFactor}] A positive numeric value specifying the minimum
step-size factor allowed on any step in the iteration.  The
increment is calculated with a Gauss-Newton algorithm and
successively halved until the residual sum of squares has been
decreased or until the step-size factor has been reduced below this
limit.
\item[\code{printEval}] a logical specifying whether the number of evaluations
(steps in the gradient direction taken each iteration) is printed.
\item[\code{warnOnly}] a logical specifying whether \code{\LinkA{nls}{nls}()} should
return instead of signalling an error in the case of termination
before convergence.
Termination before convergence happens upon completion of \code{maxiter}
iterations, in the case of a singular gradient, and in the case that the
step-size factor is reduced below \code{minFactor}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A \code{list} with exactly five components:
\begin{ldescription}
\item[\code{maxiter}] 
\item[\code{tol}] 
\item[\code{minFactor}] 
\item[\code{printEval}] 
\item[\code{warnOnly}] 
\end{ldescription}
with meanings as explained under `Arguments'.
\end{Value}
%
\begin{Author}\relax
Douglas Bates and Saikat DebRoy
\end{Author}
%
\begin{References}\relax
Bates, D. M. and Watts, D. G. (1988),
\emph{Nonlinear Regression Analysis and Its Applications}, Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
nls.control(minFactor = 1/2048)
\end{ExampleCode}
\end{Examples}
\HeaderA{NLSstAsymptotic}{Fit the Asymptotic Regression Model}{NLSstAsymptotic}
\methaliasA{NLSstAsymptotic.sortedXyData}{NLSstAsymptotic}{NLSstAsymptotic.sortedXyData}
\keyword{manip}{NLSstAsymptotic}
%
\begin{Description}\relax
Fits the asymptotic regression model, in the form \code{b0 +
      b1*(1-exp(-exp(lrc) * x)} to the \code{xy} data.  
This can be used as a building block in determining starting estimates
for more complicated models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
NLSstAsymptotic(xy)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{xy}] a \code{sortedXyData} object
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A numeric value of length 3 with components labelled \code{b0},
\code{b1}, and \code{lrc}.  \code{b0} is the estimated intercept on
the \code{y}-axis, \code{b1} is the estimated difference between the
asymptote and the \code{y}-intercept, and \code{lrc} is the estimated
logarithm of the rate constant.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{SSasymp}{SSasymp}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
Lob.329 <- Loblolly[ Loblolly$Seed == "329", ]
print(NLSstAsymptotic(sortedXyData(expression(age),
                                   expression(height),
                                   Lob.329)), digits=3)
\end{ExampleCode}
\end{Examples}
\HeaderA{NLSstClosestX}{Inverse Interpolation}{NLSstClosestX}
\methaliasA{NLSstClosestX.sortedXyData}{NLSstClosestX}{NLSstClosestX.sortedXyData}
\keyword{manip}{NLSstClosestX}
%
\begin{Description}\relax
Use inverse linear interpolation to approximate the \code{x} value at
which the function represented by \code{xy} is equal to \code{yval}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
NLSstClosestX(xy, yval)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{xy}] a \code{sortedXyData} object
\item[\code{yval}] a numeric value on the \code{y} scale
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A single numeric value on the \code{x} scale.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{sortedXyData}{sortedXyData}}, \code{\LinkA{NLSstLfAsymptote}{NLSstLfAsymptote}},
\code{\LinkA{NLSstRtAsymptote}{NLSstRtAsymptote}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
DNase.2 <- DNase[ DNase$Run == "2", ]
DN.srt <- sortedXyData(expression(log(conc)), expression(density), DNase.2)
NLSstClosestX(DN.srt, 1.0)
\end{ExampleCode}
\end{Examples}
\HeaderA{NLSstLfAsymptote}{Horizontal Asymptote on the Left Side}{NLSstLfAsymptote}
\methaliasA{NLSstLfAsymptote.sortedXyData}{NLSstLfAsymptote}{NLSstLfAsymptote.sortedXyData}
\keyword{manip}{NLSstLfAsymptote}
%
\begin{Description}\relax
Provide an initial guess at the horizontal asymptote on the left side
(i.e., small values of \code{x}) of the graph of \code{y} versus
\code{x} from the \code{xy} object.  Primarily used within
\code{initial} functions for self-starting nonlinear regression
models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
NLSstLfAsymptote(xy)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{xy}] a \code{sortedXyData} object
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A single numeric value estimating the horizontal asymptote for small
\code{x}. 
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{sortedXyData}{sortedXyData}},
\code{\LinkA{NLSstClosestX}{NLSstClosestX}},
\code{\LinkA{NLSstRtAsymptote}{NLSstRtAsymptote}},
\code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
DNase.2 <- DNase[ DNase$Run == "2", ]
DN.srt <- sortedXyData( expression(log(conc)), expression(density), DNase.2 )
NLSstLfAsymptote( DN.srt )
\end{ExampleCode}
\end{Examples}
\HeaderA{NLSstRtAsymptote}{Horizontal Asymptote on the Right Side}{NLSstRtAsymptote}
\methaliasA{NLSstRtAsymptote.sortedXyData}{NLSstRtAsymptote}{NLSstRtAsymptote.sortedXyData}
\keyword{manip}{NLSstRtAsymptote}
%
\begin{Description}\relax
Provide an initial guess at the horizontal asymptote on the right side
(i.e., large values of \code{x}) of the graph of \code{y} versus
\code{x} from the \code{xy} object.  Primarily used within
\code{initial} functions for self-starting nonlinear regression
models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
NLSstRtAsymptote(xy)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{xy}] a \code{sortedXyData} object
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A single numeric value estimating the horizontal asymptote for large
\code{x}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{sortedXyData}{sortedXyData}},
\code{\LinkA{NLSstClosestX}{NLSstClosestX}},
\code{\LinkA{NLSstRtAsymptote}{NLSstRtAsymptote}},
\code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
DNase.2 <- DNase[ DNase$Run == "2", ]
DN.srt <- sortedXyData( expression(log(conc)), expression(density), DNase.2 )
NLSstRtAsymptote( DN.srt )
\end{ExampleCode}
\end{Examples}
\HeaderA{nobs}{Extract the Number of Observations from a Fit.}{nobs}
\methaliasA{nobs.default}{nobs}{nobs.default}
\keyword{models}{nobs}
%
\begin{Description}\relax
Extract the number of `observations' from a model fit.  This is
principally intended to be used in computing BIC (see \code{\LinkA{AIC}{AIC}}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nobs(object, ...)

## Default S3 method:
nobs(object, use.fallback = FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] A fitted model object.
\item[\code{use.fallback}] logical: should fallback methods be used to try to
guess the value?
\item[\code{...}] Further arguments to be passed to methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function, with an S4 generic in package \pkg{stats4}.
There are methods in this package for objects of classes
\code{"\LinkA{lm}{lm}"}, \code{"\LinkA{glm}{glm}"}, \code{"\LinkA{nls}{nls}"} and
\code{"\LinkA{logLik}{logLik}"}, as well as a default method (which throws an
error, unless \code{use.fallback = TRUE} when it looks for
\code{weights} and \code{residuals} components -- use with care!).

The main usage is in determining the appropriate penalty for BIC, but
\code{nobs} is also used by the stepwise fitting methods
\code{\LinkA{step}{step}}, \code{\LinkA{add1}{add1}} and \code{\LinkA{drop1}{drop1}} as a
quick check that different fits have been fitted to the same set of
data (and not, say, that further rows have been dropped because of NAs
in the new predictors).

For \code{lm}, \code{glm} and \code{nls} fits, observations with zero
weight are not included.
\end{Details}
%
\begin{Value}
A single number, normally an integer.  Could be \code{NA}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{AIC}{AIC}}.
\end{SeeAlso}
\HeaderA{Normal}{The Normal Distribution}{Normal}
\aliasA{dnorm}{Normal}{dnorm}
\aliasA{pnorm}{Normal}{pnorm}
\aliasA{qnorm}{Normal}{qnorm}
\aliasA{rnorm}{Normal}{rnorm}
\keyword{distribution}{Normal}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the normal distribution with mean equal to \code{mean}
and standard deviation equal to \code{sd}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dnorm(x, mean = 0, sd = 1, log = FALSE)
pnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)
rnorm(n, mean = 0, sd = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x,q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{mean}] vector of means.
\item[\code{sd}] vector of standard deviations.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{} otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{mean} or \code{sd} are not specified they assume the default
values of \code{0} and \code{1}, respectively.

The normal distribution has density
\deqn{
    f(x) =
    \frac{1}{\sqrt{2\pi}\sigma} e^{-(x-\mu)^2/2\sigma^2}}{}
where \eqn{\mu}{} is the mean of the distribution and
\eqn{\sigma}{} the standard deviation.

\code{qnorm} is based on Wichura's algorithm AS 241 which provides
precise results up to about 16 digits.
\end{Details}
%
\begin{Value}
\code{dnorm} gives the density,
\code{pnorm} gives the distribution function,
\code{qnorm} gives the quantile function, and
\code{rnorm} generates random deviates.
\end{Value}
%
\begin{Source}\relax
For \code{pnorm}, based on

Cody, W. D. (1993)
Algorithm 715: SPECFUN -- A portable FORTRAN package of special
function routines and test drivers.
\emph{ACM Transactions on Mathematical Software} \bold{19}, 22--32.

For \code{qnorm}, the code is a C translation of

Wichura, M. J. (1988)
Algorithm AS 241: The percentage points of the normal distribution.
\emph{Applied Statistics}, \bold{37}, 477--484.

For \code{rnorm}, see \LinkA{RNG}{RNG} for how to select the algorithm and
for references to the supplied methods.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 1, chapter 13.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dlnorm}{dlnorm}} for the \emph{Log}normal distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

dnorm(0) == 1/ sqrt(2*pi)
dnorm(1) == exp(-1/2)/ sqrt(2*pi)
dnorm(1) == 1/ sqrt(2*pi*exp(1))

## Using "log = TRUE" for an extended range :
par(mfrow=c(2,1))
plot(function(x) dnorm(x, log=TRUE), -60, 50,
     main = "log { Normal density }")
curve(log(dnorm(x)), add=TRUE, col="red",lwd=2)
mtext("dnorm(x, log=TRUE)", adj=0)
mtext("log(dnorm(x))", col="red", adj=1)

plot(function(x) pnorm(x, log.p=TRUE), -50, 10,
     main = "log { Normal Cumulative }")
curve(log(pnorm(x)), add=TRUE, col="red",lwd=2)
mtext("pnorm(x, log=TRUE)", adj=0)
mtext("log(pnorm(x))", col="red", adj=1)

## if you want the so-called 'error function'
erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
## (see Abramowitz and Stegun 29.2.29)
## and the so-called 'complementary error function'
erfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)
## and the inverses
erfinv <- function (x) qnorm((1 + x)/2)/sqrt(2)
erfcinv <- function (x) qnorm(x/2, lower = FALSE)/sqrt(2)
\end{ExampleCode}
\end{Examples}
\HeaderA{numericDeriv}{Evaluate Derivatives Numerically}{numericDeriv}
\keyword{models}{numericDeriv}
%
\begin{Description}\relax
\code{numericDeriv} numerically evaluates the gradient of an expression.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
numericDeriv(expr, theta, rho = parent.frame(), dir = 1.0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{expr}] The expression to be differentiated.  The value of this
expression should be a numeric vector.
\item[\code{theta}] A character vector of names of numeric variables
used in \code{expr}.
\item[\code{rho}] An environment containing all the variables needed to
evaluate \code{expr}.
\item[\code{dir}] A numeric vector of directions to use for the finite
differences.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a front end to the C function \code{numeric\_deriv}, which is
described in \emph{Writing R Extensions}.

The numeric variables must be of type \code{real} and not \code{integer}.
\end{Details}
%
\begin{Value}
The value of \code{eval(expr, envir = rho)} plus a matrix
attribute called \code{gradient}.  The columns of this matrix are
the derivatives of the value with respect to the variables listed in
\code{theta}.
\end{Value}
%
\begin{Author}\relax
Saikat DebRoy \email{saikat@stat.wisc.edu}
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}

myenv <- new.env()
assign("mean", 0., envir = myenv)
assign("sd", 1., envir = myenv)
assign("x", seq(-3., 3., len = 31), envir = myenv)
numericDeriv(quote(pnorm(x, mean, sd)), c("mean", "sd"), myenv)

\end{ExampleCode}
\end{Examples}
\HeaderA{offset}{Include an Offset in a Model Formula}{offset}
\keyword{models}{offset}
%
\begin{Description}\relax
An offset is a term to be added to a linear predictor, such as in a
generalised linear model, with known coefficient 1 rather than an
estimated coefficient.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
offset(object)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] An offset to be included in a model frame
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
There can be more than one offset in a model formula, but \code{-} is
not supported for \code{offset} terms (and is equivalent to \code{+}). 
\end{Details}
%
\begin{Value}
The input value.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{model.offset}{model.offset}}, \code{\LinkA{model.frame}{model.frame}}.

For examples see \code{\LinkA{glm}{glm}} and
\code{\LinkA{Insurance}{Insurance}} in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}.
\end{SeeAlso}
\HeaderA{oneway.test}{Test for Equal Means in a One-Way Layout}{oneway.test}
\keyword{htest}{oneway.test}
%
\begin{Description}\relax
Test whether two or more samples from normal distributions have the
same means.  The variances are not necessarily assumed to be equal.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
oneway.test(formula, data, subset, na.action, var.equal = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
gives the sample values and \code{rhs} the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{var.equal}] a logical variable indicating whether to treat the
variances in the samples as equal.  If \code{TRUE}, then a simple F
test for the equality of means in a one-way analysis of variance is 
performed.  If \code{FALSE}, an approximate method of Welch (1951)
is used, which generalizes the commonly known 2-sample Welch test to
the case of arbitrarily many samples.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the test statistic.
\item[\code{parameter}] the degrees of freedom of the exact or approximate F
distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] a character string indicating the test performed.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
B. L. Welch (1951),
On the comparison of several mean values: an alternative approach.
\emph{Biometrika}, \bold{38}, 330--336.
\end{References}
%
\begin{SeeAlso}\relax
The standard t test (\code{\LinkA{t.test}{t.test}}) as the special case for two
samples;
the Kruskal-Wallis test \code{\LinkA{kruskal.test}{kruskal.test}} for a nonparametric
test for equal location parameters in a one-way layout.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Not assuming equal variances
oneway.test(extra ~ group, data = sleep)
## Assuming equal variances
oneway.test(extra ~ group, data = sleep, var.equal = TRUE)
## which gives the same result as
anova(lm(extra ~ group, data = sleep))
\end{ExampleCode}
\end{Examples}
\HeaderA{optim}{General-purpose Optimization}{optim}
\aliasA{optimHess}{optim}{optimHess}
\keyword{nonlinear}{optim}
\keyword{optimize}{optim}
%
\begin{Description}\relax
General-purpose optimization based on Nelder--Mead, quasi-Newton and
conjugate-gradient algorithms. It includes an option for
box-constrained optimization and simulated annealing.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
optim(par, fn, gr = NULL, ...,
      method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"),
      lower = -Inf, upper = Inf,
      control = list(), hessian = FALSE)

optimHess(par, fn, gr = NULL, ..., control = list())
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{par}] Initial values for the parameters to be optimized over.
\item[\code{fn}] A function to be minimized (or maximized), with first
argument the vector of parameters over which minimization is to take
place.  It should return a scalar result.
\item[\code{gr}] A function to return the gradient for the \code{"BFGS"},
\code{"CG"} and \code{"L-BFGS-B"} methods.  If it is \code{NULL}, a
finite-difference approximation will be used.

For the \code{"SANN"} method it specifies a function to generate a new
candidate point.  If it is \code{NULL} a default Gaussian Markov
kernel is used.
\item[\code{...}] Further arguments to be passed to \code{fn} and \code{gr}.
\item[\code{method}] The method to be used. See `Details'.
\item[\code{lower, upper}] Bounds on the variables for the \code{"L-BFGS-B"}
method, or bounds in which to \emph{search} for method \code{"Brent"}.
\item[\code{control}] A list of control parameters. See `Details'.
\item[\code{hessian}] Logical. Should a numerically differentiated Hessian
matrix be returned?
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Note that arguments after \code{...} must be matched exactly.

By default \code{optim} performs minimization, but it will maximize
if \code{control\$fnscale} is negative.  \code{optimHess} is an
auxiliary function to compute the Hessian at a later stage if
\code{hessian = TRUE} was forgotten.

The default method is an implementation of that of Nelder and Mead
(1965), that uses only function values and is robust but relatively
slow.  It will work reasonably well for non-differentiable functions.

Method \code{"BFGS"} is a quasi-Newton method (also known as a variable
metric algorithm), specifically that published simultaneously in 1970
by Broyden, Fletcher, Goldfarb and Shanno.  This uses function values
and gradients to build up a picture of the surface to be optimized.

Method \code{"CG"} is a conjugate gradients method based on that by
Fletcher and Reeves (1964) (but with the option of Polak--Ribiere or
Beale--Sorenson updates).  Conjugate gradient methods will generally
be more fragile than the BFGS method, but as they do not store a
matrix they may be successful in much larger optimization problems.

Method \code{"L-BFGS-B"} is that of Byrd \emph{et. al.} (1995) which
allows \emph{box constraints}, that is each variable can be given a lower
and/or upper bound. The initial value must satisfy the constraints.
This uses a limited-memory modification of the BFGS quasi-Newton
method. If non-trivial bounds are supplied, this method will be
selected, with a warning.

Nocedal and Wright (1999) is a comprehensive reference for the
previous three methods.

Method \code{"SANN"} is by default a variant of simulated annealing
given in Belisle (1992). Simulated-annealing belongs to the class of
stochastic global optimization methods. It uses only function values
but is relatively slow. It will also work for non-differentiable
functions. This implementation uses the Metropolis function for the
acceptance probability. By default the next candidate point is
generated from a Gaussian Markov kernel with scale proportional to the
actual temperature. If a function to generate a new candidate point is
given, method \code{"SANN"} can also be used to solve combinatorial
optimization problems. Temperatures are decreased according to the
logarithmic cooling schedule as given in Belisle (1992, p. 890);
specifically, the temperature is set to
\code{temp / log(((t-1) \%/\% tmax)*tmax + exp(1))}, where \code{t} is
the current iteration step and \code{temp} and \code{tmax} are
specifiable via \code{control}, see below.  Note that the
\code{"SANN"} method depends critically on the settings of the control
parameters. It is not a general-purpose method but can be very useful
in getting to a good value on a very rough surface.

Method \code{"Brent"} is for one-dimensional problems only, using
\code{\LinkA{optimize}{optimize}()}.  It can be useful in cases where
\code{optim()} is used inside other functions where only \code{method}
can be specified, such as in \code{\LinkA{mle}{mle}} from package \pkg{stats4}.

Function \code{fn} can return \code{NA} or \code{Inf} if the function
cannot be evaluated at the supplied value, but the initial value must
have a computable finite value of \code{fn}.
(Except for method \code{"L-BFGS-B"} where the values should always be
finite.)

\code{optim} can be used recursively, and for a single parameter
as well as many.  It also accepts a zero-length \code{par}, and just
evaluates the function with that argument.

The \code{control} argument is a list that can supply any of the
following components:
\begin{description}

\item[\code{trace}] Non-negative integer. If positive,
tracing information on the
progress of the optimization is produced. Higher values may
produce more tracing information: for method \code{"L-BFGS-B"}
there are six levels of tracing.  (To understand exactly what
these do see the source code: higher levels give more detail.)
\item[\code{fnscale}] An overall scaling to be applied to the value
of \code{fn} and \code{gr} during optimization. If negative,
turns the problem into a maximization problem. Optimization is
performed on \code{fn(par)/fnscale}.
\item[\code{parscale}] A vector of scaling values for the parameters.
Optimization is performed on \code{par/parscale} and these should be
comparable in the sense that a unit change in any element produces
about a unit change in the scaled value.  Not used (nor needed)
for \code{method = "Brent"}.
\item[\code{ndeps}] A vector of step sizes for the finite-difference
approximation to the gradient, on \code{par/parscale}
scale. Defaults to \code{1e-3}.
\item[\code{maxit}] The maximum number of iterations. Defaults to
\code{100} for the derivative-based methods, and
\code{500} for \code{"Nelder-Mead"}.

For \code{"SANN"} \code{maxit} gives the total number of function
evaluations: there is no other stopping criterion. Defaults to
\code{10000}.

\item[\code{abstol}] The absolute convergence tolerance. Only
useful for non-negative functions, as a tolerance for reaching zero.
\item[\code{reltol}] Relative convergence tolerance.  The algorithm
stops if it is unable to reduce the value by a factor of
\code{reltol * (abs(val) + reltol)} at a step.  Defaults to
\code{sqrt(.Machine\$double.eps)}, typically about \code{1e-8}.
\item[\code{alpha}, \code{beta}, \code{gamma}] Scaling parameters
for the \code{"Nelder-Mead"} method. \code{alpha} is the reflection
factor (default 1.0), \code{beta} the contraction factor (0.5) and
\code{gamma} the expansion factor (2.0).
\item[\code{REPORT}] The frequency of reports for the \code{"BFGS"},
\code{"L-BFGS-B"} and \code{"SANN"} methods if \code{control\$trace}
is positive. Defaults to every 10 iterations for \code{"BFGS"} and
\code{"L-BFGS-B"}, or every 100 temperatures for \code{"SANN"}.
\item[\code{type}] for the conjugate-gradients method. Takes value
\code{1} for the Fletcher--Reeves update, \code{2} for
Polak--Ribiere and \code{3} for Beale--Sorenson.
\item[\code{lmm}] is an integer giving the number of BFGS updates
retained in the \code{"L-BFGS-B"} method, It defaults to \code{5}.
\item[\code{factr}] controls the convergence of the \code{"L-BFGS-B"}
method. Convergence occurs when the reduction in the objective is
within this factor of the machine tolerance. Default is \code{1e7},
that is a tolerance of about \code{1e-8}.
\item[\code{pgtol}] helps control the convergence of the \code{"L-BFGS-B"}
method. It is a tolerance on the projected gradient in the current
search direction. This defaults to zero, when the check is
suppressed.
\item[\code{temp}] controls the \code{"SANN"} method. It is the
starting temperature for the cooling schedule. Defaults to
\code{10}.
\item[\code{tmax}] is the number of function evaluations at each
temperature for the \code{"SANN"} method. Defaults to \code{10}.

\end{description}


Any names given to \code{par} will be copied to the vectors passed to
\code{fn} and \code{gr}.  Note that no other attributes of \code{par}
are copied over.

The parameter vector passed to \code{fn} has special semantics and may
be shared between calls: the function should not change or copy it.
\end{Details}
%
\begin{Value}
For \code{optim}, a list with components:
\begin{ldescription}
\item[\code{par}] The best set of parameters found.
\item[\code{value}] The value of \code{fn} corresponding to \code{par}.
\item[\code{counts}] A two-element integer vector giving the number of calls
to \code{fn} and \code{gr} respectively. This excludes those calls needed
to compute the Hessian, if requested, and any calls to \code{fn} to
compute a finite-difference approximation to the gradient.
\item[\code{convergence}] An integer code. \code{0} indicates successful
completion (which is always the case for \code{"SANN"} and
\code{"Brent"}).  Possible error codes are
\begin{description}

\item[\code{1}] indicates that the iteration limit \code{maxit}
had been reached.
\item[\code{10}] indicates degeneracy of the Nelder--Mead simplex.
\item[\code{51}] indicates a warning from the \code{"L-BFGS-B"}
method; see component \code{message} for further details.
\item[\code{52}] indicates an error from the \code{"L-BFGS-B"}
method; see component \code{message} for further details.

\end{description}


\item[\code{message}] A character string giving any additional information
returned by the optimizer, or \code{NULL}.
\item[\code{hessian}] Only if argument \code{hessian} is true. A symmetric
matrix giving an estimate of the Hessian at the solution found.  Note
that this is the Hessian of the unconstrained problem even if the
box constraints are active.

\end{ldescription}
For \code{optimHess}, the description of the \code{hessian} component
applies.
\end{Value}
%
\begin{Note}\relax
\code{optim} will work with one-dimensional \code{par}s, but the
default method does not work well (and will warn).  Method
\code{"Brent"} uses \code{\LinkA{optimize}{optimize}} and needs bounds to be available;
\code{"BFGS"} often works well enough if not.
\end{Note}
%
\begin{Source}\relax
The code for methods \code{"Nelder-Mead"}, \code{"BFGS"} and
\code{"CG"} was based originally on Pascal code in Nash (1990) that was
translated by \code{p2c} and then hand-optimized.  Dr Nash has agreed
that the code can be made freely available.

The code for method \code{"L-BFGS-B"} is based on Fortran code by Zhu,
Byrd, Lu-Chen and Nocedal obtained from Netlib (file
\file{opt/lbfgs\_bcm.shar}: another version is in \file{toms/778}).

The code for method \code{"SANN"} was contributed by A. Trapletti.
\end{Source}
%
\begin{References}\relax
Belisle, C. J. P. (1992) Convergence theorems for a class of simulated
annealing algorithms on \eqn{R^d}{}. \emph{J. Applied Probability},
\bold{29}, 885--895.

Byrd, R. H., Lu, P., Nocedal, J. and Zhu, C.  (1995) A limited
memory algorithm for bound constrained optimization.
\emph{SIAM J. Scientific Computing}, \bold{16}, 1190--1208.

Fletcher, R. and Reeves, C. M. (1964) Function minimization by
conjugate gradients. \emph{Computer Journal} \bold{7}, 148--154.

Nash, J. C. (1990) \emph{Compact Numerical Methods for
Computers. Linear Algebra and Function Minimisation.} Adam Hilger.

Nelder, J. A. and Mead, R. (1965) A simplex algorithm for function
minimization. \emph{Computer Journal} \bold{7}, 308--313.

Nocedal, J. and Wright, S. J. (1999) \emph{Numerical Optimization}.
Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{nlm}{nlm}}, \code{\LinkA{nlminb}{nlminb}}.

\code{\LinkA{optimize}{optimize}} for one-dimensional minimization and
\code{\LinkA{constrOptim}{constrOptim}} for constrained optimization.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(graphics)

fr <- function(x) {   ## Rosenbrock Banana function
    x1 <- x[1]
    x2 <- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr <- function(x) { ## Gradient of 'fr'
    x1 <- x[1]
    x2 <- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}
optim(c(-1.2,1), fr)
(res <- optim(c(-1.2,1), fr, grr, method = "BFGS"))
optimHess(res$par, fr, grr)
optim(c(-1.2,1), fr, NULL, method = "BFGS", hessian = TRUE)
## These do not converge in the default number of steps
optim(c(-1.2,1), fr, grr, method = "CG")
optim(c(-1.2,1), fr, grr, method = "CG", control=list(type=2))
optim(c(-1.2,1), fr, grr, method = "L-BFGS-B")

flb <- function(x)
    { p <- length(x); sum(c(1, rep(4, p-1)) * (x - c(1, x[-p])^2)^2) }
## 25-dimensional box constrained
optim(rep(3, 25), flb, NULL, method = "L-BFGS-B",
      lower=rep(2, 25), upper=rep(4, 25)) # par[24] is *not* at boundary

## "wild" function , global minimum at about -15.81515
fw <- function (x)
    10*sin(0.3*x)*sin(1.3*x^2) + 0.00001*x^4 + 0.2*x+80
plot(fw, -50, 50, n=1000, main = "optim() minimising 'wild function'")

res <- optim(50, fw, method="SANN",
             control=list(maxit=20000, temp=20, parscale=20))
res
## Now improve locally {typically only by a small bit}:
(r2 <- optim(res$par, fw, method="BFGS"))
points(r2$par, r2$value, pch = 8, col = "red", cex = 2)

## Combinatorial optimization: Traveling salesman problem
library(stats) # normally loaded

eurodistmat <- as.matrix(eurodist)

distance <- function(sq) {  # Target function
    sq2 <- embed(sq, 2)
    sum(eurodistmat[cbind(sq2[,2],sq2[,1])])
}

genseq <- function(sq) {  # Generate new candidate sequence
    idx <- seq(2, NROW(eurodistmat)-1)
    changepoints <- sample(idx, size=2, replace=FALSE)
    tmp <- sq[changepoints[1]]
    sq[changepoints[1]] <- sq[changepoints[2]]
    sq[changepoints[2]] <- tmp
    sq
}

sq <- c(1:nrow(eurodistmat), 1)  # Initial sequence: alphabetic
distance(sq)
# rotate for conventional orientation
loc <- -cmdscale(eurodist, add=TRUE)$points
x <- loc[,1]; y <- loc[,2]
s <- seq_len(nrow(eurodistmat))
tspinit <- loc[sq,]

plot(x, y, type="n", asp=1, xlab="", ylab="",
     main="initial solution of traveling salesman problem", axes = FALSE)
arrows(tspinit[s,1], tspinit[s,2], tspinit[s+1,1], tspinit[s+1,2],
       angle=10, col="green")
text(x, y, labels(eurodist), cex=0.8)

set.seed(123) # chosen to get a good soln relatively quickly
res <- optim(sq, distance, genseq, method = "SANN",
             control = list(maxit = 30000, temp = 2000, trace = TRUE,
                            REPORT = 500))
res  # Near optimum distance around 12842

tspres <- loc[res$par,]
plot(x, y, type="n", asp=1, xlab="", ylab="",
     main="optim() 'solving' traveling salesman problem", axes = FALSE)
arrows(tspres[s,1], tspres[s,2], tspres[s+1,1], tspres[s+1,2],
       angle=10, col="red")
text(x, y, labels(eurodist), cex=0.8)
\end{ExampleCode}
\end{Examples}
\HeaderA{optimize}{One Dimensional Optimization}{optimize}
\aliasA{optimise}{optimize}{optimise}
\keyword{optimize}{optimize}
%
\begin{Description}\relax
The function \code{optimize} searches the interval from
\code{lower} to \code{upper} for a minimum or maximum of
the function \code{f} with respect to its first argument.

\code{optimise} is an alias for \code{optimize}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
optimize(f = , interval = ,  ..., lower = min(interval),
         upper = max(interval), maximum = FALSE,
         tol = .Machine$double.eps^0.25)
optimise(f = , interval = ,  ..., lower = min(interval),
         upper = max(interval), maximum = FALSE,
         tol = .Machine$double.eps^0.25)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{f}] the function to be optimized.  The function is
either minimized or maximized over its first argument
depending on the value of \code{maximum}.
\item[\code{interval}] a vector containing the end-points of the interval
to be searched for the minimum.
\item[\code{...}] additional named or unnamed arguments to be passed
to \code{f}.
\item[\code{lower}] the lower end point of the interval
to be searched.
\item[\code{upper}] the upper end point of the interval
to be searched.
\item[\code{maximum}] logical.  Should we maximize or minimize (the default)?
\item[\code{tol}] the desired accuracy.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Note that arguments after \code{...} must be matched exactly.

The method used is a combination of golden section search and
successive parabolic interpolation, and was designed for use with
continuous functions.  Convergence is never much slower
than that for a Fibonacci search.  If \code{f} has a continuous second
derivative which is positive at the minimum (which is not at \code{lower} or
\code{upper}), then convergence is superlinear, and usually of the
order of about 1.324.

The function \code{f} is never evaluated at two points closer together
than \eqn{\epsilon}{}\eqn{ |x_0| + (tol/3)}{}, where
\eqn{\epsilon}{} is approximately \code{sqrt(\LinkA{.Machine}{.Machine}\$double.eps)}
and \eqn{x_0}{} is the final abscissa \code{optimize()\$minimum}.\\{}
If \code{f} is a unimodal function and the computed values of \code{f}
are always unimodal when separated by at least \eqn{\epsilon}{}
\eqn{ |x| + (tol/3)}{}, then \eqn{x_0}{} approximates the abscissa of the
global minimum of \code{f} on the interval \code{lower,upper} with an
error less than \eqn{\epsilon}{}\eqn{ |x_0|+ tol}{}.\\{}
If \code{f} is not unimodal, then \code{optimize()} may approximate a
local, but perhaps non-global, minimum to the same accuracy.

The first evaluation of \code{f} is always at
\eqn{x_1 = a + (1-\phi)(b-a)}{} where \code{(a,b) = (lower, upper)} and
\eqn{\phi = (\sqrt 5 - 1)/2 = 0.61803..}{}
is the golden section ratio.
Almost always, the second evaluation is at
\eqn{x_2 = a + \phi(b-a)}{}.
Note that a local minimum inside \eqn{[x_1,x_2]}{} will be found as
solution, even when \code{f} is constant in there, see the last
example.

\code{f} will be called as \code{f(\var{x}, ...)} for a numeric value
of \var{x}.
\end{Details}
%
\begin{Value}
A list with components \code{minimum} (or \code{maximum})
and \code{objective} which give the location of the minimum (or maximum)
and the value of the function at that point.
\end{Value}
%
\begin{Note}\relax
The argument passed to \code{f} has special semantics and is shared
between calls.  The function should not copy it.
\end{Note}
%
\begin{Source}\relax
A C translation of Fortran code \url{http://www.netlib.org/fmm/fmin.f}
(author(s) unstated)
based on the Algol 60 procedure \code{localmin} given in the reference.
\end{Source}
%
\begin{References}\relax
Brent, R. (1973)
\emph{Algorithms for Minimization without Derivatives.}
Englewood Cliffs N.J.: Prentice-Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{nlm}{nlm}}, \code{\LinkA{uniroot}{uniroot}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

f <- function (x,a) (x-a)^2
xmin <- optimize(f, c(0, 1), tol = 0.0001, a = 1/3)
xmin

## See where the function is evaluated:
optimize(function(x) x^2*(print(x)-1), lower=0, upper=10)

## "wrong" solution with unlucky interval and piecewise constant f():
f  <- function(x) ifelse(x > -1, ifelse(x < 4, exp(-1/abs(x - 1)), 10), 10)
fp <- function(x) { print(x); f(x) }

plot(f, -2,5, ylim = 0:1, col = 2)
optimize(fp, c(-4, 20))# doesn't see the minimum
optimize(fp, c(-7, 20))# ok
\end{ExampleCode}
\end{Examples}
\HeaderA{order.dendrogram}{Ordering or Labels of the Leaves in a Dendrogram}{order.dendrogram}
\aliasA{labels.dendrogram}{order.dendrogram}{labels.dendrogram}
\keyword{manip}{order.dendrogram}
%
\begin{Description}\relax
Theses functions return the order (index) or the \code{"label"}
attribute for the leaves in a
dendrogram.  These indices can then be used to access the appropriate
components of any additional data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
order.dendrogram(x)

## S3 method for class 'dendrogram'
labels(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, object}] a dendrogram (see \code{\LinkA{as.dendrogram}{as.dendrogram}}).
\item[\code{...}] additional arguments
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The indices or labels for the leaves in left to right order are retrieved.
\end{Details}
%
\begin{Value}
A vector with length equal to the number of leaves in the dendrogram
is returned.  From \code{r <- order.dendrogram()}, each element is the
index into the original data (from which the dendrogram was computed).
\end{Value}
%
\begin{Author}\relax
R. Gentleman (\code{order.dendrogram}) and Martin Maechler
(\code{labels.dendrogram}).
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{reorder}{reorder}}, \code{\LinkA{dendrogram}{dendrogram}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
set.seed(123)
x <- rnorm(10)
hc <- hclust(dist(x))
hc$order
dd <- as.dendrogram(hc)
order.dendrogram(dd) ## the same :
stopifnot(hc$order == order.dendrogram(dd))

d2 <- as.dendrogram(hclust(dist(USArrests)))
labels(d2) ## in this case the same as
stopifnot(identical(labels(d2),
   rownames(USArrests)[order.dendrogram(d2)]))
\end{ExampleCode}
\end{Examples}
\HeaderA{p.adjust}{Adjust P-values for Multiple Comparisons}{p.adjust}
\methaliasA{p.adjust.methods}{p.adjust}{p.adjust.methods}
\keyword{htest}{p.adjust}
%
\begin{Description}\relax
Given a set of p-values, returns p-values adjusted using
one of several methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
p.adjust(p, method = p.adjust.methods, n = length(p))

p.adjust.methods
# c("holm", "hochberg", "hommel", "bonferroni", "BH", "BY",
#   "fdr", "none")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{p}] numeric vector of p-values (possibly with \code{\LinkA{NA}{NA}}s).
Any other \R{} is coerced by \code{\LinkA{as.numeric}{as.numeric}}.
\item[\code{method}] correction method
\item[\code{n}] number of comparisons, must be at least \code{length(p)};
only set this (to non-default) when you know what you are doing!
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The adjustment methods include the Bonferroni correction
(\code{"bonferroni"}) in which the p-values are multiplied by the
number of comparisons.  Less conservative corrections are also
included by Holm (1979) (\code{"holm"}), Hochberg (1988)
(\code{"hochberg"}), Hommel (1988) (\code{"hommel"}), Benjamini \&
Hochberg (1995) (\code{"BH"} or its alias \code{"fdr"}), and Benjamini \&
Yekutieli (2001) (\code{"BY"}), respectively.
A pass-through option (\code{"none"}) is also included.
The set of methods are contained in the \code{p.adjust.methods} vector
for the benefit of methods that need to have the method as an option
and pass it on to \code{p.adjust}.

The first four methods are designed to give strong control of the
family-wise error rate.  There seems no reason to use the unmodified
Bonferroni correction because it is dominated by Holm's method, which
is also valid under arbitrary assumptions.

Hochberg's and Hommel's methods are valid when the hypothesis tests
are independent or when they are non-negatively associated (Sarkar,
1998; Sarkar and Chang, 1997).  Hommel's method is more powerful than
Hochberg's, but the difference is usually small and the Hochberg
p-values are faster to compute.

The \code{"BH"} (aka \code{"fdr"}) and \code{"BY"} method of
Benjamini, Hochberg, and Yekutieli control the false discovery rate,
the expected proportion of false discoveries amongst the rejected
hypotheses.  The false discovery rate is a less stringent condition
than the family-wise error rate, so these methods are more powerful
than the others.

Note that you can set \code{n} larger than \code{length(p)} which
means the unobserved p-values are assumed to be greater than all the
observed p for \code{"bonferroni"} and \code{"holm"} methods and equal
to 1 for the other methods.
\end{Details}
%
\begin{Value}
A numeric vector of corrected p-values (of the same length as
\code{p}, with names copied from \code{p}).
\end{Value}
%
\begin{References}\relax
Benjamini, Y., and Hochberg, Y. (1995).
Controlling the false discovery rate: a practical and powerful
approach to multiple testing.
\emph{Journal of the Royal Statistical Society Series} B, \bold{57},
289--300.

Benjamini, Y., and Yekutieli, D. (2001).
The control of the false discovery rate in multiple testing under
dependency.
\emph{Annals of Statistics} \bold{29}, 1165--1188.

Holm, S. (1979).
A simple sequentially rejective multiple test procedure.
\emph{Scandinavian Journal of Statistics}, \bold{6}, 65--70.

Hommel, G. (1988).
A stagewise rejective multiple test procedure based on a modified
Bonferroni test.
\emph{Biometrika}, \bold{75}, 383--386.

Hochberg, Y. (1988).
A sharper Bonferroni procedure for multiple tests of significance.
\emph{Biometrika}, \bold{75}, 800--803.

Shaffer, J. P. (1995).
Multiple hypothesis testing.
\emph{Annual Review of Psychology}, \bold{46}, 561--576.
(An excellent review of the area.)

Sarkar, S. (1998).
Some probability inequalities for ordered MTP2 random variables: a
proof of Simes conjecture.
\emph{Annals of Statistics}, \bold{26}, 494--504.

Sarkar, S., and Chang, C. K. (1997).
Simes' method for multiple hypothesis testing with positively
dependent test statistics.
\emph{Journal of the American Statistical Association}, \bold{92},
1601--1608.

Wright, S. P. (1992).
Adjusted P-values for simultaneous inference.
\emph{Biometrics}, \bold{48}, 1005--1013.
(Explains the adjusted P-value approach.)
\end{References}
%
\begin{SeeAlso}\relax
\code{pairwise.*} functions such as \code{\LinkA{pairwise.t.test}{pairwise.t.test}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

set.seed(123)
x <- rnorm(50, mean=c(rep(0,25),rep(3,25)))
p <- 2*pnorm( sort(-abs(x)))

round(p, 3)
round(p.adjust(p), 3)
round(p.adjust(p,"BH"), 3)

## or all of them at once (dropping the "fdr" alias):
p.adjust.M <- p.adjust.methods[p.adjust.methods != "fdr"]
p.adj    <- sapply(p.adjust.M, function(meth) p.adjust(p, meth))
p.adj.60 <- sapply(p.adjust.M, function(meth) p.adjust(p, meth, n = 60))
stopifnot(identical(p.adj[,"none"], p), p.adj <= p.adj.60)
round(p.adj, 3)
## or a bit nicer:
noquote(apply(p.adj, 2, format.pval, digits = 3))


## and a graphic:
matplot(p, p.adj, ylab="p.adjust(p, meth)", type = "l", asp=1, lty=1:6,
        main = "P-value adjustments")
legend(.7,.6, p.adjust.M, col=1:6, lty=1:6)

## Can work with NA's:
pN <- p; iN <- c(46,47); pN[iN] <- NA
pN.a <- sapply(p.adjust.M, function(meth) p.adjust(pN, meth))
## The smallest 20 P-values all affected by the NA's :
round((pN.a / p.adj)[1:20, ] , 4)
\end{ExampleCode}
\end{Examples}
\HeaderA{pairwise.prop.test}{ Pairwise comparisons for proportions}{pairwise.prop.test}
\keyword{htest}{pairwise.prop.test}
%
\begin{Description}\relax
Calculate pairwise comparisons between pairs of proportions with
correction for multiple testing 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pairwise.prop.test(x, n, p.adjust.method = p.adjust.methods, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}]  Vector of counts of successes or a matrix with 2 columns
giving the counts of successes and failures, respectively. 
\item[\code{n}]  Vector of counts of trials; ignored if \code{x} is a matrix.
\item[\code{p.adjust.method}] Method for adjusting p values
(see \code{\LinkA{p.adjust}{p.adjust}}) 
\item[\code{...}]  Additional arguments to pass to \code{prop.test} 
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Object of class \code{"pairwise.htest"}
\end{Value}
%
\begin{SeeAlso}\relax
 \code{\LinkA{prop.test}{prop.test}}, \code{\LinkA{p.adjust}{p.adjust}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
smokers  <- c( 83, 90, 129, 70 )
patients <- c( 86, 93, 136, 82 )
pairwise.prop.test(smokers, patients)
\end{ExampleCode}
\end{Examples}
\HeaderA{pairwise.t.test}{ Pairwise t tests}{pairwise.t.test}
\keyword{htest}{pairwise.t.test}
%
\begin{Description}\relax
Calculate pairwise comparisons between group levels with corrections
for multiple testing 
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pairwise.t.test(x, g, p.adjust.method = p.adjust.methods,
                pool.sd = !paired, paired = FALSE,
                alternative = c("two.sided", "less", "greater"),
                ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}]  response vector. 
\item[\code{g}]  grouping vector or factor. 
\item[\code{p.adjust.method}]  Method for adjusting p values (see \code{\LinkA{p.adjust}{p.adjust}}). 
\item[\code{pool.sd}]  switch to allow/disallow the use of a pooled SD 
\item[\code{paired}]  a logical indicating whether you want paired
t-tests. 
\item[\code{alternative}]  a character string specifying the alternative
hypothesis, must be one of \code{"two.sided"} (default),
\code{"greater"} or \code{"less"}.  
\item[\code{...}]  additional arguments to pass to \code{t.test}. 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
 The \code{pool.sd} switch calculates a common SD for all
groups and uses that for all comparisons (this can be useful if some
groups are small). This method does not actually call \code{t.test},
so extra arguments are ignored. Pooling does not generalize to paired tests
so \code{pool.sd} and \code{paired} cannot both be \code{TRUE}.

Only the lower triangle of the matrix of possible comparisons is being
calculated, so setting \code{alternative} to anything other than
\code{"two.sided"} requires that the levels of \code{g} are ordered
sensibly. 
\end{Details}
%
\begin{Value}
Object of class \code{"pairwise.htest"}
\end{Value}
%
\begin{SeeAlso}\relax
 \code{\LinkA{t.test}{t.test}}, \code{\LinkA{p.adjust}{p.adjust}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
attach(airquality)
Month <- factor(Month, labels = month.abb[5:9])
pairwise.t.test(Ozone, Month)
pairwise.t.test(Ozone, Month, p.adj = "bonf")
pairwise.t.test(Ozone, Month, pool.sd = FALSE)
detach()
\end{ExampleCode}
\end{Examples}
\HeaderA{pairwise.table}{Tabulate p values for pairwise comparisons}{pairwise.table}
\keyword{htest}{pairwise.table}
%
\begin{Description}\relax
Creates  table of p values for pairwise comparisons
with corrections for multiple testing.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pairwise.table(compare.levels, level.names, p.adjust.method)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{compare.levels}]  Function to compute (raw) p value given indices
\code{i} and \code{j} 
\item[\code{level.names}]  Names of the group levels
\item[\code{p.adjust.method}] Method for multiple testing adjustment
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Functions that do multiple group comparisons create separate
\code{compare.levels} functions (assumed to be symmetrical in \code{i}
and \code{j}) and passes them to this function.
\end{Details}
%
\begin{Value}
Table of p values in lower triangular form.
\end{Value}
%
\begin{SeeAlso}\relax
 \code{\LinkA{pairwise.t.test}{pairwise.t.test}}, et al.
\end{SeeAlso}
\HeaderA{pairwise.wilcox.test}{Pairwise Wilcoxon Rank Sum Tests}{pairwise.wilcox.test}
\keyword{htest}{pairwise.wilcox.test}
%
\begin{Description}\relax
Calculate pairwise comparisons between group levels with corrections
for multiple testing.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
pairwise.wilcox.test(x, g, p.adjust.method = p.adjust.methods,
                      paired=FALSE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}]  response vector. 
\item[\code{g}]  grouping vector or factor. 
\item[\code{p.adjust.method}]  method for adjusting p values (see
\code{\LinkA{p.adjust}{p.adjust}}). 
\item[\code{paired}] a logical indicating whether you want a paired test.
\item[\code{...}] additional arguments to pass to \code{\LinkA{wilcox.test}{wilcox.test}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Extra arguments that are passed on to \code{wilcox.test} may or may
not be sensible in this context. In particular,
only the lower triangle of the matrix of possible comparisons is being
calculated, so setting \code{alternative} to anything other than
\code{"two.sided"} requires that the levels of \code{g} are ordered
sensibly.
\end{Details}
%
\begin{Value}
Object of class \code{"pairwise.htest"}
\end{Value}
%
\begin{SeeAlso}\relax
 \code{\LinkA{wilcox.test}{wilcox.test}}, \code{\LinkA{p.adjust}{p.adjust}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
attach(airquality)
Month <- factor(Month, labels = month.abb[5:9])
## These give warnings because of ties :
pairwise.wilcox.test(Ozone, Month)
pairwise.wilcox.test(Ozone, Month, p.adj = "bonf")
detach()
\end{ExampleCode}
\end{Examples}
\HeaderA{plot.acf}{Plot Autocovariance and Autocorrelation Functions}{plot.acf}
\keyword{hplot}{plot.acf}
\keyword{ts}{plot.acf}
%
\begin{Description}\relax
Plot method for objects of class \code{"acf"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'acf'
plot(x, ci = 0.95, type = "h", xlab = "Lag", ylab = NULL,
     ylim = NULL, main = NULL,
     ci.col = "blue", ci.type = c("white", "ma"),
     max.mfrow = 6, ask = Npgs > 1 && dev.interactive(),
     mar = if(nser > 2) c(3,2,2,0.8) else par("mar"),
     oma = if(nser > 2) c(1,1.2,1,1) else par("oma"),
     mgp = if(nser > 2) c(1.5,0.6,0) else par("mgp"),
     xpd = par("xpd"),
     cex.main = if(nser > 2) 1 else par("cex.main"),
     verbose = getOption("verbose"),
     ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object of class \code{"acf"}.
\item[\code{ci}] coverage probability for confidence interval.  Plotting of
the confidence interval is suppressed if \code{ci} is zero or
negative.
\item[\code{type}] the type of plot to be drawn, default to histogram like
vertical lines.
\item[\code{xlab}] the x label of the plot.
\item[\code{ylab}] the y label of the plot.
\item[\code{ylim}] numeric of length 2 giving the y limits for the plot.
\item[\code{main}] overall title for the plot.
\item[\code{ci.col}] colour to plot the confidence interval lines.
\item[\code{ci.type}] should the confidence limits assume a white noise
input or for lag \eqn{k}{} an MA(\eqn{k-1}{}) input?
\item[\code{max.mfrow}] positive integer; for multivariate \code{x}
indicating how many rows and columns of plots should be put on one
page, using \code{\LinkA{par}{par}(mfrow = c(m,m))}.
\item[\code{ask}] logical; if \code{TRUE}, the user is asked before a new
page is started.
\item[\code{mar, oma, mgp, xpd, cex.main}] graphics parameters as in
\code{\LinkA{par}{par}(*)}, by default adjusted to use smaller than
default margins for multivariate \code{x} only.

\item[\code{verbose}] logical.  Should \R{} report extra information on
progress?
\item[\code{...}] graphics parameters to be passed to the plotting
routines.
\end{ldescription}
\end{Arguments}
%
\begin{Note}\relax
The confidence interval plotted in \code{plot.acf} is based on an
\emph{uncorrelated} series and should be treated with appropriate
caution.  Using \code{ci.type = "ma"} may be less potentially
misleading.
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{acf}{acf}} which calls \code{plot.acf} by default.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)


z4  <- ts(matrix(rnorm(400), 100, 4), start=c(1961, 1), frequency=12)
z7  <- ts(matrix(rnorm(700), 100, 7), start=c(1961, 1), frequency=12)
acf(z4)
acf(z7, max.mfrow = 7)# squeeze on 1 page
acf(z7) # multi-page
\end{ExampleCode}
\end{Examples}
\HeaderA{plot.density}{Plot Method for Kernel Density Estimation}{plot.density}
\keyword{dplot}{plot.density}
%
\begin{Description}\relax
The \code{plot} method for density objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'density'
plot(x, main = NULL, xlab = NULL, ylab = "Density", type = "l",
     zero.line = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a \code{"density"} object.
\item[\code{main, xlab, ylab, type}] plotting parameters with useful defaults.
\item[\code{...}] further plotting parameters.
\item[\code{zero.line}] logical; if \code{TRUE}, add a base line at \eqn{y = 0}{}
\end{ldescription}
\end{Arguments}
%
\begin{Value}
None.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{density}{density}}.
\end{SeeAlso}
\HeaderA{plot.HoltWinters}{Plot function for HoltWinters objects}{plot.HoltWinters}
\keyword{ts}{plot.HoltWinters}
%
\begin{Description}\relax
Produces a chart of the original time series along with the fitted
values. Optionally, predicted values (and their confidence bounds) can
also be plotted.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'HoltWinters'
plot(x, predicted.values = NA, intervals = TRUE,
        separator = TRUE, col = 1, col.predicted = 2,
        col.intervals = 4, col.separator = 1, lty = 1,
        lty.predicted = 1, lty.intervals = 1, lty.separator = 3,
        ylab = "Observed / Fitted",
        main = "Holt-Winters filtering",
        ylim = NULL, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] Object of class \code{"HoltWinters"}
\item[\code{predicted.values}] Predicted values as returned by \code{predict.HoltWinters}
\item[\code{intervals}] If \code{TRUE}, the prediction intervals are plotted (default).
\item[\code{separator}] If \code{TRUE}, a separating line between fitted and predicted values is plotted (default).
\item[\code{col, lty}] Color/line type of original data (default: black solid).
\item[\code{col.predicted, lty.predicted}] Color/line type of
fitted and predicted values (default: red solid).
\item[\code{col.intervals, lty.intervals}] Color/line type of prediction
intervals (default: blue solid).
\item[\code{col.separator, lty.separator}] Color/line type of
observed/predicted values separator (default: black dashed).
\item[\code{ylab}] Label of the y-axis.
\item[\code{main}] Main title.
\item[\code{ylim}] Limits of the y-axis. If \code{NULL}, the range is chosen
such that the plot contains the original series, the fitted values,
and the predicted values if any.
\item[\code{...}] Other graphics parameters.
\end{ldescription}
\end{Arguments}
%
\begin{Author}\relax
David Meyer \email{David.Meyer@wu.ac.at}
\end{Author}
%
\begin{References}\relax
C. C. Holt (1957)
Forecasting trends and seasonals by exponentially weighted
moving averages,
\emph{ONR Research Memorandum, Carnegie Institute of Technology} \bold{52}.

P. R. Winters (1960)
Forecasting sales by exponentially weighted moving averages,
\emph{Management Science} \bold{6}, 324--342.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{HoltWinters}{HoltWinters}}, \code{\LinkA{predict.HoltWinters}{predict.HoltWinters}}
\end{SeeAlso}
\HeaderA{plot.isoreg}{Plot Method for isoreg Objects}{plot.isoreg}
\aliasA{lines.isoreg}{plot.isoreg}{lines.isoreg}
\keyword{hplot}{plot.isoreg}
\keyword{print}{plot.isoreg}
%
\begin{Description}\relax
The \code{\LinkA{plot}{plot}} and \code{\LinkA{lines}{lines}} method for
\R{} objects of class \code{\LinkA{isoreg}{isoreg}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'isoreg'
plot(x, plot.type = c("single", "row.wise", "col.wise"),
      main = paste("Isotonic regression", deparse(x$call)),
      main2 = "Cumulative Data and Convex Minorant",
      xlab = "x0", ylab = "x$y",
      par.fit = list(col = "red", cex = 1.5, pch = 13, lwd = 1.5),
      mar = if (both) 0.1 + c(3.5, 2.5, 1, 1) else par("mar"),
      mgp = if (both) c(1.6, 0.7, 0) else par("mgp"),
      grid = length(x$x) < 12, ...)

## S3 method for class 'isoreg'
lines(x, col = "red", lwd = 1.5,
       do.points = FALSE, cex = 1.5, pch = 13, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an \code{\LinkA{isoreg}{isoreg}} object.
\item[\code{plot.type}] character indicating which type of plot is desired.
The first (default) only draws the data and the fit, where the
others add a plot of the cumulative data and fit.
\item[\code{main}] main title of plot, see \code{\LinkA{title}{title}}.
\item[\code{main2}] title for second (cumulative) plot.
\item[\code{xlab, ylab}] x- and y- axis annotation.
\item[\code{par.fit}] a \code{\LinkA{list}{list}} of arguments (for
\code{\LinkA{points}{points}} and \code{\LinkA{lines}{lines}}) for drawing the fit.
\item[\code{mar, mgp}] graphical parameters, see \code{\LinkA{par}{par}}, mainly
for the case of two plots.
\item[\code{grid}] logical indicating if grid lines should be drawn.  If
true, \code{\LinkA{grid}{grid}()} is used for the first plot, where as
vertical lines are drawn at `touching' points for the
cumulative plot.
\item[\code{do.points}] for \code{lines()}: logical indicating if the step
points should be drawn as well (and as they are drawn in \code{plot()}).
\item[\code{col, lwd, cex, pch}] graphical arguments for \code{lines()},
where \code{cex} and \code{pch} are only used when \code{do.points}
is \code{TRUE}.
\item[\code{...}] further arguments passed to and from methods.

\end{ldescription}
\end{Arguments}
%
\begin{SeeAlso}\relax
\code{\LinkA{isoreg}{isoreg}} for computation of \code{isoreg} objects.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

utils::example(isoreg) # for the examples there

plot(y3, main = "simple plot(.)  +  lines(<isoreg>)")
lines(ir3)

## 'same' plot as above, "proving" that only ranks of 'x' are important
plot(isoreg(2^(1:9), c(1,0,4,3,3,5,4,2,0)), plot.type = "row", log = "x")

plot(ir3, plot.type = "row", ylab = "y3")
plot(isoreg(y3 - 4), plot.t="r", ylab = "y3 - 4")
plot(ir4, plot.type = "ro",  ylab = "y4", xlab = "x = 1:n")

## experiment a bit with these (C-c C-j):
plot(isoreg(sample(9),  y3), plot.type="row")
plot(isoreg(sample(9),  y3), plot.type="col.wise")

plot(ir <- isoreg(sample(10), sample(10, replace = TRUE)),
                  plot.type = "r")
\end{ExampleCode}
\end{Examples}
\HeaderA{plot.lm}{Plot Diagnostics for an lm Object}{plot.lm}
\aliasA{plot.mlm}{plot.lm}{plot.mlm}
\keyword{hplot}{plot.lm}
\keyword{regression}{plot.lm}
%
\begin{Description}\relax
Six plots (selectable by \code{which}) are currently available: a plot
of residuals against fitted values, a Scale-Location plot of
\eqn{\sqrt{| residuals |}}{}
against fitted values, a Normal Q-Q plot, a
plot of Cook's distances versus row labels, a plot of residuals
against leverages, and a plot of Cook's distances against
leverage/(1-leverage).  By default, the first three and \code{5} are provided.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'lm'
plot(x, which = c(1:3,5), 
     caption = list("Residuals vs Fitted", "Normal Q-Q",
       "Scale-Location", "Cook's distance",
       "Residuals vs Leverage",
       expression("Cook's dist vs Leverage  " * h[ii] / (1 - h[ii]))),
     panel = if(add.smooth) panel.smooth else points,
     sub.caption = NULL, main = "",
     ask = prod(par("mfcol")) < length(which) && dev.interactive(),
     ...,
     id.n = 3, labels.id = names(residuals(x)), cex.id = 0.75,
     qqline = TRUE, cook.levels = c(0.5, 1.0),
     add.smooth = getOption("add.smooth"), label.pos = c(4,2),
     cex.caption = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] \code{lm} object, typically result of \code{\LinkA{lm}{lm}} or
\code{\LinkA{glm}{glm}}.
\item[\code{which}] if a subset of the plots is required, specify a subset of
the numbers \code{1:6}.
\item[\code{caption}] captions to appear above the plots;
\code{\LinkA{character}{character}} vector or \code{\LinkA{list}{list}} of valid
graphics annotations, see \code{\LinkA{as.graphicsAnnot}{as.graphicsAnnot}}.
Can be set to \code{""} or \code{NA} to suppress all captions.

\item[\code{panel}] panel function.  The useful alternative to
\code{\LinkA{points}{points}}, \code{\LinkA{panel.smooth}{panel.smooth}} can be chosen
by \code{add.smooth = TRUE}.
\item[\code{sub.caption}] common title---above the figures if there are more
than one; used as \code{sub} (s.\code{\LinkA{title}{title}}) otherwise.  If
\code{NULL}, as by default, a possible abbreviated version of
\code{deparse(x\$call)} is used.
\item[\code{main}] title to each plot---in addition to \code{caption}.
\item[\code{ask}] logical; if \code{TRUE}, the user is \emph{ask}ed before
each plot, see \code{\LinkA{par}{par}(ask=.)}.
\item[\code{...}] other parameters to be passed through to plotting
functions.
\item[\code{id.n}] number of points to be labelled in each plot, starting
with the most extreme.
\item[\code{labels.id}] vector of labels, from which the labels for extreme
points will be chosen.  \code{NULL} uses observation numbers.
\item[\code{cex.id}] magnification of point labels.
\item[\code{qqline}] logical indicating if a \code{\LinkA{qqline}{qqline}()} should be
added to the normal Q-Q plot.
\item[\code{cook.levels}] levels of Cook's distance at which to draw contours.
\item[\code{add.smooth}] logical indicating if a smoother should be added to
most plots; see also \code{panel} above.
\item[\code{label.pos}] positioning of labels, for the left half and right
half of the graph respectively, for plots 1-3.
\item[\code{cex.caption}] controls the size of \code{caption}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{sub.caption}---by default the function call---is shown as
a subtitle (under the x-axis title) on each plot when plots are on
separate pages, or as a subtitle in the outer margin (if any) when
there are multiple plots per page.

The `Scale-Location' plot, also called `Spread-Location' or
`S-L' plot, takes the square root of the absolute residuals in
order to diminish skewness (\eqn{\sqrt{| E |}}{}) is much less skewed
than \eqn{| E |}{} for Gaussian zero-mean \eqn{E}{}).

The `S-L', the Q-Q, and the Residual-Leverage plot, use
\emph{standardized} residuals which have identical variance (under the
hypothesis).  They are given as
\eqn{R_i / (s \times \sqrt{1 - h_{ii}})}{}
where \eqn{h_{ii}}{} are the diagonal entries of the hat matrix,
\code{\LinkA{influence}{influence}()\$hat} (see also \code{\LinkA{hat}{hat}}), and
where the Residual-Leverage plot uses standardized Pearson residuals
(\code{\LinkA{residuals.glm}{residuals.glm}(type = "pearson")}) for \eqn{R[i]}{}.

The Residual-Leverage plot shows contours of equal Cook's distance,
for values of \code{cook.levels} (by default 0.5 and 1) and omits
cases with leverage one with a warning.  If the leverages are constant
(as is typically the case in a balanced \code{\LinkA{aov}{aov}} situation)
the plot uses factor level combinations instead of the leverages for
the x-axis.  (The factor levels are ordered by mean fitted value.)

In the Cook's distance vs leverage/(1-leverage) plot, contours of
standardized residuals that are equal in magnitude are lines through
the origin.  The contour lines are labelled with the magnitudes.
\end{Details}
%
\begin{Author}\relax
John Maindonald and Martin Maechler.
\end{Author}
%
\begin{References}\relax
Belsley, D. A., Kuh, E. and Welsch, R. E. (1980)
\emph{Regression Diagnostics.}  New York: Wiley.

Cook, R. D. and Weisberg, S. (1982)
\emph{Residuals and Influence in Regression.}
London: Chapman and Hall.

Firth, D. (1991) Generalized Linear Models.  In Hinkley, D. V. and
Reid, N. and Snell, E. J., eds: Pp. 55-82 in Statistical Theory and
Modelling. In Honour of Sir David Cox, FRS.  London: Chapman and Hall.

Hinkley, D. V. (1975) On power transformations to
symmetry. \emph{Biometrika} \bold{62}, 101--111.

McCullagh, P. and Nelder, J. A. (1989)
\emph{Generalized Linear Models.}
London: Chapman and Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{termplot}{termplot}}, \code{\LinkA{lm.influence}{lm.influence}},
\code{\LinkA{cooks.distance}{cooks.distance}}, \code{\LinkA{hatvalues}{hatvalues}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Analysis of the life-cycle savings data
## given in Belsley, Kuh and Welsch.
lm.SR <- lm(sr ~ pop15 + pop75 + dpi + ddpi, data = LifeCycleSavings)
plot(lm.SR)

## 4 plots on 1 page;
## allow room for printing model formula in outer margin:
par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))
plot(lm.SR)
plot(lm.SR, id.n = NULL)               # no id's
plot(lm.SR, id.n = 5, labels.id = NULL)# 5 id numbers

## Was default in R <= 2.1.x:
## Cook's distances instead of Residual-Leverage plot
plot(lm.SR, which = 1:4)

## Fit a smooth curve, where applicable:
plot(lm.SR, panel = panel.smooth)
## Gives a smoother curve
plot(lm.SR, panel = function(x,y) panel.smooth(x, y, span = 1))

par(mfrow=c(2,1))# same oma as above
plot(lm.SR, which = 1:2, sub.caption = "Saving Rates, n=50, p=5")


\end{ExampleCode}
\end{Examples}
\HeaderA{plot.ppr}{Plot Ridge Functions for Projection Pursuit Regression Fit}{plot.ppr}
\keyword{hplot}{plot.ppr}
%
\begin{Description}\relax
Plot ridge functions for projection pursuit regression fit.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'ppr'
plot(x, ask, type = "o", ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] 
A fit of class \code{"ppr"} as produced by a call to \code{ppr}.

\item[\code{ask}] 
the graphics parameter \code{ask}: see \code{par} for details.
If set to \code{TRUE} will ask between the plot of each cross-section.

\item[\code{type}] 
the type of line to draw

\item[\code{...}] 
further graphical parameters

\end{ldescription}
\end{Arguments}
%
\begin{Value}
None
\end{Value}
%
\begin{Section}{Side Effects}
A series of plots are drawn on the current graphical device, one for
each term in the fit.
\end{Section}
%
\begin{SeeAlso}\relax
\code{\LinkA{ppr}{ppr}}, \code{\LinkA{par}{par}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

with(rock, {
area1 <- area/10000; peri1 <- peri/10000
par(mfrow=c(3,2))# maybe: , pty="s")
rock.ppr <- ppr(log(perm) ~ area1 + peri1 + shape,
                data = rock, nterms = 2, max.terms = 5)
plot(rock.ppr, main="ppr(log(perm)~ ., nterms=2, max.terms=5)")
plot(update(rock.ppr, bass=5), main = "update(..., bass = 5)")
plot(update(rock.ppr, sm.method="gcv", gcvpen=2),
     main = "update(..., sm.method=\"gcv\", gcvpen=2)")
})
\end{ExampleCode}
\end{Examples}
\HeaderA{plot.profile.nls}{Plot a profile.nls Object}{plot.profile.nls}
\keyword{nonlinear}{plot.profile.nls}
\keyword{regression}{plot.profile.nls}
\keyword{models}{plot.profile.nls}
%
\begin{Description}\relax
Displays a series of plots of the profile t function and interpolated
confidence intervals for the parameters in a nonlinear regression
model that has been fit with \code{nls} and profiled with
\code{profile.nls}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'profile.nls'
plot(x, levels, conf = c(99, 95, 90, 80, 50)/100,
     absVal = TRUE, ylab = NULL, lty = 2, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object of class \code{"profile.nls"} 
\item[\code{levels}] levels, on the scale of the absolute value of a t
statistic, at which to interpolate intervals.  Usually \code{conf}
is used instead of giving \code{levels} explicitly.
\item[\code{conf}] a numeric vector of confidence levels for profile-based
confidence intervals on the parameters.
Defaults to \code{c(0.99, 0.95, 0.90, 0.80, 0.50).}
\item[\code{absVal}] a logical value indicating whether or not the plots
should be on the scale of the absolute value of the profile t.
Defaults to \code{TRUE}.
\item[\code{lty}] the line type to be used for axis and dropped lines.
\item[\code{ylab, ...}] other arguments to the \code{\LinkA{plot.default}{plot.default}}
function can be passed here (but not \code{xlab}, \code{xlim},
\code{ylim} nor \code{type}).
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The plots are produced in a set of hard-coded colours, but as these
are coded by number their effect can be changed by setting the
\code{\LinkA{palette}{palette}}.  Colour 1 is used for the axes and 4 for the
profile itself.  Colours 3 and 6 are used for the axis line at zero and
the horizontal/vertical lines dropping to the axes.
\end{Details}
%
\begin{Author}\relax
Douglas M. Bates and Saikat DebRoy
\end{Author}
%
\begin{References}\relax
Bates, D.M. and Watts, D.G. (1988),
\emph{Nonlinear Regression Analysis and Its Applications},
Wiley (chapter 6)
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}},
\code{\LinkA{profile}{profile}},
\code{\LinkA{profile.nls}{profile.nls}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

# obtain the fitted object
fm1 <- nls(demand ~ SSasympOrig(Time, A, lrc), data = BOD)
# get the profile for the fitted model
pr1 <- profile(fm1, alpha = 0.05)
opar <- par(mfrow = c(2,2), oma = c(1.1, 0, 1.1, 0), las = 1)
plot(pr1, conf = c(95, 90, 80, 50)/100)
plot(pr1, conf = c(95, 90, 80, 50)/100, absVal = FALSE)
mtext("Confidence intervals based on the profile sum of squares",
      side = 3, outer = TRUE)
mtext("BOD data - confidence levels of 50%, 80%, 90% and 95%",
      side = 1, outer = TRUE)
par(opar)
\end{ExampleCode}
\end{Examples}
\HeaderA{plot.spec}{Plotting Spectral Densities}{plot.spec}
\methaliasA{plot.spec.coherency}{plot.spec}{plot.spec.coherency}
\methaliasA{plot.spec.phase}{plot.spec}{plot.spec.phase}
\keyword{hplot}{plot.spec}
\keyword{ts}{plot.spec}
%
\begin{Description}\relax
Plotting method for objects of class \code{"spec"}.  For multivariate
time series it plots the marginal spectra of the series or pairs plots
of the coherency and phase of the cross-spectra.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'spec'
plot(x, add = FALSE, ci = 0.95, log = c("yes", "dB", "no"),
     xlab = "frequency", ylab = NULL, type = "l",
     ci.col = "blue", ci.lty = 3, 
     main = NULL, sub = NULL,
     plot.type = c("marginal", "coherency", "phase"),
     ...)

plot.spec.phase(x, ci = 0.95,
                xlab = "frequency", ylab = "phase",
                ylim = c(-pi, pi), type = "l",
                main = NULL, ci.col = "blue", ci.lty = 3, ...) 

plot.spec.coherency(x, ci = 0.95,
                    xlab = "frequency",
                    ylab = "squared coherency",
                    ylim = c(0, 1), type = "l",
                    main = NULL, ci.col = "blue", ci.lty = 3, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object of class \code{"spec"}.
\item[\code{add}] logical.  If \code{TRUE}, add to already existing plot.
Only valid for \code{plot.type = "marginal"}.

\item[\code{ci}] coverage probability for confidence interval.  Plotting of
the confidence bar/limits is omitted unless \code{ci} is strictly
positive.
\item[\code{log}] If \code{"dB"}, plot on log10 (decibel) scale (as S-PLUS),
otherwise use conventional log scale or linear scale.  Logical
values are also accepted.  The default is \code{"yes"} unless
\code{options(ts.S.compat = TRUE)} has been set, when it is
\code{"dB"}.  Only valid for \code{plot.type = "marginal"}.

\item[\code{xlab}] the x label of the plot. 
\item[\code{ylab}] the y label of the plot.  If missing a suitable label will
be constructed.
\item[\code{type}] the type of plot to be drawn, defaults to lines.
\item[\code{ci.col}] colour for plotting confidence bar or confidence
intervals for coherency and phase.
\item[\code{ci.lty}] line type for confidence intervals for coherency and
phase.
\item[\code{main}] overall title for the plot. If missing, a suitable title
is constructed.
\item[\code{sub}] a sub title for the plot.  Only used for \code{plot.type =
      "marginal"}.  If missing, a description of the smoothing is used.
\item[\code{plot.type}] For multivariate time series, the type of plot
required.  Only the first character is needed.
\item[\code{ylim, ...}] Graphical parameters.
\end{ldescription}
\end{Arguments}
%
\begin{SeeAlso}\relax
\code{\LinkA{spectrum}{spectrum}}
\end{SeeAlso}
\HeaderA{plot.stepfun}{Plot Step Functions}{plot.stepfun}
\aliasA{lines.stepfun}{plot.stepfun}{lines.stepfun}
\keyword{hplot}{plot.stepfun}
%
\begin{Description}\relax
Method of the generic \code{\LinkA{plot}{plot}} for \code{\LinkA{stepfun}{stepfun}}
objects and utility for plotting piecewise constant functions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'stepfun'
plot(x, xval, xlim, ylim = range(c(y,Fn.kn)),
     xlab = "x", ylab = "f(x)", main = NULL,
     add = FALSE, verticals = TRUE, do.points = (n < 1000),
     pch = par("pch"), col = par("col"),
     col.points = col, cex.points = par("cex"),
     col.hor = col, col.vert = col,
     lty = par("lty"), lwd = par("lwd"), ...)

## S3 method for class 'stepfun'
lines(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an \R{} object inheriting from \code{"stepfun"}.
\item[\code{xval}] numeric vector of abscissa values at which to evaluate
\code{x}.  Defaults to \code{\LinkA{knots}{knots}(x)} restricted to \code{xlim}.
\item[\code{xlim, ylim}] limits for the plot region: see
\code{\LinkA{plot.window}{plot.window}}.  Both have sensible defaults if omitted.
\item[\code{xlab, ylab}] labels for x and y axis.
\item[\code{main}] main title.
\item[\code{add}] logical; if \code{TRUE} only \emph{add} to an existing plot.
\item[\code{verticals}] logical;  if \code{TRUE}, draw vertical lines at steps.
\item[\code{do.points}] logical;  if \code{TRUE}, also draw points at the
(\code{xlim} restricted) knot locations.  Default is true, for
sample size \eqn{< 1000}{}.
\item[\code{pch}] character; point character if \code{do.points}.
\item[\code{col}] default color of all points and lines.
\item[\code{col.points}] character or integer code; color of points if
\code{do.points}.
\item[\code{cex.points}] numeric; character expansion factor if \code{do.points}.
\item[\code{col.hor}] color of horizontal lines.
\item[\code{col.vert}] color of vertical lines.
\item[\code{lty, lwd}] line type and thickness for all lines.
\item[\code{...}] further arguments of \code{\LinkA{plot}{plot}(.)}, or if\code{(add)}
\code{\LinkA{segments}{segments}(.)}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with two components
\begin{ldescription}
\item[\code{t}] abscissa (x) values, including the two outermost ones.
\item[\code{y}] y values `in between' the \code{t[]}.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Martin Maechler \email{maechler@stat.math.ethz.ch}, 1990,
1993; ported to \R{}, 1997.
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{ecdf}{ecdf}} for empirical distribution functions as
special step functions,
\code{\LinkA{approxfun}{approxfun}} and \code{\LinkA{splinefun}{splinefun}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

y0 <- c(1,2,4,3)
sfun0  <- stepfun(1:3, y0, f = 0)
sfun.2 <- stepfun(1:3, y0, f = .2)
sfun1  <- stepfun(1:3, y0, right = TRUE)

tt <- seq(0,3, by=0.1)
op <- par(mfrow=c(2,2))
plot(sfun0); plot(sfun0, xval=tt, add=TRUE, col.hor="bisque")
plot(sfun.2);plot(sfun.2,xval=tt, add=TRUE, col = "orange")# all colors
plot(sfun1);lines(sfun1, xval=tt, col.hor="coral")
##-- This is  revealing :
plot(sfun0, verticals= FALSE,
     main = "stepfun(x, y0, f=f)  for f = 0, .2, 1")
for(i in 1:3)
  lines(list(sfun0,sfun.2,stepfun(1:3,y0,f = 1))[[i]], col=i)
legend(2.5, 1.9, paste("f =", c(0,0.2,1)), col=1:3, lty=1, y.intersp=1)
par(op)

# Extend and/or restrict 'viewport':
plot(sfun0, xlim = c(0,5), ylim = c(0, 3.5),
     main = "plot(stepfun(*), xlim= . , ylim = .)")

##-- this works too (automatic call to  ecdf(.)):
plot.stepfun(rt(50, df=3), col.vert = "gray20")
\end{ExampleCode}
\end{Examples}
\HeaderA{plot.ts}{Plotting Time-Series Objects}{plot.ts}
\aliasA{lines.ts}{plot.ts}{lines.ts}
\keyword{hplot}{plot.ts}
\keyword{ts}{plot.ts}
%
\begin{Description}\relax
Plotting method for objects inheriting from class \code{"ts"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'ts'
plot(x, y = NULL, plot.type = c("multiple", "single"),
        xy.labels, xy.lines, panel = lines, nc, yax.flip = FALSE,
        mar.multi = c(0, 5.1, 0, if(yax.flip) 5.1 else 2.1),
        oma.multi = c(6, 0, 5, 0), axes = TRUE, ...)

## S3 method for class 'ts'
lines(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] time series objects, usually inheriting from class \code{"ts"}.

\item[\code{plot.type}] for multivariate time series, should the series by
plotted separately (with a common time axis) or on a single plot?

\item[\code{xy.labels}] logical, indicating if \code{\LinkA{text}{text}()} labels
should be used for an x-y plot, \emph{or} character, supplying a
vector of labels to be used.  The default is to label for up to 150
points, and not for more.

\item[\code{xy.lines}] logical, indicating if \code{\LinkA{lines}{lines}}
should be drawn for an x-y plot.  Defaults to the value of
\code{xy.labels} if that is logical, otherwise to \code{TRUE}.

\item[\code{panel}] a \code{function(x, col, bg, pch, type, ...)} which gives the
action to be carried out in each panel of the display for
\code{plot.type="multiple"}.  The default is \code{lines}.

\item[\code{nc}] the number of columns to use when \code{type="multiple"}.
Defaults to 1 for up to 4 series, otherwise to 2.
\item[\code{yax.flip}] logical indicating if the y-axis (ticks and numbering)
should flip from side 2 (left) to 4 (right) from series to series
when \code{type="multiple"}.
\item[\code{mar.multi, oma.multi}] the (default) \code{\LinkA{par}{par}} settings
for \code{plot.type="multiple"}.  Modify with care!
\item[\code{axes}] logical indicating if x- and y- axes should be drawn.
\item[\code{...}] additional graphical arguments, see \code{\LinkA{plot}{plot}},
\code{\LinkA{plot.default}{plot.default}} and \code{\LinkA{par}{par}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{y} is missing, this function creates a time series
plot, for multivariate series of one of two kinds depending on
\code{plot.type}.

If \code{y} is present, both \code{x} and \code{y} must be univariate,
and a scatter plot \code{y \textasciitilde{} x} will be drawn, enhanced by
using \code{\LinkA{text}{text}} if \code{xy.labels} is
\code{\LinkA{TRUE}{TRUE}} or \code{character}, and \code{\LinkA{lines}{lines}} if
\code{xy.lines} is \code{TRUE}.
\end{Details}
%
\begin{SeeAlso}\relax
\code{\LinkA{ts}{ts}} for basic time series construction and access
functionality.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Multivariate
z <- ts(matrix(rt(200 * 8, df = 3), 200, 8),
        start = c(1961, 1), frequency = 12)
plot(z, yax.flip = TRUE)
plot(z, axes = FALSE, ann = FALSE, frame.plot = TRUE,
     mar.multi = c(0,0,0,0), oma.multi = c(1,1,5,1))
title("plot(ts(..), axes=FALSE, ann=FALSE, frame.plot=TRUE, mar..., oma...)")

z <- window(z[,1:3], end = c(1969,12))
plot(z, type = "b")    # multiple
plot(z, plot.type="single", lty=1:3, col=4:2)

## A phase plot:
plot(nhtemp, c(nhtemp[-1], NA), cex = .8, col="blue",
     main = "Lag plot of New Haven temperatures")
## a clearer way to do this would be
## Not run: 
plot(nhtemp, lag(nhtemp, 1), cex = .8, col="blue",
     main = "Lag plot of New Haven temperatures")

## End(Not run)

## xy.lines and xy.labels are FALSE for large series:
plot(lag(sunspots, 1), sunspots, pch = ".")

SMI <- EuStockMarkets[, "SMI"]
plot(lag(SMI,  1), SMI, pch = ".")
plot(lag(SMI, 20), SMI, pch = ".", log = "xy",
     main = "4 weeks lagged SMI stocks -- log scale", xy.lines= TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{Poisson}{The Poisson Distribution}{Poisson}
\aliasA{dpois}{Poisson}{dpois}
\aliasA{ppois}{Poisson}{ppois}
\aliasA{qpois}{Poisson}{qpois}
\aliasA{rpois}{Poisson}{rpois}
\keyword{distribution}{Poisson}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the Poisson distribution with parameter \code{lambda}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dpois(x, lambda, log = FALSE)
ppois(q, lambda, lower.tail = TRUE, log.p = FALSE)
qpois(p, lambda, lower.tail = TRUE, log.p = FALSE)
rpois(n, lambda)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] vector of (non-negative integer) quantiles.
\item[\code{q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of random values to return.
\item[\code{lambda}] vector of (non-negative) means.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The Poisson distribution has density

\deqn{p(x) = \frac{\lambda^x e^{-\lambda}}{x!}}{}
for \eqn{x = 0, 1, 2, \ldots}{} .
The mean and variance are \eqn{E(X) = Var(X) = \lambda}{}.

If an element of \code{x} is not integer, the result of \code{dpois}
is zero, with a warning.
\eqn{p(x)}{} is computed using Loader's algorithm, see the reference in
\code{\LinkA{dbinom}{dbinom}}.

The quantile is right continuous: \code{qpois(p, lambda)} is the smallest
integer \eqn{x}{} such that \eqn{P(X \le x) \ge p}{}.

Setting \code{lower.tail = FALSE} allows to get much more precise
results when the default, \code{lower.tail = TRUE} would return 1, see
the example below.
\end{Details}
%
\begin{Value}
\code{dpois} gives the (log) density,
\code{ppois} gives the (log) distribution function,
\code{qpois} gives the quantile function, and
\code{rpois} generates random deviates.

Invalid \code{lambda} will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Source}\relax
\code{dpois} uses C code contributed by Catherine Loader
(see \code{\LinkA{dbinom}{dbinom}}).

\code{ppois} uses \code{pgamma}.

\code{qpois} uses the Cornish--Fisher Expansion to include a skewness
correction to a normal approximation, followed by a search.

\code{rpois} uses

Ahrens, J. H. and Dieter, U. (1982).
Computer generation of Poisson deviates from modified normal distributions.
\emph{ACM Transactions on Mathematical Software}, \bold{8}, 163--179.
\end{Source}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{dbinom}{dbinom}} for the binomial and \code{\LinkA{dnbinom}{dnbinom}} for
the negative binomial distribution.

\code{\LinkA{poisson.test}{poisson.test}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

-log(dpois(0:7, lambda=1) * gamma(1+ 0:7)) # == 1
Ni <- rpois(50, lambda = 4); table(factor(Ni, 0:max(Ni)))

1 - ppois(10*(15:25), lambda=100)  # becomes 0 (cancellation)
    ppois(10*(15:25), lambda=100, lower.tail=FALSE)  # no cancellation

par(mfrow = c(2, 1))
x <- seq(-0.01, 5, 0.01)
plot(x, ppois(x, 1), type="s", ylab="F(x)", main="Poisson(1) CDF")
plot(x, pbinom(x, 100, 0.01),type="s", ylab="F(x)",
     main="Binomial(100, 0.01) CDF")
\end{ExampleCode}
\end{Examples}
\HeaderA{poisson.test}{Exact Poisson tests}{poisson.test}
\keyword{htest}{poisson.test}
%
\begin{Description}\relax
Performs an exact test of a simple null hypothesis about the
rate parameter in Poisson distribution, or for the
ratio between two rate parameters.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
poisson.test(x, T = 1, r = 1,
    alternative = c("two.sided", "less", "greater"),
    conf.level = 0.95)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] number of events. A vector of length one or two.
\item[\code{T}] time base for event count. A vector of length one or two. 
\item[\code{r}] hypothesized rate or rate ratio
\item[\code{alternative}] indicates the alternative hypothesis and must be
one of \code{"two.sided"}, \code{"greater"} or \code{"less"}.
You can specify just the initial letter.
\item[\code{conf.level}] confidence level for the returned confidence
interval.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Confidence intervals are computed similarly to those of
\code{\LinkA{binom.test}{binom.test}} in the one-sample case, and using
\code{\LinkA{binom.test}{binom.test}} in the two sample case.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the number of events (in the first sample if there
are two.)
\item[\code{parameter}] the corresponding expected count
\item[\code{p.value}] the p-value of the test.
\item[\code{conf.int}] a confidence interval for the rate or rate ratio.
\item[\code{estimate}] the estimated rate or rate ratio.
\item[\code{null.value}] the rate or rate ratio under the null,
\code{r}.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] the character string \code{"Exact Poisson test"} or
\code{"Comparison of Poisson rates"} as appropriate.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The rate parameter in Poisson data is often given based on a
``time on test'' or similar quantity (person-years, population
size, or expected number of cases from mortality tables). This is the
role of the \code{T} argument.

The one-sample case is effectively the binomial test with a very large
\code{n}. The two sample case is converted to a binomial test by
conditioning on the total event count, and the rate ratio is directly
related to the odds in that binomial distribution.  
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{binom.test}{binom.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
### These are paraphrased from data sets in the ISwR package

## SMR, Welsh Nickel workers
poisson.test(137, 24.19893)

## eba1977, compare Fredericia to other three cities for ages 55-59
poisson.test(c(11,6+8+7),c(800, 1083+1050+878))
\end{ExampleCode}
\end{Examples}
\HeaderA{poly}{Compute Orthogonal Polynomials}{poly}
\aliasA{makepredictcall.poly}{poly}{makepredictcall.poly}
\aliasA{polym}{poly}{polym}
\aliasA{predict.poly}{poly}{predict.poly}
\keyword{math}{poly}
%
\begin{Description}\relax
Returns or evaluates orthogonal polynomials of degree 1 to
\code{degree} over the specified set of points \code{x}. These are all
orthogonal to the constant polynomial of degree 0.  Alternatively,
evaluate raw polynomials.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
poly(x, ..., degree = 1, coefs = NULL, raw = FALSE)
polym(..., degree = 1, raw = FALSE)

## S3 method for class 'poly'
predict(object, newdata, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, newdata}] a numeric vector at which to evaluate the
polynomial. \code{x} can also be a matrix.  Missing values are not
allowed in \code{x}.
\item[\code{degree}] the degree of the polynomial.  Must be less than the
number of unique points if \code{raw = TRUE}.
\item[\code{coefs}] for prediction, coefficients from a previous fit.
\item[\code{raw}] if true, use raw and not orthogonal polynomials.
\item[\code{object}] an object inheriting from class \code{"poly"}, normally
the result of a call to \code{poly} with a single vector argument.
\item[\code{...}] \code{poly, polym}: further vectors.\\{}
\code{predict.poly}: arguments to be passed to or from other methods.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Although formally \code{degree} should be named (as it follows
\code{...}), an unnamed second argument of length 1 will be
interpreted as the degree.

The orthogonal polynomial is summarized by the coefficients, which can
be used to evaluate it via the three-term recursion given in Kennedy
\& Gentle (1980, pp. 343--4), and used in the \code{predict} part of
the code.
\end{Details}
%
\begin{Value}
For \code{poly} with a single vector argument:\\{}
A matrix with rows corresponding to points in \code{x} and columns
corresponding to the degree, with attributes \code{"degree"} specifying
the degrees of the columns and (unless \code{raw = TRUE})
\code{"coefs"} which contains the centering and normalization
constants used in constructing the orthogonal polynomials.  The matrix
has given class \code{c("poly", "matrix")}.

Other cases of \code{poly} and \code{polym}, and \code{predict.poly}:
a matrix.
\end{Value}
%
\begin{Note}\relax
This routine is intended for statistical purposes such as
\code{contr.poly}: it does not attempt to orthogonalize to
machine accuracy.
\end{Note}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}.
Wadsworth \& Brooks/Cole.

Kennedy, W. J. Jr and Gentle, J. E. (1980)
\emph{Statistical Computing} Marcel Dekker.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{contr.poly}{contr.poly}}.

\code{\LinkA{cars}{cars}} for an example of polynomial regression.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
od <- options(digits=3) # avoid too much visual clutter
(z <- poly(1:10, 3))
predict(z, seq(2, 4, 0.5))
zapsmall(poly(seq(4, 6, 0.5), 3, coefs = attr(z, "coefs")))

zapsmall(polym(1:4, c(1, 4:6), degree=3)) # or just poly()
zapsmall(poly(cbind(1:4, c(1, 4:6)), degree=3))
options(od)
\end{ExampleCode}
\end{Examples}
\HeaderA{power}{Create a Power Link Object}{power}
\keyword{models}{power}
%
\begin{Description}\relax
Creates a link object based on the link function
\eqn{\eta = \mu ^ \lambda}{}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
power(lambda = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{lambda}] a real number.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{lambda} is non-positive, it is taken as zero, and the log
link is obtained.  The default \code{lambda = 1} gives the identity
link.
\end{Details}
%
\begin{Value}
A list with components \code{linkfun}, \code{linkinv},
\code{mu.eta}, and \code{valideta}.
See \code{\LinkA{make.link}{make.link}} for information on their meaning.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S.}
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{make.link}{make.link}},
\code{\LinkA{family}{family}}

To raise a number to a power, see \code{\LinkA{Arithmetic}{Arithmetic}}.

To calculate the power of a test, see various functions in the 
\pkg{stats} package, e.g., \code{\LinkA{power.t.test}{power.t.test}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
power()
quasi(link=power(1/3))[c("linkfun", "linkinv")]
\end{ExampleCode}
\end{Examples}
\HeaderA{power.anova.test}{Power Calculations for Balanced One-Way Analysis of Variance Tests}{power.anova.test}
\keyword{htest}{power.anova.test}
%
\begin{Description}\relax
Compute power of test or determine parameters to obtain target power.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
power.anova.test(groups = NULL, n = NULL,
                 between.var = NULL, within.var = NULL,
                 sig.level = 0.05, power = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{groups}] Number of groups
\item[\code{n}] Number of observations (per group)
\item[\code{between.var}] Between group variance
\item[\code{within.var}] Within group variance
\item[\code{sig.level}] Significance level (Type I error probability)
\item[\code{power}] Power of test (1 minus Type II error probability)
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Exactly one of the parameters \code{groups}, \code{n}, \code{between.var},
\code{power}, \code{within.var}, and \code{sig.level} must be passed as NULL,
and that parameter is determined from the others. Notice that
\code{sig.level} has non-NULL default so NULL must be explicitly
passed if you want it computed.
\end{Details}
%
\begin{Value}
Object of class \code{"power.htest"}, a list of the arguments
(including the computed one) augmented with \code{method} and
\code{note} elements. 
\end{Value}
%
\begin{Note}\relax
\code{uniroot} is used to solve power equation for unknowns, so
you may see errors from it, notably about inability to bracket the
root when invalid arguments are given.
\end{Note}
%
\begin{Author}\relax
Claus EkstrÃ¸m
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{anova}{anova}}, \code{\LinkA{lm}{lm}}, \code{\LinkA{uniroot}{uniroot}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
power.anova.test(groups=4, n=5, between.var=1, within.var=3)
# Power = 0.3535594

power.anova.test(groups=4, between.var=1, within.var=3,
                 power=.80)
# n = 11.92613

## Assume we have prior knowledge of the group means:
groupmeans <- c(120, 130, 140, 150)
power.anova.test(groups = length(groupmeans),
                 between.var=var(groupmeans),
                 within.var=500, power=.90) # n = 15.18834

\end{ExampleCode}
\end{Examples}
\HeaderA{power.prop.test}{Power Calculations for Two-Sample Test for Proportions}{power.prop.test}
\keyword{htest}{power.prop.test}
%
\begin{Description}\relax
Compute power of test, or determine parameters to obtain target power.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
power.prop.test(n = NULL, p1 = NULL, p2 = NULL, sig.level = 0.05,
                power = NULL,
                alternative = c("two.sided", "one.sided"),
                strict = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] Number of observations (per group)
\item[\code{p1}] probability in one group
\item[\code{p2}] probability in other group
\item[\code{sig.level}] Significance level (Type I error probability)
\item[\code{power}] Power of test (1 minus Type II error probability)
\item[\code{alternative}] One- or two-sided test
\item[\code{strict}] Use strict interpretation in two-sided case
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Exactly one of the parameters \code{n}, \code{p1}, \code{p2},
\code{power}, and \code{sig.level} must be passed as NULL, and that
parameter is determined from the others. Notice that \code{sig.level}
has a non-NULL default so NULL must be explicitly passed if you want
it  computed.

If \code{strict = TRUE} is used, the power will include the probability of
rejection in the opposite direction of the true effect, in the two-sided 
case. Without this the power will be half the significance level if the 
true difference is zero.
\end{Details}
%
\begin{Value}
Object of class \code{"power.htest"}, a list of the arguments
(including the computed one) augmented with \code{method} and
\code{note} elements. 
\end{Value}
%
\begin{Note}\relax
\code{uniroot} is used to solve power equation for unknowns, so
you may see errors from it, notably about inability to bracket the
root when invalid arguments are given. If one of them is computed
\code{p1 < p2} will hold, although this is not enforced when both are
specified.
\end{Note}
%
\begin{Author}\relax
Peter Dalgaard.  Based on previous work by Claus
EkstrÃ¸m
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{prop.test}{prop.test}}, \code{\LinkA{uniroot}{uniroot}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
 power.prop.test(n = 50, p1 = .50, p2 = .75)
 power.prop.test(p1 = .50, p2 = .75, power = .90)
 power.prop.test(n = 50, p1 = .5, power = .90)
\end{ExampleCode}
\end{Examples}
\HeaderA{power.t.test}{Power calculations for one and two sample t tests}{power.t.test}
\keyword{htest}{power.t.test}
%
\begin{Description}\relax
Compute power of test, or determine parameters to obtain target power.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05,
             power = NULL,
             type = c("two.sample", "one.sample", "paired"),
             alternative = c("two.sided", "one.sided"),
             strict = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] Number of observations (per group)
\item[\code{delta}] True difference in means
\item[\code{sd}] Standard deviation
\item[\code{sig.level}] Significance level (Type I error probability)
\item[\code{power}] Power of test (1 minus Type II error probability)
\item[\code{type}] Type of t test
\item[\code{alternative}] One- or two-sided test
\item[\code{strict}] Use strict interpretation in two-sided case
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Exactly one of the parameters \code{n}, \code{delta}, \code{power},
\code{sd}, and \code{sig.level} must be passed as NULL, and that
parameter is determined from the others. Notice that the last two have
non-NULL defaults so NULL must be explicitly passed if you want to
compute them.

If \code{strict = TRUE} is used, the power will include the probability of
rejection in the opposite direction of the true effect, in the two-sided 
case. Without this the power will be half the significance level if the 
true difference is zero.
\end{Details}
%
\begin{Value}
Object of class \code{"power.htest"}, a list of the arguments
(including the computed one) augmented with \code{method} and
\code{note} elements. 
\end{Value}
%
\begin{Note}\relax
\code{uniroot} is used to solve power equation for unknowns, so
you may see errors from it, notably about inability to bracket the
root when invalid arguments are given.
\end{Note}
%
\begin{Author}\relax
Peter Dalgaard.  Based on previous work by Claus
EkstrÃ¸m
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{t.test}{t.test}}, \code{\LinkA{uniroot}{uniroot}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
 power.t.test(n = 20, delta = 1)
 power.t.test(power = .90, delta = 1)
 power.t.test(power = .90, delta = 1, alternative = "one.sided")
\end{ExampleCode}
\end{Examples}
\HeaderA{PP.test}{Phillips-Perron Test for Unit Roots}{PP.test}
\keyword{ts}{PP.test}
%
\begin{Description}\relax
Computes the Phillips-Perron test for the null hypothesis that
\code{x} has a unit root against a stationary alternative.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
PP.test(x, lshort = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector or univariate time series.
\item[\code{lshort}] a logical indicating whether the short or long version
of the truncation lag parameter is used.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The general regression equation which incorporates a constant and a
linear trend is used and the corrected t-statistic for a first order
autoregressive coefficient equals one is computed.  To estimate
\code{sigma\textasciicircum{}2} the Newey-West estimator is used.  If \code{lshort}
is \code{TRUE}, then the truncation lag parameter is set to
\code{trunc(4*(n/100)\textasciicircum{}0.25)}, otherwise
\code{trunc(12*(n/100)\textasciicircum{}0.25)} is used.  The p-values are
interpolated from Table 4.2, page 103 of Banerjee \emph{et al.}
(1993).

Missing values are not handled.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the test statistic.
\item[\code{parameter}] the truncation lag parameter.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] a character string indicating what type of test was
performed.
\item[\code{data.name}] a character string giving the name of the data.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
A. Trapletti
\end{Author}
%
\begin{References}\relax
A. Banerjee, J. J. Dolado, J. W. Galbraith, and D. F. Hendry (1993)
\emph{Cointegration, Error Correction, and the Econometric Analysis
of Non-Stationary Data}, Oxford University Press, Oxford.

P. Perron (1988) Trends and random walks in macroeconomic time
series. \emph{Journal of Economic Dynamics and Control} \bold{12},
297--332.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
x <- rnorm(1000)
PP.test(x)
y <- cumsum(x) # has unit root
PP.test(y)
\end{ExampleCode}
\end{Examples}
\HeaderA{ppoints}{Ordinates for Probability Plotting}{ppoints}
\keyword{dplot}{ppoints}
\keyword{arith}{ppoints}
\keyword{distribution}{ppoints}
%
\begin{Description}\relax
Generates the sequence of probability points
\code{(1:m - a)/(m + (1-a)-a)}
where \code{m} is either \code{n}, if \code{length(n)==1}, or
\code{length(n)}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ppoints(n, a = ifelse(n <= 10, 3/8, 1/2))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] either the number of points generated or a vector of
observations.
\item[\code{a}] the offset fraction to be used; typically in \eqn{(0,1)}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \eqn{0 < a < 1}{}, the resulting values are within \eqn{(0,1)}{}
(excluding boundaries).
In any case, the resulting sequence is symmetric in \eqn{[0,1]}{}, i.e.,
\code{p + rev(p) == 1}.

\code{ppoints()} is used in \code{qqplot} and \code{qqnorm} to generate
the set of probabilities at which to evaluate the inverse distribution.

The choice of \code{a} follows the documentation of the function of the
same name in Becker \emph{et al} (1988), and appears to have been
motivated by results from Blom (1958) on approximations to expect normal
order statistics (see also \code{\LinkA{quantile}{quantile}}).
\end{Details}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Blom, G. (1958)
\emph{Statistical Estimates and Transformed Beta Variables.}
Wiley
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{qqplot}{qqplot}}, \code{\LinkA{qqnorm}{qqnorm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ppoints(4) # the same as  ppoints(1:4)
ppoints(10)
ppoints(10, a=1/2)
\end{ExampleCode}
\end{Examples}
\HeaderA{ppr}{Projection Pursuit Regression}{ppr}
\methaliasA{ppr.default}{ppr}{ppr.default}
\methaliasA{ppr.formula}{ppr}{ppr.formula}
\keyword{regression}{ppr}
%
\begin{Description}\relax
Fit a projection pursuit regression model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ppr(x, ...)

## S3 method for class 'formula'
ppr(formula, data, weights, subset, na.action,
    contrasts = NULL, ..., model = FALSE)

## Default S3 method:
ppr(x, y, weights = rep(1,n),
    ww = rep(1,q), nterms, max.terms = nterms, optlevel = 2,
    sm.method = c("supsmu", "spline", "gcvspline"),
    bass = 0, span = 0, df = 5, gcvpen = 1, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] 
a formula specifying one or more numeric response variables and the
explanatory variables.

\item[\code{x}] 
numeric matrix of explanatory variables.  Rows represent observations, and
columns represent variables.  Missing values are not accepted.

\item[\code{y}] 
numeric matrix of response variables.  Rows represent observations, and
columns represent variables.  Missing values are not accepted.

\item[\code{nterms}] number of terms to include in the final model.
\item[\code{data}] 
a data frame (or similar: see \code{\LinkA{model.frame}{model.frame}}) from which
variables specified in \code{formula} are preferentially to be taken.

\item[\code{weights}] a vector of weights \code{w\_i} for each \emph{case}.
\item[\code{ww}] 
a vector of weights for each \emph{response}, so the fit criterion is
the sum over case \code{i} and responses \code{j} of
\code{w\_i ww\_j (y\_ij - fit\_ij)\textasciicircum{}2} divided by the sum of \code{w\_i}.

\item[\code{subset}] 
an index vector specifying the cases to be used in the training
sample.  (NOTE: If given, this argument must be named.)

\item[\code{na.action}] 
a function to specify the action to be taken if \code{\LinkA{NA}{NA}}s are
found. The default action is given by \code{getOption("na.action")}.
(NOTE: If given, this argument must be named.)

\item[\code{contrasts}] 
the contrasts to be used when any factor explanatory variables are coded.

\item[\code{max.terms}] 
maximum number of terms to choose from when building the model.

\item[\code{optlevel}] 
integer from 0 to 3 which determines the thoroughness of an
optimization routine in the SMART program. See the `Details'
section.

\item[\code{sm.method}] 
the method used for smoothing the ridge functions.  The default is to
use Friedman's super smoother \code{supsmu}.  The alternatives are to use
the smoothing spline code underlying \code{smooth.spline}, either with a
specified (equivalent) degrees of freedom for each ridge functions, or
to allow the smoothness to be chosen by GCV.

\item[\code{bass}] 
super smoother bass tone control used with automatic span selection
(see \code{supsmu}); the range of values is 0 to 10, with larger values
resulting in increased smoothing.

\item[\code{span}] 
super smoother span control (see \code{supsmu}).  The default, \code{0},
results in automatic span selection by local cross validation. \code{span}
can also take a value in \code{(0, 1]}.

\item[\code{df}] 
if \code{sm.method} is \code{"spline"} specifies the smoothness of
each ridge term via the requested equivalent degrees of freedom.

\item[\code{gcvpen}] 
if \code{sm.method} is \code{"gcvspline"} this is the penalty used
in the GCV selection for each degree of freedom used.

\item[\code{...}] arguments to be passed to or from other methods.
\item[\code{model}] logical.  If true, the model frame is returned.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The basic method is given by Friedman (1984), and is essentially the
same code used by S-PLUS's \code{ppreg}.  This code is extremely
sensitive to the compiler used.

The algorithm first adds up to \code{max.terms} ridge terms one at a
time; it will use less if it is unable to find a term to add that makes
sufficient difference.  It then removes the least
important term at each step until \code{nterms} terms
are left.

The levels of optimization (argument \code{optlevel})
differ in how thoroughly the models are refitted during this process.
At level 0 the existing ridge terms are not refitted.  At level 1
the projection directions are not refitted, but the ridge
functions and the regression coefficients are.

Levels 2 and 3 refit all the terms and are equivalent for one
response; level 3 is more careful to re-balance the contributions
from each regressor at each step and so is a little less likely to
converge to a saddle point of the sum of squares criterion.
\end{Details}
%
\begin{Value}
A list with the following components, many of which are for use by the
method functions.

\begin{ldescription}
\item[\code{call}] the matched call
\item[\code{p}] the number of explanatory variables (after any coding)
\item[\code{q}] the number of response variables
\item[\code{mu}] the argument \code{nterms}
\item[\code{ml}] the argument \code{max.terms}
\item[\code{gof}] the overall residual (weighted) sum of squares for the
selected model
\item[\code{gofn}] the overall residual (weighted) sum of squares against the
number of terms, up to \code{max.terms}.  Will be invalid (and zero)
for less than \code{nterms}.
\item[\code{df}] the argument \code{df}
\item[\code{edf}] if \code{sm.method} is \code{"spline"} or \code{"gcvspline"}
the equivalent number of degrees of freedom for each ridge term used.
\item[\code{xnames}] the names of the explanatory variables
\item[\code{ynames}] the names of the response variables
\item[\code{alpha}] a matrix of the projection directions, with a column for
each ridge term
\item[\code{beta}] a matrix of the coefficients applied for each response to
the ridge terms: the rows are the responses and the columns the ridge terms
\item[\code{yb}] the weighted means of each response
\item[\code{ys}] the overall scale factor used: internally the responses are
divided by \code{ys} to have unit total weighted sum of squares.
\item[\code{fitted.values}] the fitted values, as a matrix if \code{q > 1}.
\item[\code{residuals}] the residuals, as a matrix if \code{q > 1}.
\item[\code{smod}] internal work array, which includes the ridge functions
evaluated at the training set points.
\item[\code{model}] (only if \code{model=TRUE}) the model frame.
\end{ldescription}
\end{Value}
%
\begin{Source}\relax
Friedman (1984): converted to double precision and added interface to
smoothing splines by B. D. Ripley, originally for the \pkg{MASS} package.
\end{Source}
%
\begin{References}\relax
Friedman, J. H. and Stuetzle, W. (1981)
Projection pursuit regression.
\emph{Journal of the American Statistical Association},
\bold{76}, 817--823.

Friedman, J. H. (1984)
SMART User's Guide.
Laboratory for Computational Statistics, Stanford University Technical
Report No. 1.

Venables, W. N. and Ripley, B. D. (2002)
\emph{Modern Applied Statistics with S.}  Springer.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{plot.ppr}{plot.ppr}}, \code{\LinkA{supsmu}{supsmu}}, \code{\LinkA{smooth.spline}{smooth.spline}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

# Note: your numerical values may differ
attach(rock)
area1 <- area/10000; peri1 <- peri/10000
rock.ppr <- ppr(log(perm) ~ area1 + peri1 + shape,
                data = rock, nterms = 2, max.terms = 5)
rock.ppr
# Call:
# ppr.formula(formula = log(perm) ~ area1 + peri1 + shape, data = rock,
#     nterms = 2, max.terms = 5)
#
# Goodness of fit:
#  2 terms  3 terms  4 terms  5 terms
# 8.737806 5.289517 4.745799 4.490378

summary(rock.ppr)
# .....  (same as above)
# .....
#
# Projection direction vectors:
#       term 1      term 2
# area1  0.34357179  0.37071027
# peri1 -0.93781471 -0.61923542
# shape  0.04961846  0.69218595
#
# Coefficients of ridge terms:
#    term 1    term 2
# 1.6079271 0.5460971

par(mfrow=c(3,2))# maybe: , pty="s")
plot(rock.ppr, main="ppr(log(perm)~ ., nterms=2, max.terms=5)")
plot(update(rock.ppr, bass=5), main = "update(..., bass = 5)")
plot(update(rock.ppr, sm.method="gcv", gcvpen=2),
     main = "update(..., sm.method=\"gcv\", gcvpen=2)")
cbind(perm=rock$perm, prediction=round(exp(predict(rock.ppr)), 1))
detach()
\end{ExampleCode}
\end{Examples}
\HeaderA{prcomp}{Principal Components Analysis}{prcomp}
\aliasA{plot.prcomp}{prcomp}{plot.prcomp}
\methaliasA{prcomp.default}{prcomp}{prcomp.default}
\methaliasA{prcomp.formula}{prcomp}{prcomp.formula}
\aliasA{predict.prcomp}{prcomp}{predict.prcomp}
\aliasA{print.prcomp}{prcomp}{print.prcomp}
\aliasA{print.summary.prcomp}{prcomp}{print.summary.prcomp}
\aliasA{summary.prcomp}{prcomp}{summary.prcomp}
\keyword{multivariate}{prcomp}
%
\begin{Description}\relax
Performs a principal components analysis on the given data matrix
and returns the results as an object of class \code{prcomp}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
prcomp(x, ...)

## S3 method for class 'formula'
prcomp(formula, data = NULL, subset, na.action, ...)

## Default S3 method:
prcomp(x, retx = TRUE, center = TRUE, scale. = FALSE,
       tol = NULL, ...)

## S3 method for class 'prcomp'
predict(object, newdata, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a formula with no response variable, referring only to
numeric variables.
\item[\code{data}] an optional data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector used to select rows (observations) of the
data matrix \code{x}.
\item[\code{na.action}] a function which indicates what should happen
when the data contain \code{NA}s.  The default is set by
the \code{na.action} setting of \code{\LinkA{options}{options}}, and is
\code{\LinkA{na.fail}{na.fail}} if that is unset. The `factory-fresh'
default is \code{\LinkA{na.omit}{na.omit}}.
\item[\code{...}] arguments passed to or from other methods. If \code{x} is
a formula one might specify \code{scale.} or \code{tol}.
\item[\code{x}] a numeric or complex matrix (or data frame) which provides
the data for the principal components analysis.
\item[\code{retx}] a logical value indicating whether the rotated variables
should be returned.
\item[\code{center}] a logical value indicating whether the variables
should be shifted to be zero centered. Alternately, a vector of
length equal the number of columns of \code{x} can be supplied.
The value is passed to \code{scale}.
\item[\code{scale.}] a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes
place. The default is \code{FALSE} for consistency with S, but
in general scaling is advisable.  Alternatively, a vector of length
equal the number of columns of \code{x} can be supplied.  The
value is passed to \code{\LinkA{scale}{scale}}.
\item[\code{tol}] a value indicating the magnitude below which components
should be omitted. (Components are omitted if their
standard deviations are less than or equal to \code{tol} times the
standard deviation of the first component.)
With the default null setting, no components
are omitted.  Other settings for tol could be \code{tol = 0} or
\code{tol = sqrt(.Machine\$double.eps)}, which would omit
essentially constant components.
\item[\code{object}] Object of class inheriting from \code{"prcomp"}
\item[\code{newdata}] An optional data frame or matrix in which to look for
variables with which to predict.  If omitted, the scores are used.
If the original fit used a formula or a data frame or a matrix with
column names, \code{newdata} must contain columns with the same
names. Otherwise it must contain the same number of columns, to be
used in the same order.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The calculation is done by a singular value decomposition of the
(centered and possibly scaled) data matrix, not by using
\code{eigen} on the covariance matrix.  This
is generally the preferred method for numerical accuracy.  The
\code{print} method for these objects prints the results in a nice
format and the \code{plot} method produces a scree plot.

Unlike \code{\LinkA{princomp}{princomp}}, variances are computed with the usual
divisor \eqn{N - 1}{}.

Note that \code{scale = TRUE} cannot be used if there are zero or
constant (for \code{center = TRUE}) variables.
\end{Details}
%
\begin{Value}
\code{prcomp} returns a list with class \code{"prcomp"}
containing the following components:
\begin{ldescription}
\item[\code{sdev}] the standard deviations of the principal components
(i.e., the square roots of the eigenvalues of the
covariance/correlation matrix, though the calculation
is actually done with the singular values of the data matrix).
\item[\code{rotation}] the matrix of variable loadings (i.e., a matrix
whose columns contain the eigenvectors).  The function
\code{princomp} returns this in the element \code{loadings}.
\item[\code{x}] if \code{retx} is true the value of the rotated data (the
centred (and scaled if requested) data multiplied by the
\code{rotation} matrix) is returned.  Hence, \code{cov(x)} is the
diagonal matrix \code{diag(sdev\textasciicircum{}2)}.  For the formula method,
\code{\LinkA{napredict}{napredict}()} is applied to handle the treatment of values
omitted by the \code{na.action}.
\item[\code{center, scale}] the centering and scaling used, or \code{FALSE}.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The signs of the columns of the rotation matrix are arbitrary, and
so may differ between different programs for PCA, and even between
different builds of \R{}.
\end{Note}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Mardia, K. V., J. T. Kent, and J. M. Bibby (1979)
\emph{Multivariate Analysis}, London: Academic Press.

Venables, W. N. and B. D. Ripley (2002)
\emph{Modern Applied Statistics with S}, Springer-Verlag.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{biplot.prcomp}{biplot.prcomp}}, \code{\LinkA{screeplot}{screeplot}},
\code{\LinkA{princomp}{princomp}}, \code{\LinkA{cor}{cor}}, \code{\LinkA{cov}{cov}},
\code{\LinkA{svd}{svd}}, \code{\LinkA{eigen}{eigen}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## signs are random
require(graphics)

## the variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
prcomp(USArrests)  # inappropriate
prcomp(USArrests, scale = TRUE)
prcomp(~ Murder + Assault + Rape, data = USArrests, scale = TRUE)
plot(prcomp(USArrests))
summary(prcomp(USArrests, scale = TRUE))
biplot(prcomp(USArrests, scale = TRUE))
\end{ExampleCode}
\end{Examples}
\HeaderA{predict}{Model Predictions}{predict}
\keyword{methods}{predict}
%
\begin{Description}\relax
\code{predict} is a generic function for predictions from the results of
various model fitting functions.  The function invokes particular
\emph{methods} which depend on the \code{\LinkA{class}{class}} of
the first argument.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
predict (object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a model object for which prediction is desired.
\item[\code{...}] additional arguments affecting the predictions produced.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Most prediction methods which are similar to those for linear models
have an argument \code{newdata} specifying the first place to look for
explanatory variables to be used for prediction.  Some considerable
attempts are made to match up the columns in \code{newdata} to those
used for fitting, for example that they are of comparable types and
that any factors have the same level set in the same order (or can be
transformed to be so).

Time series prediction methods in package \pkg{stats} have an argument
\code{n.ahead} specifying how many time steps ahead to predict.

Many methods have a logical argument \code{se.fit} saying if standard
errors are to returned.
\end{Details}
%
\begin{Value}
The form of the value returned by \code{predict} depends on the
class of its argument.  See the documentation of the
particular methods for details of what is produced by that method.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{predict.glm}{predict.glm}},
\code{\LinkA{predict.lm}{predict.lm}},
\code{\LinkA{predict.loess}{predict.loess}},
\code{\LinkA{predict.nls}{predict.nls}},
\code{\LinkA{predict.poly}{predict.poly}},
\code{\LinkA{predict.princomp}{predict.princomp}},
\code{\LinkA{predict.smooth.spline}{predict.smooth.spline}}.

\LinkA{SafePrediction}{SafePrediction} for prediction from polynomial and spline fits.

For time-series prediction,
\code{\LinkA{predict.ar}{predict.ar}},
\code{\LinkA{predict.Arima}{predict.Arima}},
\code{\LinkA{predict.arima0}{predict.arima0}},
\code{\LinkA{predict.HoltWinters}{predict.HoltWinters}},
\code{\LinkA{predict.StructTS}{predict.StructTS}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(utils)

## All the "predict" methods found
## NB most of the methods in the standard packages are hidden.
for(fn in methods("predict"))
   try({
       f <- eval(substitute(getAnywhere(fn)$objs[[1]], list(fn = fn)))
       cat(fn, ":\n\t", deparse(args(f)), "\n")
       }, silent = TRUE)

\end{ExampleCode}
\end{Examples}
\HeaderA{predict.Arima}{Forecast from ARIMA fits}{predict.Arima}
\keyword{ts}{predict.Arima}
%
\begin{Description}\relax
Forecast from models fitted by \code{\LinkA{arima}{arima}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'Arima'
predict(object, n.ahead = 1, newxreg = NULL,
        se.fit = TRUE, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] The result of an \code{arima} fit.
\item[\code{n.ahead}] The number of steps ahead for which prediction is required.
\item[\code{newxreg}] New values of \code{xreg} to be used for
prediction. Must have at least \code{n.ahead} rows.
\item[\code{se.fit}] Logical: should standard errors of prediction be returned?
\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Finite-history prediction is used, via \code{\LinkA{KalmanForecast}{KalmanForecast}}.
This is only statistically efficient if the MA part of the fit is
invertible, so \code{predict.Arima} will give a warning for
non-invertible MA models.

The standard errors of prediction exclude the uncertainty in the
estimation of the ARMA model and the regression coefficients.
According to Harvey (1993, pp. 58--9) the effect is small.
\end{Details}
%
\begin{Value}
A time series of predictions, or if \code{se.fit = TRUE}, a list
with components \code{pred}, the predictions, and \code{se},
the estimated standard errors.  Both components are time series.
\end{Value}
%
\begin{References}\relax
Durbin, J. and Koopman, S. J. (2001) \emph{Time Series Analysis by
State Space Methods.}  Oxford University Press.

Harvey, A. C. and McKenzie, C. R. (1982) Algorithm AS182.
An algorithm for finite sample prediction from ARIMA processes.
\emph{Applied Statistics} \bold{31}, 180--187.

Harvey, A. C. (1993) \emph{Time Series Models},
2nd Edition, Harvester Wheatsheaf, sections 3.3 and 4.4.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{arima}{arima}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
od <- options(digits=5) # avoid too much spurious accuracy
predict(arima(lh, order = c(3,0,0)), n.ahead = 12)

(fit <- arima(USAccDeaths, order = c(0,1,1),
              seasonal = list(order=c(0,1,1))))
predict(fit, n.ahead = 6)
options(od)
\end{ExampleCode}
\end{Examples}
\HeaderA{predict.glm}{Predict Method for GLM Fits}{predict.glm}
\keyword{models}{predict.glm}
\keyword{regression}{predict.glm}
%
\begin{Description}\relax
Obtains predictions and optionally estimates standard errors of those
predictions from a fitted generalized linear model object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'glm'
predict(object, newdata = NULL,
            type = c("link", "response", "terms"),
            se.fit = FALSE, dispersion = NULL, terms = NULL,
            na.action = na.pass, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a fitted object of class inheriting from \code{"glm"}.
\item[\code{newdata}] optionally, a data frame in which to look for variables with
which to predict.  If omitted, the fitted linear predictors are used.
\item[\code{type}] the type of prediction required.  The default is on the
scale of the linear predictors; the alternative \code{"response"}
is on the scale of the response variable.  Thus for a default
binomial model the default predictions are of log-odds (probabilities
on logit scale) and \code{type = "response"} gives the predicted
probabilities.  The \code{"terms"} option returns a matrix giving the
fitted values of each term in the model formula on the linear predictor
scale.

The value of this argument can be abbreviated.

\item[\code{se.fit}] logical switch indicating if standard errors are required.
\item[\code{dispersion}] the dispersion of the GLM fit to be assumed in
computing the standard errors.  If omitted, that returned by
\code{summary} applied to the object is used.
\item[\code{terms}] with \code{type="terms"} by default all terms are returned.
A character vector specifies which terms are to be returned
\item[\code{na.action}] function determining what should be done with missing
values in \code{newdata}.  The default is to predict \code{NA}.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{newdata} is omitted the predictions are based on the data
used for the fit.  In that case how cases with missing values in the
original fit is determined by the \code{na.action} argument of that
fit.  If \code{na.action = na.omit} omitted cases will not appear in
the residuals, whereas if \code{na.action = na.exclude} they will
appear (in predictions and standard errors), with residual value
\code{NA}.  See also \code{\LinkA{napredict}{napredict}}.
\end{Details}
%
\begin{Value}
If \code{se = FALSE}, a vector or matrix of predictions.
If \code{se = TRUE}, a list with components
\begin{ldescription}
\item[\code{fit}] Predictions
\item[\code{se.fit}] Estimated standard errors
\item[\code{residual.scale}] A scalar giving the square root of the
dispersion used in computing the standard errors.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Variables are first looked for in \code{newdata} and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in \code{newdata}
if it was supplied.
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{glm}{glm}}, \code{\LinkA{SafePrediction}{SafePrediction}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## example from Venables and Ripley (2002, pp. 190-2.)
ldose <- rep(0:5, 2)
numdead <- c(1, 4, 9, 13, 18, 20, 0, 2, 6, 10, 12, 16)
sex <- factor(rep(c("M", "F"), c(6, 6)))
SF <- cbind(numdead, numalive=20-numdead)
budworm.lg <- glm(SF ~ sex*ldose, family=binomial)
summary(budworm.lg)

plot(c(1,32), c(0,1), type = "n", xlab = "dose",
     ylab = "prob", log = "x")
text(2^ldose, numdead/20, as.character(sex))
ld <- seq(0, 5, 0.1)
lines(2^ld, predict(budworm.lg, data.frame(ldose=ld,
   sex=factor(rep("M", length(ld)), levels=levels(sex))),
   type = "response"))
lines(2^ld, predict(budworm.lg, data.frame(ldose=ld,
   sex=factor(rep("F", length(ld)), levels=levels(sex))),
   type = "response"))
\end{ExampleCode}
\end{Examples}
\HeaderA{predict.HoltWinters}{Prediction Function for Fitted Holt-Winters Models}{predict.HoltWinters}
\keyword{ts}{predict.HoltWinters}
%
\begin{Description}\relax
Computes predictions and prediction intervals for models fitted by
the Holt-Winters method.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'HoltWinters'
predict(object, n.ahead=1, prediction.interval = FALSE,
       level = 0.95, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] An object of class \code{HoltWinters}.
\item[\code{n.ahead}] Number of future periods to predict.
\item[\code{prediction.interval}] logical. If \code{TRUE}, the lower and
upper bounds of the corresponding prediction intervals are computed.
\item[\code{level}] Confidence level for the prediction interval.
\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A time series of the predicted values. If prediction intervals are
requested, a multiple time series is returned with columns \code{fit},
\code{lwr} and \code{upr} for the predicted values and the lower and
upper bounds respectively.
\end{Value}
%
\begin{Author}\relax
David Meyer \email{David.Meyer@wu.ac.at}
\end{Author}
%
\begin{References}\relax
C. C. Holt (1957)
Forecasting trends and seasonals by exponentially weighted
moving averages,
\emph{ONR Research Memorandum, Carnegie Institute of Technology} \bold{52}.

P. R. Winters (1960)
Forecasting sales by exponentially weighted moving averages,
\emph{Management Science} \bold{6}, 324--342.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{HoltWinters}{HoltWinters}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

m <- HoltWinters(co2)
p <- predict(m, 50, prediction.interval = TRUE)
plot(m, p)
\end{ExampleCode}
\end{Examples}
\HeaderA{predict.lm}{Predict method for Linear Model Fits}{predict.lm}
\aliasA{predict.mlm}{predict.lm}{predict.mlm}
\keyword{regression}{predict.lm}
%
\begin{Description}\relax
Predicted values based on linear model object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'lm'
predict(object, newdata, se.fit = FALSE, scale = NULL, df = Inf,
        interval = c("none", "confidence", "prediction"),
        level = 0.95, type = c("response", "terms"),
        terms = NULL, na.action = na.pass,
        pred.var = res.var/weights, weights = 1, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] Object of class inheriting from \code{"lm"}
\item[\code{newdata}] An optional data frame in which to look for variables with
which to predict.  If omitted, the fitted values are used.
\item[\code{se.fit}] A switch indicating if standard errors are required.
\item[\code{scale}] Scale parameter for std.err. calculation
\item[\code{df}] Degrees of freedom for scale
\item[\code{interval}] Type of interval calculation.
\item[\code{level}] Tolerance/confidence level
\item[\code{type}] Type of prediction (response or model term).
\item[\code{terms}] If \code{type="terms"}, which terms (default is all terms)
\item[\code{na.action}] function determining what should be done with missing
values in \code{newdata}.  The default is to predict \code{NA}.
\item[\code{pred.var}] the variance(s) for future observations to be assumed
for prediction intervals.  See `Details'.
\item[\code{weights}] variance weights for prediction. This can be a numeric
vector or a one-sided model formula. In the latter case, it is
interpreted as an expression evaluated in \code{newdata}
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{predict.lm} produces predicted values, obtained by evaluating
the regression function in the frame \code{newdata} (which defaults to
\code{model.frame(object)}.  If the logical \code{se.fit} is
\code{TRUE}, standard errors of the predictions are calculated.  If
the numeric argument \code{scale} is set (with optional \code{df}), it
is used as the residual standard deviation in the computation of the
standard errors, otherwise this is extracted from the model fit.
Setting \code{intervals} specifies computation of confidence or
prediction (tolerance) intervals at the specified \code{level}, sometimes
referred to as narrow vs. wide intervals.

If the fit is rank-deficient, some of the columns of the design matrix
will have been dropped.  Prediction from such a fit only makes sense
if \code{newdata} is contained in the same subspace as the original
data.  That cannot be checked accurately, so a warning is issued.

If \code{newdata} is omitted the predictions are based on the data
used for the fit.  In that case how cases with missing values in the
original fit is determined by the \code{na.action} argument of that
fit.  If \code{na.action = na.omit} omitted cases will not appear in
the residuals, whereas if \code{na.action = na.exclude} they will
appear (in predictions, standard errors or interval limits),
with residual value \code{NA}.  See also \code{\LinkA{napredict}{napredict}}.

The prediction intervals are for a single observation at each case in
\code{newdata} (or by default, the data used for the fit) with error
variance(s) \code{pred.var}. This can be a multiple of \code{res.var},
the estimated
value of \eqn{\sigma^2}{}: the default is to assume that future
observations have the same error variance as those
used for fitting. If \code{weights} is supplied, the inverse of this
is used as a scale factor. For a weighted fit, if the prediction
is for the original data frame, \code{weights} defaults to the weights
used for the  model fit, with a warning since it might not be the
intended result. If the fit was weighted and \code{newdata} is given, the
default is to assume constant prediction variance, with a warning.
\end{Details}
%
\begin{Value}
\code{predict.lm} produces a vector of predictions or a matrix of
predictions and bounds with column names \code{fit}, \code{lwr}, and
\code{upr} if \code{interval} is set.  If \code{se.fit} is
\code{TRUE}, a list with the following components is returned:
\begin{ldescription}
\item[\code{fit}] vector or matrix as above
\item[\code{se.fit}] standard error of predicted means
\item[\code{residual.scale}] residual standard deviations
\item[\code{df}] degrees of freedom for residual
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Variables are first looked for in \code{newdata} and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in \code{newdata}
if it was supplied.

Notice that prediction variances and prediction intervals always refer
to \emph{future} observations, possibly corresponding to the same
predictors as used for the fit. The variance of the \emph{residuals}
will be smaller.

Strictly speaking, the formula used for prediction limits assumes that
the degrees of freedom for the fit are the same as those for the
residual variance.  This may not be the case if \code{res.var} is
not obtained from the fit.
\end{Note}
%
\begin{SeeAlso}\relax
The model fitting function \code{\LinkA{lm}{lm}}, \code{\LinkA{predict}{predict}}.

\LinkA{SafePrediction}{SafePrediction} for prediction from polynomial and spline fits.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Predictions
x <- rnorm(15)
y <- x + rnorm(15)
predict(lm(y ~ x))
new <- data.frame(x = seq(-3, 3, 0.5))
predict(lm(y ~ x), new, se.fit = TRUE)
pred.w.plim <- predict(lm(y ~ x), new, interval="prediction")
pred.w.clim <- predict(lm(y ~ x), new, interval="confidence")
matplot(new$x,cbind(pred.w.clim, pred.w.plim[,-1]),
        lty=c(1,2,2,3,3), type="l", ylab="predicted y")

## Prediction intervals, special cases
##  The first three of these throw warnings
w <- 1 + x^2
fit <- lm(y ~ x)
wfit <- lm(y ~ x, weights = w)
predict(fit, interval = "prediction")
predict(wfit, interval = "prediction")
predict(wfit, new, interval = "prediction")
predict(wfit, new, interval = "prediction", weights = (new$x)^2)
predict(wfit, new, interval = "prediction", weights = ~x^2)
\end{ExampleCode}
\end{Examples}
\HeaderA{predict.loess}{Predict Loess Curve or Surface}{predict.loess}
\keyword{smooth}{predict.loess}
%
\begin{Description}\relax
Predictions from a \code{loess} fit, optionally with standard errors.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'loess'
predict(object, newdata = NULL, se = FALSE,
        na.action = na.pass, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object fitted by \code{loess}.
\item[\code{newdata}] an optional data frame in which to look for variables with
which to predict, or a matrix or vector containing exactly the variables
needs for prediction.  If missing, the original data points are used.
\item[\code{se}] should standard errors be computed?
\item[\code{na.action}] function determining what should be done with missing
values in data frame \code{newdata}.  The default is to predict \code{NA}.
\item[\code{...}] arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The standard errors calculation is slower than prediction.

When the fit was made using \code{surface="interpolate"} (the
default), \code{predict.loess} will not extrapolate -- so points outside
an axis-aligned hypercube enclosing the original data will have
missing (\code{NA}) predictions and standard errors.

The default for \code{na.action} prior to \R{} 2.12.0 was
\code{\LinkA{na.omit}{na.omit}}.
\end{Details}
%
\begin{Value}
If \code{se = FALSE}, a vector giving the prediction for each row of
\code{newdata} (or the original data). If \code{se = TRUE}, a list
containing components
\begin{ldescription}
\item[\code{fit}] the predicted values.
\item[\code{se}] an estimated standard error for each predicted value.
\item[\code{residual.scale}] the estimated scale of the residuals used in
computing the standard errors.
\item[\code{df}] an estimate of the effective degrees of freedom used in
estimating the residual scale, intended for use with t-based
confidence intervals. 
\end{ldescription}
If \code{newdata} was the result of a call to
\code{\LinkA{expand.grid}{expand.grid}}, the predictions (and s.e.'s if requested)
will be an array of the appropriate dimensions.

Predictions from infinite inputs will be \code{NA} since \code{loess}
does not support extrapolation.
\end{Value}
%
\begin{Note}\relax
Variables are first looked for in \code{newdata} and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in \code{newdata}
if it was supplied.
\end{Note}
%
\begin{Author}\relax
B. D. Ripley, based on the \code{cloess} package of Cleveland,
Grosse and Shyu.
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{loess}{loess}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
cars.lo <- loess(dist ~ speed, cars)
predict(cars.lo, data.frame(speed=seq(5, 30, 1)), se=TRUE)
# to get extrapolation
cars.lo2 <- loess(dist ~ speed, cars,
  control=loess.control(surface="direct"))
predict(cars.lo2, data.frame(speed=seq(5, 30, 1)), se=TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{predict.nls}{Predicting from Nonlinear Least Squares Fits}{predict.nls}
\keyword{nonlinear}{predict.nls}
\keyword{regression}{predict.nls}
\keyword{models}{predict.nls}
%
\begin{Description}\relax
\code{predict.nls} produces predicted values, obtained by evaluating
the regression function in the frame \code{newdata}.  If the logical
\code{se.fit} is \code{TRUE}, standard errors of the predictions are
calculated.  If the numeric argument \code{scale} is set (with
optional \code{df}), it is used as the residual standard deviation in
the computation of the standard errors, otherwise this is extracted
from the model fit.  Setting \code{intervals} specifies computation of
confidence or prediction (tolerance) intervals at the specified
\code{level}.

At present \code{se.fit} and \code{interval} are ignored.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'nls'
predict(object, newdata , se.fit = FALSE, scale = NULL, df = Inf, 
        interval = c("none", "confidence", "prediction"),
        level = 0.95, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] An object that inherits from class \code{nls}.
\item[\code{newdata}] A named list or data frame in which to look for variables with
which to predict.  If \code{newdata} is
missing the fitted values at the original data points are returned.
\item[\code{se.fit}] A logical value indicating if the standard errors of the
predictions should be calculated.  Defaults to \code{FALSE}.  At
present this argument is ignored.
\item[\code{scale}] A numeric scalar.  If it is set (with optional
\code{df}), it is used as the residual standard deviation in the
computation of the standard errors, otherwise this information is
extracted from the model fit. At present this argument is ignored.
\item[\code{df}] A positive numeric scalar giving the number of degrees of
freedom for the \code{scale} estimate. At present this argument is
ignored.
\item[\code{interval}] A character string indicating if prediction intervals
or a confidence interval on the mean responses are to be
calculated. At present this argument is ignored.
\item[\code{level}] A numeric scalar between 0 and 1 giving the confidence
level for the intervals (if any) to be calculated.  At present
this argument is ignored.
\item[\code{...}] Additional optional arguments.  At present no optional
arguments are used.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{predict.nls} produces a vector of predictions.
When implemented, \code{interval} will produce a matrix of
predictions and bounds with column names \code{fit}, \code{lwr}, and
\code{upr}.  When implemented, if \code{se.fit} is
\code{TRUE}, a list with the following components will be returned:

\begin{ldescription}
\item[\code{fit}] vector or matrix as above
\item[\code{se.fit}] standard error of predictions
\item[\code{residual.scale}] residual standard deviations
\item[\code{df}] degrees of freedom for residual
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Variables are first looked for in \code{newdata} and then searched for
in the usual way (which will include the environment of the formula
used in the fit).  A warning will be given if the
variables found are not of the same length as those in \code{newdata}
if it was supplied.
\end{Note}
%
\begin{SeeAlso}\relax
The model fitting function \code{\LinkA{nls}{nls}},
\code{\LinkA{predict}{predict}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(graphics)

fm <- nls(demand ~ SSasympOrig(Time, A, lrc), data = BOD)
predict(fm)              # fitted values at observed times
## Form data plot and smooth line for the predictions
opar <- par(las = 1)
plot(demand ~ Time, data = BOD, col = 4,
     main = "BOD data and fitted first-order curve",
     xlim = c(0,7), ylim = c(0, 20) )
tt <- seq(0, 8, length = 101)
lines(tt, predict(fm, list(Time = tt)))
par(opar)

\end{ExampleCode}
\end{Examples}
\HeaderA{predict.smooth.spline}{Predict from Smoothing Spline Fit}{predict.smooth.spline}
\keyword{smooth}{predict.smooth.spline}
%
\begin{Description}\relax
Predict a smoothing spline fit at new points, return the derivative if
desired. The predicted fit is linear beyond the original data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'smooth.spline'
predict(object, x, deriv = 0, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a fit from \code{smooth.spline}.
\item[\code{x}] the new values of x.
\item[\code{deriv}] integer; the order of the derivative required.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with components
\begin{ldescription}
\item[\code{x}] The input \code{x}.
\item[\code{y}] The fitted values or derivatives at \code{x}.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{smooth.spline}{smooth.spline}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

attach(cars)
cars.spl <- smooth.spline(speed, dist, df=6.4)


## "Proof" that the derivatives are okay, by comparing with approximation
diff.quot <- function(x,y) {
  ## Difference quotient (central differences where available)
  n <- length(x); i1 <- 1:2; i2 <- (n-1):n
  c(diff(y[i1]) / diff(x[i1]), (y[-i1] - y[-i2]) / (x[-i1] - x[-i2]),
    diff(y[i2]) / diff(x[i2]))
}

xx <- unique(sort(c(seq(0,30, by = .2), kn <- unique(speed))))
i.kn <- match(kn, xx)# indices of knots within xx
op <- par(mfrow = c(2,2))
plot(speed, dist, xlim = range(xx), main = "Smooth.spline & derivatives")
lines(pp <- predict(cars.spl, xx), col = "red")
points(kn, pp$y[i.kn], pch = 3, col="dark red")
mtext("s(x)", col = "red")
for(d in 1:3){
  n <- length(pp$x)
  plot(pp$x, diff.quot(pp$x,pp$y), type = 'l', xlab="x", ylab="",
       col = "blue", col.main = "red",
       main= paste0("s",paste(rep("'",d), collapse=""),"(x)"))
  mtext("Difference quotient approx.(last)", col = "blue")
  lines(pp <- predict(cars.spl, xx, deriv = d), col = "red")

  points(kn, pp$y[i.kn], pch = 3, col="dark red")
  abline(h=0, lty = 3, col = "gray")
}
detach(); par(op)
\end{ExampleCode}
\end{Examples}
\HeaderA{preplot}{Pre-computations for a Plotting Object}{preplot}
\keyword{models}{preplot}
%
\begin{Description}\relax
Compute an object to be used for plots relating to the given model object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
preplot(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a fitted model object.
\item[\code{...}] additional arguments for specific methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Only the generic function is currently provided in base \R{}, but some
add-on packages have methods. Principally here for S compatibility. 
\end{Details}
%
\begin{Value}
An object set up to make a plot that describes \code{object}.
\end{Value}
\HeaderA{princomp}{Principal Components Analysis}{princomp}
\aliasA{plot.princomp}{princomp}{plot.princomp}
\aliasA{predict.princomp}{princomp}{predict.princomp}
\methaliasA{princomp.default}{princomp}{princomp.default}
\methaliasA{princomp.formula}{princomp}{princomp.formula}
\aliasA{print.princomp}{princomp}{print.princomp}
\keyword{multivariate}{princomp}
%
\begin{Description}\relax
\code{princomp} performs a principal components analysis on the given
numeric data matrix and returns the results as an object of class
\code{princomp}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
princomp(x, ...)

## S3 method for class 'formula'
princomp(formula, data = NULL, subset, na.action, ...)

## Default S3 method:
princomp(x, cor = FALSE, scores = TRUE, covmat = NULL,
         subset = rep(TRUE, nrow(as.matrix(x))), ...)

## S3 method for class 'princomp'
predict(object, newdata, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a formula with no response variable, referring only to
numeric variables.
\item[\code{data}] an optional data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector used to select rows (observations) of the
data matrix \code{x}.
\item[\code{na.action}] a function which indicates what should happen
when the data contain \code{NA}s.  The default is set by
the \code{na.action} setting of \code{\LinkA{options}{options}}, and is
\code{\LinkA{na.fail}{na.fail}} if that is unset. The `factory-fresh'
default is \code{\LinkA{na.omit}{na.omit}}.
\item[\code{x}] a numeric matrix or data frame which provides the data for the
principal components analysis.
\item[\code{cor}] a logical value indicating whether the calculation should
use the correlation matrix or the covariance matrix.  (The
correlation matrix can only be used if there are no constant variables.)
\item[\code{scores}] a logical value indicating whether the score on each
principal component should be calculated.
\item[\code{covmat}] a covariance matrix, or a covariance list as returned by
\code{\LinkA{cov.wt}{cov.wt}} (and \code{\LinkA{cov.mve}{cov.mve}} or
\code{\LinkA{cov.mcd}{cov.mcd}} from package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}).
If supplied, this is used rather than the covariance matrix of
\code{x}.
\item[\code{...}] arguments passed to or from other methods. If \code{x} is
a formula one might specify \code{cor} or \code{scores}.
\item[\code{object}] Object of class inheriting from \code{"princomp"}
\item[\code{newdata}] An optional data frame or matrix in which to look for
variables with which to predict.  If omitted, the scores are used.
If the original fit used a formula or a data frame or a matrix with
column names, \code{newdata} must contain columns with the same
names. Otherwise it must contain the same number of columns, to be
used in the same order.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{princomp} is a generic function with \code{"formula"} and
\code{"default"} methods.

The calculation is done using \code{\LinkA{eigen}{eigen}} on the correlation or
covariance matrix, as determined by \code{\LinkA{cor}{cor}}.  This is done for
compatibility with the S-PLUS result.  A preferred method of
calculation is to use \code{\LinkA{svd}{svd}} on \code{x}, as is done in
\code{prcomp}.

Note that the default calculation uses divisor \code{N} for the
covariance matrix.

The \code{\LinkA{print}{print}} method for these objects prints the
results in a nice format and the \code{\LinkA{plot}{plot}} method produces
a scree plot (\code{\LinkA{screeplot}{screeplot}}).  There is also a
\code{\LinkA{biplot}{biplot}} method.

If \code{x} is a formula then the standard NA-handling is applied to
the scores (if requested): see \code{\LinkA{napredict}{napredict}}.

\code{princomp} only handles so-called R-mode PCA, that is feature
extraction of variables.  If a data matrix is supplied (possibly via a
formula) it is required that there are at least as many units as
variables.  For Q-mode PCA use \code{\LinkA{prcomp}{prcomp}}.
\end{Details}
%
\begin{Value}
\code{princomp} returns a list with class \code{"princomp"}
containing the following components:
\begin{ldescription}
\item[\code{sdev}] the standard deviations of the principal components.
\item[\code{loadings}] the matrix of variable loadings (i.e., a matrix
whose columns contain the eigenvectors).  This is of class
\code{"loadings"}: see \code{\LinkA{loadings}{loadings}} for its \code{print}
method.
\item[\code{center}] the means that were subtracted.
\item[\code{scale}] the scalings applied to each variable.
\item[\code{n.obs}] the number of observations.
\item[\code{scores}] if \code{scores = TRUE}, the scores of the supplied
data on the principal components.  These are non-null only if
\code{x} was supplied, and if \code{covmat} was also supplied if it
was a covariance list.  For the formula method,
\code{\LinkA{napredict}{napredict}()} is applied to handle the treatment of
values omitted by the \code{na.action}.
\item[\code{call}] the matched call.
\item[\code{na.action}] If relevant.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The signs of the columns of the loadings and scores are arbitrary, and
so may differ between different programs for PCA, and even between
different builds of \R{}.
\end{Note}
%
\begin{References}\relax
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venables, W. N. and B. D. Ripley (2002).
\emph{Modern Applied Statistics with S}, Springer-Verlag.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{summary.princomp}{summary.princomp}}, \code{\LinkA{screeplot}{screeplot}},
\code{\LinkA{biplot.princomp}{biplot.princomp}},
\code{\LinkA{prcomp}{prcomp}}, \code{\LinkA{cor}{cor}}, \code{\LinkA{cov}{cov}},
\code{\LinkA{eigen}{eigen}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## The variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
(pc.cr <- princomp(USArrests))  # inappropriate
princomp(USArrests, cor = TRUE) # =^= prcomp(USArrests, scale=TRUE)
## Similar, but different:
## The standard deviations differ by a factor of sqrt(49/50)

summary(pc.cr <- princomp(USArrests, cor = TRUE))
loadings(pc.cr)  ## note that blank entries are small but not zero
plot(pc.cr) # shows a screeplot.
biplot(pc.cr)

## Formula interface
princomp(~ ., data = USArrests, cor = TRUE)

## NA-handling
USArrests[1, 2] <- NA
pc.cr <- princomp(~ Murder + Assault + UrbanPop,
                  data = USArrests, na.action=na.exclude, cor = TRUE)
pc.cr$scores[1:5, ]

## (Simple) Robust PCA:
## Classical:
(pc.cl  <- princomp(stackloss))
## Robust:
(pc.rob <- princomp(stackloss, covmat = MASS::cov.rob(stackloss)))
\end{ExampleCode}
\end{Examples}
\HeaderA{print.power.htest}{Print method for power calculation object}{print.power.htest}
\keyword{htest}{print.power.htest}
%
\begin{Description}\relax
Print object of class \code{"power.htest"} in nice layout.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'power.htest'
print(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] Object of class \code{"power.htest"}.
\item[\code{...}] further arguments to be passed to or from methods. 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
A \code{power.htest} object is just a named list of numbers and
character strings, supplemented with \code{method} and \code{note}
elements.  The \code{method} is displayed as a title, the \code{note}
as a footnote, and the remaining elements are given in an aligned
`name = value' format.
\end{Details}
%
\begin{Value}
none
\end{Value}
%
\begin{Author}\relax
Peter Dalgaard
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{power.t.test}{power.t.test}},
\code{\LinkA{power.prop.test}{power.prop.test}}
\end{SeeAlso}
\HeaderA{print.ts}{Printing Time-Series Objects}{print.ts}
\keyword{ts}{print.ts}
%
\begin{Description}\relax
Print method for time series objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'ts'
print(x, calendar, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a time series object.
\item[\code{calendar}] enable/disable the display of information about
month names, quarter names or year when printing.  The default is
\code{TRUE} for a frequency of 4 or 12, \code{FALSE} otherwise.
\item[\code{...}] additional arguments to \code{\LinkA{print}{print}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is the \code{\LinkA{print}{print}} methods for objects inheriting from
class \code{"ts"}.
\end{Details}
%
\begin{SeeAlso}\relax
\code{\LinkA{print}{print}},
\code{\LinkA{ts}{ts}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
print(ts(1:10, frequency = 7, start = c(12, 2)), calendar = TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{printCoefmat}{Print Coefficient Matrices}{printCoefmat}
\keyword{print}{printCoefmat}
%
\begin{Description}\relax
Utility function to be used in higher-level \code{\LinkA{print}{print}}
methods, such as \code{\LinkA{print.summary.lm}{print.summary.lm}},
\code{\LinkA{print.summary.glm}{print.summary.glm}} and \code{\LinkA{print.anova}{print.anova}}.  The
goal is to provide a flexible interface with smart defaults such
that often, only \code{x} needs to be specified.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
printCoefmat(x, digits=max(3, getOption("digits") - 2),
             signif.stars = getOption("show.signif.stars"),
             signif.legend = signif.stars,
             dig.tst = max(1, min(5, digits - 1)),
             cs.ind = 1L:k, tst.ind = k + 1L,
             zap.ind = integer(0), P.values = NULL,
             has.Pvalue = nc >= 4L &&
                          substr(colnames(x)[nc], 1L, 3L) == "Pr(",
             eps.Pvalue = .Machine$double.eps,
             na.print = "NA", ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric matrix like object, to be printed.
\item[\code{digits}] minimum number of significant digits to be used for
most numbers.
\item[\code{signif.stars}] logical; if \code{TRUE}, P-values are additionally
encoded visually as `significance stars' in order to help scanning
of long coefficient tables.  It defaults to the
\code{show.signif.stars} slot of \code{\LinkA{options}{options}}.
\item[\code{signif.legend}] logical; if \code{TRUE}, a legend for the 
`significance stars' is printed provided \code{signif.stars=TRUE}.
\item[\code{dig.tst}] minimum number of significant digits for the test statistics,
see \code{tst.ind}.
\item[\code{cs.ind}] indices (integer) of column numbers which are (like)
\bold{c}oefficients and \bold{s}tandard errors to be formatted
together.
\item[\code{tst.ind}] indices (integer) of column numbers for test
statistics.
\item[\code{zap.ind}] indices (integer) of column numbers which should be
formatted by \code{\LinkA{zapsmall}{zapsmall}}, i.e., by `zapping' values
close to 0.
\item[\code{P.values}] logical or \code{NULL}; if \code{TRUE}, the last
column of \code{x} is formatted by \code{\LinkA{format.pval}{format.pval}} as P
values.  If \code{P.values = NULL}, the default, it is set to
\code{TRUE} only if \code{\LinkA{options}{options}("show.coef.Pvalue")} is
\code{TRUE} \emph{and} \code{x} has at least 4 columns \emph{and}
the last column name of \code{x} starts with \code{"Pr("}.
\item[\code{has.Pvalue}] logical; if \code{TRUE}, the last column of \code{x}
contains P values; in that case, it is printed if and only if
\code{P.values} (above) is true.
\item[\code{eps.Pvalue}] number,..
\item[\code{na.print}] a character string to code \code{\LinkA{NA}{NA}} values in
printed output.
\item[\code{...}] further arguments for \code{\LinkA{print}{print}}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Invisibly returns its argument, \code{x}.
\end{Value}
%
\begin{Author}\relax
Martin Maechler
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{print.summary.lm}{print.summary.lm}},
\code{\LinkA{format.pval}{format.pval}},
\code{\LinkA{format}{format}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
cmat <- cbind(rnorm(3, 10), sqrt(rchisq(3, 12)))
cmat <- cbind(cmat, cmat[,1]/cmat[,2])
cmat <- cbind(cmat, 2*pnorm(-cmat[,3]))
colnames(cmat) <- c("Estimate", "Std.Err", "Z value", "Pr(>z)")
printCoefmat(cmat[,1:3])
printCoefmat(cmat)
options(show.coef.Pvalues = FALSE)
printCoefmat(cmat, digits=2)
printCoefmat(cmat, digits=2, P.values = TRUE)
options(show.coef.Pvalues = TRUE)# revert
\end{ExampleCode}
\end{Examples}
\HeaderA{profile}{Generic Function for Profiling Models}{profile}
\keyword{models}{profile}
%
\begin{Description}\relax
Investigates behavior of objective function near the solution
represented by \code{fitted}.

See documentation on method functions for further details.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
profile(fitted, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{fitted}] the original fitted model object.
\item[\code{...}] additional parameters. See documentation on individual
methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with an element for each parameter being profiled. See the
individual methods for further details.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{profile.nls}{profile.nls}},
\code{\LinkA{profile.glm}{profile.glm}} in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}},
\ldots

For profiling R code, see \code{\LinkA{Rprof}{Rprof}}.
\end{SeeAlso}
\HeaderA{profile.nls}{Method for Profiling nls Objects}{profile.nls}
\keyword{nonlinear}{profile.nls}
\keyword{regression}{profile.nls}
\keyword{models}{profile.nls}
%
\begin{Description}\relax
Investigates the profile log-likelihood function for a fitted model of
class \code{"nls"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'nls'
profile(fitted, which = 1:npar, maxpts = 100, alphamax = 0.01,
        delta.t = cutoff/5, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{fitted}] the original fitted model object.
\item[\code{which}] the original model parameters which should be profiled.
This can be a numeric or character vector.
By default, all non-linear parameters are profiled.
\item[\code{maxpts}] maximum number of points to be used for profiling each
parameter.
\item[\code{alphamax}] highest significance level allowed
for the profile t-statistics.
\item[\code{delta.t}] suggested change on the scale of the profile
t-statistics.  Default value chosen to allow profiling at about
10 parameter values.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The profile t-statistics is defined as the square root of change in
sum-of-squares divided by residual standard error with an
appropriate sign.
\end{Details}
%
\begin{Value}
A list with an element for each parameter being profiled. The elements
are data-frames with two variables 
\begin{ldescription}
\item[\code{par.vals}] a matrix of parameter values for each fitted model.
\item[\code{tau}] the profile t-statistics.
\end{ldescription}
\end{Value}
%
\begin{Author}\relax
Of the original version, 
Douglas M. Bates and Saikat DebRoy
\end{Author}
%
\begin{References}\relax
Bates, D. M. and Watts, D. G. (1988), \emph{Nonlinear Regression Analysis
and Its Applications}, Wiley (chapter 6).
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{profile}{profile}}, \code{\LinkA{plot.profile.nls}{plot.profile.nls}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

# obtain the fitted object
fm1 <- nls(demand ~ SSasympOrig(Time, A, lrc), data = BOD)
# get the profile for the fitted model: default level is too extreme
pr1 <- profile(fm1, alpha = 0.05)
# profiled values for the two parameters
pr1$A
pr1$lrc
# see also example(plot.profile.nls)

\end{ExampleCode}
\end{Examples}
\HeaderA{proj}{Projections of Models}{proj}
\methaliasA{proj.aov}{proj}{proj.aov}
\methaliasA{proj.aovlist}{proj}{proj.aovlist}
\methaliasA{proj.default}{proj}{proj.default}
\methaliasA{proj.lm}{proj}{proj.lm}
\keyword{models}{proj}
%
\begin{Description}\relax
\code{proj} returns a matrix or list of matrices giving the projections
of the data onto the terms of a linear model.  It is most frequently
used for \code{\LinkA{aov}{aov}} models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
proj(object, ...)  

## S3 method for class 'aov'
proj(object, onedf = FALSE, unweighted.scale = FALSE, ...)  

## S3 method for class 'aovlist'
proj(object, onedf = FALSE, unweighted.scale = FALSE, ...)  

## Default S3 method:
proj(object, onedf = TRUE, ...)  

## S3 method for class 'lm'
proj(object, onedf = FALSE, unweighted.scale = FALSE, ...)  
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] An object of class \code{"lm"} or a class inheriting from
it, or an object with a similar structure including in particular
components \code{qr} and \code{effects}.
\item[\code{onedf}] A logical flag. If \code{TRUE}, a projection is returned for all
the columns of the model matrix. If \code{FALSE}, the single-column
projections are collapsed by terms of the model (as represented in
the analysis of variance table).
\item[\code{unweighted.scale}] If the fit producing \code{object} used
weights, this determines if the projections correspond to weighted or
unweighted observations.
\item[\code{...}] Swallow and ignore any other arguments.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
A projection is given for each stratum of the object, so for \code{aov}
models with an \code{Error} term the result is a list of projections.
\end{Details}
%
\begin{Value}
A projection matrix or (for multi-stratum objects) a list of
projection matrices.

Each projection is a matrix with a row for each observations and
either a column for each term (\code{onedf = FALSE}) or for each
coefficient (\code{onedf = TRUE}). Projection matrices from the
default method have orthogonal columns representing the projection of
the response onto the column space of the Q matrix from the QR
decomposition.  The fitted values are the sum of the projections, and
the sum of squares for each column is the reduction in sum of squares
from fitting that column (after those to the left of it).

The methods for \code{lm} and \code{aov} models add a column to the
projection matrix giving the residuals (the projection of the data
onto the orthogonal complement of the model space).

Strictly, when \code{onedf = FALSE} the result is not a projection,
but the columns represent sums of projections onto the columns of the
model matrix corresponding to that term. In this case the matrix does
not depend on the coding used.
\end{Value}
%
\begin{Author}\relax
The design was inspired by the S function of the same name described
in Chambers \emph{et al.} (1992).
\end{Author}
%
\begin{References}\relax
Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
\emph{Analysis of variance; designed experiments.}
Chapter 5 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{aov}{aov}}, \code{\LinkA{lm}{lm}}, \code{\LinkA{model.tables}{model.tables}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield <- c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,62.8,55.8,69.5,
55.0, 62.0,48.8,45.5,44.2,52.0,51.5,49.8,48.8,57.2,59.0,53.2,56.0)

npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
                  K=factor(K), yield=yield)
npk.aov <- aov(yield ~ block + N*P*K, npk)
proj(npk.aov)

## as a test, not particularly sensible
options(contrasts=c("contr.helmert", "contr.treatment"))
npk.aovE <- aov(yield ~  N*P*K + Error(block), npk)
proj(npk.aovE)
\end{ExampleCode}
\end{Examples}
\HeaderA{prop.test}{Test of Equal or Given Proportions}{prop.test}
\keyword{htest}{prop.test}
%
\begin{Description}\relax
\code{prop.test} can be used for testing the null that the
proportions (probabilities of success) in several groups are the
same, or that they equal certain given values.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
prop.test(x, n, p = NULL,
          alternative = c("two.sided", "less", "greater"),
          conf.level = 0.95, correct = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a vector of counts of successes, a one-dimensional table with
two entries, or a two-dimensional table (or matrix) with 2 columns,
giving the counts of successes and failures, respectively.
\item[\code{n}] a vector of counts of trials; ignored if \code{x} is a
matrix or a table.
\item[\code{p}] a vector of probabilities of success.  The length of
\code{p} must be the same as the number of groups specified by
\code{x}, and its elements must be greater than 0 and less than 1.
\item[\code{alternative}] a character string specifying the alternative
hypothesis, must be one of \code{"two.sided"} (default),
\code{"greater"} or \code{"less"}.  You can specify just the initial
letter.  Only used for testing the null that a single proportion
equals a given value, or that two proportions are equal; ignored
otherwise.
\item[\code{conf.level}] confidence level of the returned confidence
interval.  Must be a single number between 0 and 1.  Only used
when testing the null that a single proportion equals a given
value, or that two proportions are equal; ignored otherwise.
\item[\code{correct}] a logical indicating whether Yates' continuity
correction should be applied where possible.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Only groups with finite numbers of successes and failures are used.
Counts of successes and failures must be nonnegative and hence not
greater than the corresponding numbers of trials which must be
positive.  All finite counts should be integers.

If \code{p} is \code{NULL} and there is more than one group, the null
tested is that the proportions in each group are the same.  If there
are two groups, the alternatives are that the probability of success
in the first group is less than, not equal to, or greater than the
probability of success in the second group, as specified by
\code{alternative}.  A confidence interval for the difference of
proportions with confidence level as specified by \code{conf.level}
and clipped to \eqn{[-1,1]}{} is returned.  Continuity correction is
used only if it does not exceed the difference of the sample
proportions in absolute value.  Otherwise, if there are more than 2
groups, the alternative is always \code{"two.sided"}, the returned
confidence interval is \code{NULL}, and continuity correction is never
used.

If there is only one group, then the null tested is that the
underlying probability of success is \code{p}, or .5 if \code{p} is
not given.  The alternative is that the probability of success is less
than, not equal to, or greater than \code{p} or 0.5, respectively, as
specified by \code{alternative}.  A confidence interval for the
underlying proportion with confidence level as specified by
\code{conf.level} and clipped to \eqn{[0,1]}{} is returned.  Continuity
correction is used only if it does not exceed the difference between
sample and null proportions in absolute value. The confidence interval
is computed by inverting the score test.

Finally, if \code{p} is given and there are more than 2 groups, the
null tested is that the underlying probabilities of success are those
given by \code{p}.  The alternative is always \code{"two.sided"}, the
returned confidence interval is \code{NULL}, and continuity correction
is never used.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following
components:
\begin{ldescription}
\item[\code{statistic}] the value of Pearson's chi-squared test statistic.
\item[\code{parameter}] the degrees of freedom of the approximate
chi-squared distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{estimate}] a vector with the sample proportions \code{x/n}.
\item[\code{conf.int}] a confidence interval for the true proportion if
there is one group, or for the difference in proportions if
there are 2 groups and \code{p} is not given, or \code{NULL}
otherwise.  In the cases where it is not \code{NULL}, the
returned confidence interval has an asymptotic confidence level
as specified by \code{conf.level}, and is appropriate to the
specified alternative hypothesis.
\item[\code{null.value}] the value of \code{p} if specified by the null, or
\code{NULL} otherwise.
\item[\code{alternative}] a character string describing the alternative.
\item[\code{method}] a character string indicating the method used, and
whether Yates' continuity correction was applied.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Wilson, E.B. (1927) Probable inference, the law of succession, and
statistical inference.
\emph{J. Am. Stat. Assoc.}, \bold{22}, 209--212.

Newcombe R.G. (1998) Two-Sided Confidence Intervals for the Single
Proportion: Comparison of Seven Methods.
\emph{Statistics in Medicine} \bold{17}, 857--872.

Newcombe R.G. (1998) Interval Estimation for the Difference Between
Independent Proportions: Comparison of Eleven Methods.
\emph{Statistics in Medicine} \bold{17}, 873--890.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{binom.test}{binom.test}} for an \emph{exact} test of a binomial
hypothesis.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
heads <- rbinom(1, size=100, prob = .5)
prop.test(heads, 100)          # continuity correction TRUE by default
prop.test(heads, 100, correct = FALSE)

## Data from Fleiss (1981), p. 139.
## H0: The null hypothesis is that the four populations from which
##     the patients were drawn have the same true proportion of smokers.
## A:  The alternative is that this proportion is different in at
##     least one of the populations.

smokers  <- c( 83, 90, 129, 70 )
patients <- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
\end{ExampleCode}
\end{Examples}
\HeaderA{prop.trend.test}{Test for trend in proportions}{prop.trend.test}
\keyword{htest}{prop.trend.test}
%
\begin{Description}\relax
Performs chi-squared test for trend in proportions, i.e., a test
asymptotically optimal for local alternatives where the log odds vary
in proportion with \code{score}.  By default, \code{score} is chosen
as the group numbers.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
prop.trend.test(x, n, score = seq_along(x))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}]  Number of events 
\item[\code{n}]  Number of trials 
\item[\code{score}]  Group score 
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An object of class \code{"htest"} with title, test statistic, p-value,
etc.
\end{Value}
%
\begin{Note}\relax
 This really should get integrated with \code{prop.test} 
\end{Note}
%
\begin{Author}\relax
 Peter Dalgaard 
\end{Author}
%
\begin{SeeAlso}\relax
 \code{\LinkA{prop.test}{prop.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
smokers  <- c( 83, 90, 129, 70 )
patients <- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
prop.trend.test(smokers, patients)
prop.trend.test(smokers, patients,c(0,0,0,1))
\end{ExampleCode}
\end{Examples}
\HeaderA{qqnorm}{Quantile-Quantile Plots}{qqnorm}
\aliasA{qqline}{qqnorm}{qqline}
\methaliasA{qqnorm.default}{qqnorm}{qqnorm.default}
\aliasA{qqplot}{qqnorm}{qqplot}
\keyword{hplot}{qqnorm}
\keyword{distribution}{qqnorm}
%
\begin{Description}\relax
\code{qqnorm} is a generic function the default method of which
produces a normal QQ plot of the values in \code{y}.
\code{qqline} adds a line to a ``theoretical'', by default
normal, quantile-quantile plot which passes through the \code{probs}
quantiles, by default the first and third quartiles.

\code{qqplot} produces a QQ plot of two datasets.

Graphical parameters may be given as arguments to \code{qqnorm},
\code{qqplot} and \code{qqline}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
qqnorm(y, ...)
## Default S3 method:
qqnorm(y, ylim, main = "Normal Q-Q Plot",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles",
       plot.it = TRUE, datax = FALSE, ...)

qqline(y, datax = FALSE, distribution = qnorm,
       probs = c(0.25, 0.75), qtype = 7, ...)

qqplot(x, y, plot.it = TRUE, xlab = deparse(substitute(x)),
       ylab = deparse(substitute(y)), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] The first sample for \code{qqplot}.
\item[\code{y}] The second or only data sample.
\item[\code{xlab, ylab, main}] plot labels.  The \code{xlab} and \code{ylab}
refer to the y and x axes respectively if \code{datax = TRUE}.
\item[\code{plot.it}] logical. Should the result be plotted?
\item[\code{datax}] logical. Should data values be on the x-axis?
\item[\code{distribution}] quantile function for reference theoretical distribution.
\item[\code{probs}] numeric vector of length two, representing probabilities.
Corresponding quantile pairs define the line drawn.
\item[\code{qtype}] the \code{type} of quantile computation used in \code{\LinkA{quantile}{quantile}}.

\item[\code{ylim, ...}] graphical parameters.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
For \code{qqnorm} and \code{qqplot}, a list with components
\begin{ldescription}
\item[\code{x}] The x coordinates of the points that were/would be plotted
\item[\code{y}] The original \code{y} vector, i.e., the corresponding y
coordinates \emph{including \code{\LinkA{NA}{NA}}s}.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ppoints}{ppoints}}, used by \code{qqnorm} to generate
approximations to expected order statistics for a normal distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

y <- rt(200, df = 5)
qqnorm(y); qqline(y, col = 2)
qqplot(y, rt(300, df = 5))

qqnorm(precip, ylab = "Precipitation [in/yr] for 70 US cities")

## "QQ-Chisquare" : --------------------------
y <- rchisq(500, df=3)
## Q-Q plot for Chi^2 data against true theoretical distribution:
qqplot(qchisq(ppoints(500), df=3), y,
       main = expression("Q-Q plot for" ~~ {chi^2}[nu==3]))
qqline(y, distribution = function(p) qchisq(p, df=3),
       prob = c(0.1, 0.6), col=2)
mtext("qqline(*, dist = qchisq(., df=3), prob = c(0.1, 0.6))")
\end{ExampleCode}
\end{Examples}
\HeaderA{quade.test}{Quade Test}{quade.test}
\methaliasA{quade.test.default}{quade.test}{quade.test.default}
\methaliasA{quade.test.formula}{quade.test}{quade.test.formula}
\keyword{htest}{quade.test}
%
\begin{Description}\relax
Performs a Quade test with unreplicated blocked data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
quade.test(y, ...)

## Default S3 method:
quade.test(y, groups, blocks, ...)

## S3 method for class 'formula'
quade.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{y}] either a numeric vector of data values, or a data matrix.
\item[\code{groups}] a vector giving the group for the corresponding elements
of \code{y} if this is a vector;  ignored if \code{y} is a matrix.
If not a factor object, it is coerced to one.
\item[\code{blocks}] a vector giving the block for the corresponding elements
of \code{y} if this is a vector;  ignored if \code{y} is a matrix.
If not a factor object, it is coerced to one.
\item[\code{formula}] a formula of the form \code{a \textasciitilde{} b | c}, where \code{a},
\code{b} and \code{c} give the data values and corresponding groups
and blocks, respectively.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{quade.test} can be used for analyzing unreplicated complete
block designs (i.e., there is exactly one observation in \code{y}
for each combination of levels of \code{groups} and \code{blocks})
where the normality assumption may be violated.

The null hypothesis is that apart from an effect of \code{blocks},
the location parameter of \code{y} is the same in each of the
\code{groups}.

If \code{y} is a matrix, \code{groups} and \code{blocks} are obtained
from the column and row indices, respectively.  \code{NA}'s are not
allowed in \code{groups} or \code{blocks};  if \code{y} contains
\code{NA}'s, corresponding blocks are removed.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of Quade's F statistic.
\item[\code{parameter}] a vector with the numerator and denominator degrees 
of freedom of the approximate F distribution of the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{method}] the character string \code{"Quade test"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
D. Quade (1979),
Using weighted rankings in the analysis of complete blocks with
additive block effects.
\emph{Journal of the American Statistical Association}, \bold{74},
680--683.

William J. Conover (1999),
\emph{Practical nonparametric statistics}.
New York: John Wiley \& Sons.
Pages 373--380.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{friedman.test}{friedman.test}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Conover (1999, p. 375f):
## Numbers of five brands of a new hand lotion sold in seven stores
## during one week.
y <- matrix(c( 5,  4,  7, 10, 12,
               1,  3,  1,  0,  2,
              16, 12, 22, 22, 35,
               5,  4,  3,  5,  4,
              10,  9,  7, 13, 10,
              19, 18, 28, 37, 58,
              10,  7,  6,  8,  7),
            nrow = 7, byrow = TRUE,
            dimnames =
            list(Store = as.character(1:7),
                 Brand = LETTERS[1:5]))
y
quade.test(y)
\end{ExampleCode}
\end{Examples}
\HeaderA{quantile}{Sample Quantiles}{quantile}
\methaliasA{quantile.default}{quantile}{quantile.default}
\keyword{univar}{quantile}
%
\begin{Description}\relax
The generic function \code{quantile} produces sample quantiles
corresponding to the given probabilities.
The smallest observation corresponds to a probability of 0 and the
largest to a probability of 1.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
quantile(x, ...)

## Default S3 method:
quantile(x, probs = seq(0, 1, 0.25), na.rm = FALSE,
         names = TRUE, type = 7, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric vector whose sample quantiles are wanted, or an
object of a class for which a method has been defined (see also
`details'). \code{\LinkA{NA}{NA}} and \code{NaN} values are not
allowed in numeric vectors unless \code{na.rm} is \code{TRUE}.
\item[\code{probs}] numeric vector of probabilities with values in
\eqn{[0,1]}{}.  (Values up to \samp{2e-14} outside that
range are accepted and moved to the nearby endpoint.)
\item[\code{na.rm}] logical; if true, any \code{\LinkA{NA}{NA}} and \code{NaN}'s
are removed from \code{x} before the quantiles are computed.
\item[\code{names}] logical; if true, the result has a \code{\LinkA{names}{names}}
attribute.  Set to \code{FALSE} for speedup with many \code{probs}.
\item[\code{type}] an integer between 1 and 9 selecting one of the
nine quantile algorithms detailed below to be used.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
A vector of length \code{length(probs)} is returned;
if \code{names = TRUE}, it has a \code{\LinkA{names}{names}} attribute.

\code{\LinkA{NA}{NA}} and \code{\LinkA{NaN}{NaN}} values in \code{probs} are
propagated to the result.

The default method works with classed objects sufficiently like
numeric vectors that \code{sort} and (not needed by types 1 and 3)
addition of elements and multiplication by a number work correctly.
Note that as this is in a namespace, the copy of \code{sort} in
\pkg{base} will be used, not some S4 generic of that name.  Also note
that that is no check on the `correctly', and so
e.g. \code{quantile} can be applied to complex vectors which (apart
from ties) will be ordered on their real parts.

There is a method for the date-time classes (see
\code{"\LinkA{POSIXt}{POSIXt}"}).  Types 1 and 3 can be used for class
\code{"\LinkA{Date}{Date}"} and for ordered factors.
\end{Details}
%
\begin{Section}{Types}
\code{quantile} returns estimates of underlying distribution quantiles
based on one or two order statistics from the supplied elements in
\code{x} at probabilities in \code{probs}.  One of the nine quantile
algorithms discussed in Hyndman and Fan (1996), selected by
\code{type}, is employed.

All sample quantiles are defined as weighted averages of
consecutive order statistics. Sample quantiles of type \eqn{i}{}
are defined by:
\deqn{Q_{i}(p) = (1 - \gamma)x_{j} + \gamma x_{j+1}}{}
where \eqn{1 \le i \le 9}{},
\eqn{\frac{j - m}{n} \le p < \frac{j - m + 1}{n}}{},
\eqn{x_{j}}{} is the \eqn{j}{}th order statistic, \eqn{n}{} is the
sample size, the value of \eqn{\gamma}{} is a function of
\eqn{j = \lfloor np + m\rfloor}{} and \eqn{g = np + m - j}{},
and \eqn{m}{} is a constant determined by the sample quantile type.

\strong{Discontinuous sample quantile types 1, 2, and 3}

For types 1, 2 and 3, \eqn{Q_i(p)}{} is a discontinuous
function of \eqn{p}{}, with \eqn{m = 0}{} when \eqn{i = 1}{} and \eqn{i =
  2}{}, and \eqn{m = -1/2}{} when \eqn{i = 3}{}.

\begin{description}

\item[Type 1] Inverse of empirical distribution function.
\eqn{\gamma = 0}{} if \eqn{g = 0}{}, and 1 otherwise.
\item[Type 2] Similar to type 1 but with averaging at discontinuities.
\eqn{\gamma = 0.5}{} if \eqn{g = 0}{}, and 1 otherwise.
\item[Type 3] SAS definition: nearest even order statistic.
\eqn{\gamma = 0}{} if \eqn{g = 0}{} and \eqn{j}{} is even,
and 1 otherwise.

\end{description}


\strong{Continuous sample quantile types 4 through 9}

For types 4 through 9, \eqn{Q_i(p)}{} is a continuous function
of \eqn{p}{}, with \eqn{\gamma = g}{} and \eqn{m}{} given below. The
sample quantiles can be obtained equivalently by linear interpolation
between the points \eqn{(p_k,x_k)}{} where \eqn{x_k}{}
is the \eqn{k}{}th order statistic.  Specific expressions for
\eqn{p_k}{} are given below.

\begin{description}

\item[Type 4] \eqn{m = 0}{}. \eqn{p_k = \frac{k}{n}}{}.
That is, linear interpolation of the empirical cdf.


\item[Type 5] \eqn{m = 1/2}{}.
\eqn{p_k = \frac{k - 0.5}{n}}{}.
That is a piecewise linear function where the knots are the values
midway through the steps of the empirical cdf.  This is popular
amongst hydrologists.


\item[Type 6] \eqn{m = p}{}. \eqn{p_k = \frac{k}{n + 1}}{}.
Thus \eqn{p_k = \mbox{E}[F(x_{k})]}{}.
This is used by Minitab and by SPSS.


\item[Type 7] \eqn{m = 1-p}{}.
\eqn{p_k = \frac{k - 1}{n - 1}}{}.
In this case, \eqn{p_k = \mbox{mode}[F(x_{k})]}{}.
This is used by S.


\item[Type 8] \eqn{m = (p+1)/3}{}.
\eqn{p_k = \frac{k - 1/3}{n + 1/3}}{}.
Then \eqn{p_k \approx \mbox{median}[F(x_{k})]}{}.
The resulting quantile estimates are approximately median-unbiased
regardless of the distribution of \code{x}.


\item[Type 9] \eqn{m = p/4 + 3/8}{}.
\eqn{p_k = \frac{k - 3/8}{n + 1/4}}{}.
The resulting quantile estimates are approximately unbiased for
the expected order statistics if \code{x} is normally distributed.


\end{description}

Further details are provided in Hyndman and Fan (1996) who recommended type 8.
The default method is type 7, as used by S and by \R{} < 2.0.0.
\end{Section}
%
\begin{Author}\relax
of the version used in \R{} >= 2.0.0, Ivan Frohne and Rob J Hyndman.
\end{Author}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Hyndman, R. J. and Fan, Y. (1996) Sample quantiles in statistical
packages, \emph{American Statistician}, \bold{50}, 361--365.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ecdf}{ecdf}} for empirical distributions of which
\code{quantile} is an inverse;
\code{\LinkA{boxplot.stats}{boxplot.stats}} and \code{\LinkA{fivenum}{fivenum}} for computing
other versions of quartiles, etc.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
quantile(x <- rnorm(1001)) # Extremes & Quartiles by default
quantile(x,  probs = c(0.1, 0.5, 1, 2, 5, 10, 50, NA)/100)

### Compare different types
p <- c(0.1, 0.5, 1, 2, 5, 10, 50)/100
res <- matrix(as.numeric(NA), 9, 7)
for(type in 1:9) res[type, ] <- y <- quantile(x,  p, type = type)
dimnames(res) <- list(1:9, names(y))
round(res, 3)
\end{ExampleCode}
\end{Examples}
\HeaderA{r2dtable}{Random 2-way Tables with Given Marginals}{r2dtable}
\keyword{distribution}{r2dtable}
%
\begin{Description}\relax
Generate random 2-way tables with given marginals using Patefield's
algorithm.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
r2dtable(n, r, c)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] a non-negative numeric giving the number of tables to be
drawn.
\item[\code{r}] a non-negative vector of length at least 2 giving the row
totals, to be coerced to \code{integer}.  Must sum to the same as
\code{c}.
\item[\code{c}] a non-negative vector of length at least 2 giving the column
totals, to be coerced to \code{integer}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list of length \code{n} containing the generated tables as its
components.
\end{Value}
%
\begin{References}\relax
Patefield, W. M. (1981)
Algorithm AS159.  An efficient method of generating r x c tables
with given row and column totals.
\emph{Applied Statistics} \bold{30}, 91--97.
\end{References}
%
\begin{Examples}
\begin{ExampleCode}
## Fisher's Tea Drinker data.
TeaTasting <-
matrix(c(3, 1, 1, 3),
       nrow = 2,
       dimnames = list(Guess = c("Milk", "Tea"),
                       Truth = c("Milk", "Tea")))
## Simulate permutation test for independence based on the maximum
## Pearson residuals (rather than their sum).
rowTotals <- rowSums(TeaTasting)
colTotals <- colSums(TeaTasting)
nOfCases <- sum(rowTotals)
expected <- outer(rowTotals, colTotals, "*") / nOfCases
maxSqResid <- function(x) max((x - expected) ^ 2 / expected)
simMaxSqResid <-
    sapply(r2dtable(1000, rowTotals, colTotals), maxSqResid)
sum(simMaxSqResid >= maxSqResid(TeaTasting)) / 1000
## Fisher's exact test gives p = 0.4857 ...
\end{ExampleCode}
\end{Examples}
\HeaderA{read.ftable}{Manipulate Flat Contingency Tables}{read.ftable}
\aliasA{format.ftable}{read.ftable}{format.ftable}
\aliasA{write.ftable}{read.ftable}{write.ftable}
\keyword{category}{read.ftable}
%
\begin{Description}\relax
Read, write and coerce `flat' contingency tables.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
read.ftable(file, sep = "", quote = "\"",
            row.var.names, col.vars, skip = 0)

write.ftable(x, file = "", quote = TRUE, append = FALSE,
             digits = getOption("digits"))

## S3 method for class 'ftable'
format(x, quote = TRUE, digits = getOption("digits"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{file}] either a character string naming a file or a connection
which the data are to be read from or written to.  \code{""}
indicates input from the console for reading and output to the
console for writing.
\item[\code{sep}] the field separator string.  Values on each line of the file
are separated by this string.
\item[\code{quote}] a character string giving the set of quoting characters
for \code{read.ftable}; to disable quoting altogether, use
\code{quote=""}.  For \code{write.table}, a logical indicating
whether strings in the data will be surrounded by double quotes.
\item[\code{row.var.names}] a character vector with the names of the row
variables, in case these cannot be determined automatically.
\item[\code{col.vars}] a list giving the names and levels of the column
variables, in case these cannot be determined automatically.
\item[\code{skip}] the number of lines of the data file to skip before
beginning to read data.
\item[\code{x}] an object of class \code{"ftable"}.
\item[\code{append}] logical.  If \code{TRUE} and \code{file} is the name of
a file (and not a connection or \code{"|cmd"}), the output from
\code{write.ftable} is appended to the file.  If \code{FALSE},
the contents of \code{file} will be overwritten.
\item[\code{digits}] an integer giving the number of significant digits to
use for (the cell entries of) \code{x}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{read.ftable} reads in a flat-like contingency table from a
file.  If the file contains the written representation of a flat
table (more precisely, a header with all information on names and
levels of column variables, followed by a line with the names of the
row variables), no further arguments are needed.  Similarly, flat
tables with only one column variable the name of which is the only
entry in the first line are handled automatically.  Other variants can
be dealt with by skipping all header information using \code{skip},
and providing the names of the row variables and the names and levels
of the column variable using \code{row.var.names} and \code{col.vars},
respectively.  See the examples below.

Note that flat tables are characterized by their `ragged'
display of row (and maybe also column) labels.  If the full grid of
levels of the row variables is given, one should instead use
\code{read.table} to read in the data, and create the contingency
table from this using \code{\LinkA{xtabs}{xtabs}}.

\code{write.ftable} writes a flat table to a file, which is useful for
generating `pretty' ASCII representations of contingency tables.
\end{Details}
%
\begin{References}\relax
Agresti, A. (1990)
\emph{Categorical data analysis}.
New York: Wiley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ftable}{ftable}} for more information on flat contingency tables.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Agresti (1990), page 157, Table 5.8.
## Not in ftable standard format, but o.k.
file <- tempfile()
cat("             Intercourse\n",
    "Race  Gender     Yes  No\n",
    "White Male        43 134\n",
    "      Female      26 149\n",
    "Black Male        29  23\n",
    "      Female      22  36\n",
    file = file)
file.show(file)
ft <- read.ftable(file)
ft
unlink(file)

## Agresti (1990), page 297, Table 8.16.
## Almost o.k., but misses the name of the row variable.
file <- tempfile()
cat("                      \"Tonsil Size\"\n",
    "            \"Not Enl.\" \"Enl.\" \"Greatly Enl.\"\n",
    "Noncarriers       497     560           269\n",
    "Carriers           19      29            24\n",
    file = file)
file.show(file)
ft <- read.ftable(file, skip = 2,
                  row.var.names = "Status",
                  col.vars = list("Tonsil Size" =
                      c("Not Enl.", "Enl.", "Greatly Enl.")))
ft
unlink(file)

ft22 <- ftable(Titanic, row.vars = 2:1, col.vars = 4:3)
write.ftable(ft22, quote = FALSE)

\end{ExampleCode}
\end{Examples}
\HeaderA{rect.hclust}{Draw Rectangles Around Hierarchical Clusters}{rect.hclust}
\keyword{aplot}{rect.hclust}
\keyword{cluster}{rect.hclust}
%
\begin{Description}\relax
Draws rectangles around the branches of a dendrogram highlighting the
corresponding clusters. First the dendrogram is cut at a certain
level, then a rectangle is drawn around selected branches.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
rect.hclust(tree, k = NULL, which = NULL, x = NULL, h = NULL,
            border = 2, cluster = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{tree}] an object of the type produced by \code{hclust}.
\item[\code{k, h}] Scalar. Cut the dendrogram such that either exactly
\code{k} clusters are produced or by cutting at height \code{h}.
\item[\code{which, x}] A vector selecting the clusters around which a
rectangle should be drawn. \code{which} selects clusters by number
(from left to right in the tree), \code{x} selects clusters
containing the respective horizontal coordinates. Default is
\code{which = 1:k}.
\item[\code{border}] Vector with border colors for the rectangles.
\item[\code{cluster}] Optional vector with cluster memberships as returned by
\code{cutree(hclust.obj, k = k)}, can be specified for efficiency if
already computed.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
(Invisibly) returns a list where each element contains a vector of
data points contained in the respective cluster.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{hclust}{hclust}}, \code{\LinkA{identify.hclust}{identify.hclust}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

hca <- hclust(dist(USArrests))
plot(hca)
rect.hclust(hca, k=3, border="red")
x <- rect.hclust(hca, h=50, which=c(2,7), border=3:4)
x
\end{ExampleCode}
\end{Examples}
\HeaderA{relevel}{Reorder Levels of Factor}{relevel}
\methaliasA{relevel.default}{relevel}{relevel.default}
\methaliasA{relevel.factor}{relevel}{relevel.factor}
\methaliasA{relevel.ordered}{relevel}{relevel.ordered}
\keyword{utilities}{relevel}
\keyword{models}{relevel}
%
\begin{Description}\relax
The levels of a factor are re-ordered so that the level specified by
\code{ref} is first and the others are moved down. This is useful
for \code{contr.treatment} contrasts which take the first level as
the reference.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
relevel(x, ref, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] An unordered factor.
\item[\code{ref}] The reference level.
\item[\code{...}] Additional arguments for future methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A factor of the same length as \code{x}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{factor}{factor}}, \code{\LinkA{contr.treatment}{contr.treatment}},
\code{\LinkA{levels}{levels}}, \code{\LinkA{reorder}{reorder}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
warpbreaks$tension <- relevel(warpbreaks$tension, ref="M")
summary(lm(breaks ~ wool + tension, data=warpbreaks))
\end{ExampleCode}
\end{Examples}
\HeaderA{reorder.default}{Reorder Levels of a Factor}{reorder.default}
\aliasA{reorder}{reorder.default}{reorder}
\keyword{utilities}{reorder.default}
%
\begin{Description}\relax
\code{reorder} is a generic function.  The \code{"default"} method
treats its first argument as a categorical variable, and reorders its
levels based on the values of a second variable, usually numeric.  
\end{Description}
%
\begin{Usage}
\begin{verbatim}
reorder(x, ...)

## Default S3 method:
reorder(x, X, FUN = mean, ...,
        order = is.ordered(x))

\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] 
An atomic vector, usually a factor (possibly ordered).  The vector
is treated as a categorical variable whose levels will be reordered.
If \code{x} is not a factor, its unique values will be used as
the implicit levels.

\item[\code{X}]  a vector of the same length as \code{x}, whose subset
of values for each unique level of \code{x} determines the
eventual order of that level.

\item[\code{FUN}]  a function whose first argument is a vector and
returns a scalar, to be applied to each subset of \code{X}
determined by the levels of \code{x}.

\item[\code{...}]  optional: extra arguments supplied to \code{FUN}
\item[\code{order}]  logical, whether return value will be an ordered factor
rather than a factor.

\end{ldescription}
\end{Arguments}
%
\begin{Value}
A factor or an ordered factor (depending on the value of
\code{order}), with the order of the levels determined by
\code{FUN} applied to \code{X} grouped by \code{x}.  The
levels are ordered such that the values returned by \code{FUN}
are in increasing order.  Empty levels will be dropped. 

Additionally, the values of \code{FUN} applied to the subsets of
\code{X} (in the original order of the levels of \code{x}) is returned
as the \code{"scores"} attribute.
\end{Value}
%
\begin{Author}\relax
Deepayan Sarkar \email{deepayan.sarkar@r-project.org}
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{reorder.dendrogram}{reorder.dendrogram}}, \code{\LinkA{levels}{levels}},
\code{\LinkA{relevel}{relevel}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

bymedian <- with(InsectSprays, reorder(spray, count, median))
boxplot(count ~ bymedian, data = InsectSprays,
        xlab = "Type of spray", ylab = "Insect count",
        main = "InsectSprays data", varwidth = TRUE,
        col = "lightgray")
\end{ExampleCode}
\end{Examples}
\HeaderA{reorder.dendrogram}{Reorder a Dendrogram}{reorder.dendrogram}
\keyword{manip}{reorder.dendrogram}
%
\begin{Description}\relax
A method for the generic function \code{\LinkA{reorder}{reorder}}.

There are many different orderings of a dendrogram that are consistent
with the structure imposed.  This function takes a dendrogram and a
vector of values and reorders the dendrogram in the order of the
supplied vector, maintaining the constraints on the dendrogram.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'dendrogram'
reorder(x, wts, agglo.FUN = sum, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] the (dendrogram) object to be reordered
\item[\code{wts}] numeric weights (arbitrary values) for reordering.
\item[\code{agglo.FUN}] a function for weights agglomeration, see below.
\item[\code{...}] additional arguments
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Using the weights \code{wts}, the leaves of the dendrogram are
reordered so as to be in an order as consistent as possible with the
weights.  At each node, the branches are ordered in
increasing weights where the weight of a branch is defined as
\eqn{f(w_j)}{} where \eqn{f}{} is \code{agglo.FUN} and \eqn{w_j}{} is the
weight of the \eqn{j}{}-th sub branch).
\end{Details}
%
\begin{Value}
A dendrogram where each node has a further attribute \code{value} with
its corresponding weight.
\end{Value}
%
\begin{Author}\relax
R. Gentleman and M. Maechler
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{reorder}{reorder}}.

\code{\LinkA{rev.dendrogram}{rev.dendrogram}} which simply reverses the nodes'
order; \code{\LinkA{heatmap}{heatmap}}, \code{\LinkA{cophenetic}{cophenetic}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

set.seed(123)
x <- rnorm(10)
hc <- hclust(dist(x))
dd <- as.dendrogram(hc)
dd.reorder <- reorder(dd, 10:1)
plot(dd, main = "random dendrogram 'dd'")

op <- par(mfcol = 1:2)
plot(dd.reorder, main = "reorder(dd, 10:1)")
plot(reorder(dd,10:1, agglo.FUN= mean),
     main = "reorder(dd, 10:1, mean)")
par(op)
\end{ExampleCode}
\end{Examples}
\HeaderA{replications}{Number of Replications of Terms}{replications}
\keyword{models}{replications}
%
\begin{Description}\relax
Returns a vector or a list of the number of replicates for
each term in the formula.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
replications(formula, data=NULL, na.action)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a formula or a terms object or a data frame.
\item[\code{data}] a data frame used  to  find  the  objects in \code{formula}.
\item[\code{na.action}] function for handling missing values.  Defaults to
a \code{na.action} attribute of \code{data}, then
a setting of the option \code{na.action}, or \code{na.fail} if that
is not set.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{formula} is a data frame and \code{data} is missing,
\code{formula} is used for \code{data} with the formula \code{\textasciitilde{} .}.
\end{Details}
%
\begin{Value}
A vector or list with one entry for each term in the formula giving
the number(s) of replications for each level. If all levels are
balanced (have the same number of replications) the result is a
vector, otherwise it is a list with a component for each terms,
as a vector, matrix or array as required.

A test for balance is \code{!is.list(replications(formula,data))}.
\end{Value}
%
\begin{Author}\relax
The design was inspired by the S function of the same name described
in Chambers \emph{et al.} (1992).
\end{Author}
%
\begin{References}\relax
Chambers, J. M., Freeny, A and Heiberger, R. M. (1992)
\emph{Analysis of variance; designed experiments.}
Chapter 5 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{model.tables}{model.tables}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## From Venables and Ripley (2002) p.165.
N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield <- c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,62.8,55.8,69.5,
55.0, 62.0,48.8,45.5,44.2,52.0,51.5,49.8,48.8,57.2,59.0,53.2,56.0)

npk <- data.frame(block=gl(6,4), N=factor(N), P=factor(P),
                  K=factor(K), yield=yield)
replications(~ . - yield, npk)
\end{ExampleCode}
\end{Examples}
\HeaderA{reshape}{Reshape Grouped Data}{reshape}
\keyword{manip}{reshape}
%
\begin{Description}\relax
This function reshapes a data frame between `wide' format with
repeated measurements in separate columns of the same record and
`long' format with the repeated measurements in separate
records.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
reshape(data, varying = NULL, v.names = NULL, timevar = "time",
        idvar = "id", ids = 1:NROW(data),
        times = seq_along(varying[[1]]),
        drop = NULL, direction, new.row.names = NULL,
        sep = ".",
        split = if (sep == "") {
            list(regexp = "[A-Za-z][0-9]", include = TRUE)
        } else {
            list(regexp = sep, include = FALSE, fixed = TRUE)}
        )

\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] a data frame
\item[\code{varying}] names of sets of variables in the wide format that
correspond to single variables in long format
(`time-varying').  This is canonically a list of vectors of
variable names, but it can optionally be a matrix of names, or a
single vector of names.  In each case, the names can be replaced by
indices which are interpreted as referring to \code{names(data)}. 
See `Details' for more details and options.
\item[\code{v.names}] names of variables in the long format that correspond
to multiple variables in the wide format.  See `Details'.
\item[\code{timevar}] the variable in long format that differentiates multiple
records from the same group or individual.  If more than one record
matches, the first will be taken (with a warning). 
\item[\code{idvar}] Names of one or more variables in long format that
identify multiple records from the same group/individual.  These
variables may also be present in wide format.
\item[\code{ids}] the values to use for a newly created \code{idvar}
variable in long format.
\item[\code{times}] the values to use for a newly created \code{timevar}
variable in long format.  See `Details'.
\item[\code{drop}] a vector of names of variables to drop before reshaping.
\item[\code{direction}] character string, either \code{"wide"} to reshape to
wide format, or \code{"long"} to reshape to long format.
\item[\code{new.row.names}] logical; if \code{TRUE} and \code{direction = "wide"},
create new row names in long format from the values of the \code{id} and
\code{time} variables.
\item[\code{sep}] A character vector of length 1, indicating a separating
character in the variable names in the wide format.  This is used for
guessing \code{v.names} and \code{times} arguments based on the
names in \code{varying}.  If \code{sep == ""}, the split is just before
the first numeral that follows an alphabetic character.  This is
also used to create variable names when reshaping to wide format.
\item[\code{split}] A list with three components, \code{regexp},
\code{include}, and (optionally) \code{fixed}.  This allows an
extended interface to variable name splitting.  See `Details'.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The arguments to this function are described in terms of longitudinal
data, as that is the application motivating the functions.  A
`wide' longitudinal dataset will have one record for each
individual with some time-constant variables that occupy single
columns and some time-varying variables that occupy a column for each
time point.  In `long' format there will be multiple records
for each individual, with some variables being constant across these
records and others varying across the records.  A `long' format
dataset also needs a `time' variable identifying which time
point each record comes from and an `id' variable showing which
records refer to the same person.

If the data frame resulted from a previous \code{reshape} then the
operation can be reversed simply by \code{reshape(a)}.  The
\code{direction} argument is optional and the other arguments are
stored as attributes on the data frame.

If \code{direction = "wide"} and no \code{varying} or \code{v.names}
arguments are supplied it is assumed that all variables except
\code{idvar} and \code{timevar} are time-varying.  They are all
expanded into multiple variables in wide format.

If \code{direction = "long"} the \code{varying} argument can be a vector
of column names (or a corresponding index).  The function will attempt
to guess the \code{v.names} and \code{times} from these names.  The
default is variable names like \code{x.1}, \code{x.2}, where
\code{sep = "."} specifies to split at the dot and drop it from the
name.  To have alphabetic followed by numeric times use \code{sep = ""}.

Variable name splitting as described above is only attempted in the
case where \code{varying} is an atomic vector, if it is a list or a
matrix, \code{v.names} and \code{times} will generally need to be
specified, although they will default to, respectively, the first
variable name in each set, and sequential times.

Also, guessing is not attempted if \code{v.names} is given
explicitly.  Notice that the order of variables in \code{varying} is
like \code{x.1},\code{y.1},\code{x.2},\code{y.2}.

The \code{split} argument should not usually be necessary.  The
\code{split\$regexp} component is passed to either \code{strsplit()} or
\code{regexp()}, where the latter is used if \code{split\$include} is
\code{TRUE}, in which case the splitting occurs after the first
character of the matched string.  In the \code{strsplit()} case, the
separator is not included in the result, and it is possible to specify
fixed-string matching using \code{split\$fixed}.
\end{Details}
%
\begin{Value}
The reshaped data frame with added attributes to simplify reshaping
back to the original form.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{stack}{stack}}, \code{\LinkA{aperm}{aperm}};
\code{\LinkA{relist}{relist}} for reshaping the result of \code{\LinkA{unlist}{unlist}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
summary(Indometh)
wide <- reshape(Indometh, v.names = "conc", idvar = "Subject",
                timevar = "time", direction = "wide")
wide

reshape(wide, direction = "long")
reshape(wide, idvar = "Subject", varying = list(2:12),
        v.names = "conc", direction = "long")

## times need not be numeric
df <- data.frame(id = rep(1:4, rep(2,4)),
                 visit = I(rep(c("Before","After"), 4)),
                 x = rnorm(4), y = runif(4))
df
reshape(df, timevar = "visit", idvar = "id", direction = "wide")
## warns that y is really varying
reshape(df, timevar = "visit", idvar = "id", direction = "wide", v.names = "x")


##  unbalanced 'long' data leads to NA fill in 'wide' form
df2 <- df[1:7, ]
df2
reshape(df2, timevar = "visit", idvar = "id", direction = "wide")

## Alternative regular expressions for guessing names
df3 <- data.frame(id = 1:4, age = c(40,50,60,50), dose1 = c(1,2,1,2),
                  dose2 = c(2,1,2,1), dose4 = c(3,3,3,3))
reshape(df3, direction = "long", varying = 3:5, sep = "")


## an example that isn't longitudinal data
state.x77 <- as.data.frame(state.x77)
long <- reshape(state.x77, idvar = "state", ids = row.names(state.x77),
                times = names(state.x77), timevar = "Characteristic",
                varying = list(names(state.x77)), direction = "long")

reshape(long, direction = "wide")

reshape(long, direction = "wide", new.row.names = unique(long$state))

## multiple id variables
df3 <- data.frame(school = rep(1:3, each = 4), class = rep(9:10, 6),
                  time = rep(c(1,1,2,2), 3), score = rnorm(12))
wide <- reshape(df3, idvar = c("school","class"), direction = "wide")
wide
## transform back
reshape(wide)

\end{ExampleCode}
\end{Examples}
\HeaderA{residuals}{Extract Model Residuals}{residuals}
\aliasA{resid}{residuals}{resid}
\methaliasA{residuals.default}{residuals}{residuals.default}
\keyword{models}{residuals}
\keyword{regression}{residuals}
%
\begin{Description}\relax
\code{residuals} is a generic function which extracts model residuals
from objects returned by modeling functions.

The abbreviated form \code{resid} is an alias for \code{residuals}.
It is intended to encourage users to access object components through
an accessor function rather than by directly referencing an object
slot.

All object classes which are returned by model fitting functions
should provide a \code{residuals} method.  (Note that the method is
for \samp{residuals} and not \samp{resid}.)

Methods can make use of \code{\LinkA{naresid}{naresid}} methods to compensate
for the omission of missing values.  The default, \code{\LinkA{nls}{nls}} and
\code{\LinkA{smooth.spline}{smooth.spline}} methods do.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
residuals(object, ...)
resid(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object for which the extraction of model residuals is
meaningful.
\item[\code{...}] other arguments.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Residuals extracted from the object \code{object}.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{coefficients}{coefficients}}, \code{\LinkA{fitted.values}{fitted.values}},
\code{\LinkA{glm}{glm}}, \code{\LinkA{lm}{lm}}.

\LinkA{influence.measures}{influence.measures} for standardized (\code{\LinkA{rstandard}{rstandard}})
and studentized (\code{\LinkA{rstudent}{rstudent}}) residuals.
\end{SeeAlso}
\HeaderA{runmed}{Running Medians -- Robust Scatter Plot Smoothing}{runmed}
\keyword{smooth}{runmed}
\keyword{robust}{runmed}
%
\begin{Description}\relax
Compute running medians of odd span.  This is the `most robust'
scatter plot smoothing possible.  For efficiency (and historical
reason), you can use one of two different algorithms giving identical
results.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
runmed(x, k, endrule = c("median", "keep", "constant"),
       algorithm = NULL, print.level = 0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric vector, the `dependent' variable to be
smoothed.
\item[\code{k}] integer width of median window; must be odd.  Turlach had a
default of \code{k <- 1 + 2 * min((n-1)\%/\% 2, ceiling(0.1*n))}.
Use \code{k = 3} for `minimal' robust smoothing eliminating
isolated outliers.
\item[\code{endrule}] character string indicating how the values at the
beginning and the end (of the data) should be treated.
\begin{description}

\item[\code{"keep"}] keeps the first and last \eqn{k_2}{} values
at both ends, where \eqn{k_2}{} is the half-bandwidth
\code{k2 = k \%/\% 2},
i.e., \code{y[j] = x[j]} for \eqn{j \in \{1,\ldots,k_2;
          n-k_2+1,\ldots,n\}}{};
\item[\code{"constant"}] copies \code{median(y[1:k2])} to the first
values and analogously for the last ones making the smoothed ends
\emph{constant};
\item[\code{"median"}] the default, smooths the ends by using
symmetrical medians of subsequently smaller bandwidth, but for
the very first and last value where Tukey's robust end-point
rule is applied, see \code{\LinkA{smoothEnds}{smoothEnds}}.

\end{description}


\item[\code{algorithm}] character string (partially matching \code{"Turlach"} or
\code{"Stuetzle"}) or the default \code{NULL}, specifying which algorithm
should be applied.  The default choice depends on \code{n = length(x)}
and \code{k} where \code{"Turlach"} will be used for larger problems.
\item[\code{print.level}] integer, indicating verboseness of algorithm;
should rarely be changed by average users.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Apart from the end values, the result \code{y = runmed(x, k)} simply has
\code{y[j] = median(x[(j-k2):(j+k2)])} (\code{k = 2*k2+1}), computed very
efficiently.

The two algorithms are internally entirely different:
\begin{description}

\item[\code{"Turlach"}] is the HÃ¤rdle--Steiger
algorithm (see Ref.) as implemented by Berwin Turlach.
A tree algorithm is used, ensuring performance \eqn{O(n \log
        k)}{} where \code{n = length(x)} which is
asymptotically optimal.
\item[\code{"Stuetzle"}] is the (older) Stuetzle--Friedman implementation
which makes use of median \emph{updating} when one observation
enters and one leaves the smoothing window.  While this performs as
\eqn{O(n \times k)}{} which is slower asymptotically, it is
considerably faster for small \eqn{k}{} or \eqn{n}{}.

\end{description}

\end{Details}
%
\begin{Value}
vector of smoothed values of the same length as \code{x} with an
\code{\LinkA{attr}{attr}}ibute \code{k} containing (the `oddified')
\code{k}.
\end{Value}
%
\begin{Author}\relax
Martin Maechler \email{maechler@stat.math.ethz.ch},
based on Fortran code from Werner Stuetzle and S-PLUS and C code from
Berwin Turlach.
\end{Author}
%
\begin{References}\relax
HÃ¤rdle, W. and Steiger, W. (1995)
[Algorithm AS 296] Optimal median smoothing,
\emph{Applied Statistics} \bold{44}, 258--264.

Jerome H. Friedman and Werner Stuetzle (1982)
\emph{Smoothing of Scatterplots};
Report, Dep. Statistics, Stanford U., Project Orion 003.

Martin Maechler (2003)
Fast Running Medians: Finite Sample and Asymptotic Optimality;
working paper available from the author.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{smoothEnds}{smoothEnds}} which implements Tukey's end point rule and
is called by default from \code{runmed(*, endrule = "median")}.
\code{\LinkA{smooth}{smooth}} uses running
medians of 3 for its compound smoothers.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

utils::example(nhtemp)
myNHT <- as.vector(nhtemp)
myNHT[20] <- 2 * nhtemp[20]
plot(myNHT, type="b", ylim = c(48,60), main = "Running Medians Example")
lines(runmed(myNHT, 7), col = "red")

## special: multiple y values for one x
plot(cars, main = "'cars' data and runmed(dist, 3)")
lines(cars, col = "light gray", type = "c")
with(cars, lines(speed, runmed(dist, k = 3), col = 2))


## nice quadratic with a few outliers
y <- ys <- (-20:20)^2
y [c(1,10,21,41)] <- c(150, 30, 400, 450)
all(y == runmed(y, 1)) # 1-neighbourhood <==> interpolation
plot(y) ## lines(y, lwd=.1, col="light gray")
lines(lowess(seq(y),y, f = .3), col = "brown")
lines(runmed(y, 7), lwd=2, col = "blue")
lines(runmed(y,11), lwd=2, col = "red")

## Lowess is not robust
y <- ys ; y[21] <- 6666 ; x <- seq(y)
col <- c("black", "brown","blue")
plot(y, col=col[1])
lines(lowess(x,y, f = .3), col = col[2])


lines(runmed(y, 7),      lwd=2, col = col[3])
legend(length(y),max(y), c("data", "lowess(y, f = 0.3)", "runmed(y, 7)"),
       xjust = 1, col = col, lty = c(0, 1,1), pch = c(1,NA,NA))
\end{ExampleCode}
\end{Examples}
\HeaderA{rWishart}{Random Wishart Distributed Matrices}{rWishart}
\keyword{multivariate}{rWishart}
%
\begin{Description}\relax
Generate \code{n} random matrices, distributed according to the
Wishart distribution with parameters \code{Sigma, df},
\eqn{W_p(\Sigma, m),\ m=\code{df},\ \Sigma=\code{Sigma}}{}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
rWishart(n, df, Sigma)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{n}] integer sample size.
\item[\code{df}] numeric parameter, ``degrees of freedom''.
\item[\code{Sigma}] positive definite (\eqn{p\times p}{}) ``scale''
matrix, the matrix parameter of the distribution.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \eqn{X_1,\dots, X_m, \ X_i\in\mathbf{R}^p}{} is
a sample of \eqn{m}{} independent multivariate Gaussians with mean (vector) 0, and
covariance matrix \eqn{\Sigma}{}, the distribution of
\eqn{M = X'X}{} is \eqn{W_p(\Sigma, m)}{}.

Consequently, the expectation of \eqn{M}{} is
\deqn{E[M] = m\times\Sigma.}{}
Further, if \code{Sigma} is scalar (\eqn{p = 1}{}), the Wishart
distribution is a scaled chi-squared (\eqn{\chi^2}{})
distribution with \code{df} degrees of freedom,
\eqn{W_1(\sigma^2, m) = \sigma^2 \chi^2_m}{}.

The component wise variance is
\deqn{\mathrm{Var}(M_{ij}) = m(\Sigma_{ij}^2 + \Sigma_{ii} \Sigma_{jj}).}{}
\end{Details}
%
\begin{Value}
a numeric \code{\LinkA{array}{array}}, say \code{R}, of dimension
\eqn{p \times p \times n}{}, where each \code{R[,{},i]} is a
positive definite matrix, a realization of the Wishart distribution
\eqn{W_p(\Sigma, m),\ \ m=\code{df},\ \Sigma=\code{Sigma}}{}.
\end{Value}
%
\begin{Author}\relax
Douglas Bates
\end{Author}
%
\begin{References}\relax
Mardia, K. V., J. T. Kent, and J. M. Bibby (1979)
\emph{Multivariate Analysis}, London: Academic Press.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{cov}{cov}}, \code{\LinkA{rnorm}{rnorm}}, \code{\LinkA{rchisq}{rchisq}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Artificial
S <- toeplitz((10:1)/10)
set.seed(11)
R <- rWishart(1000, 20, S)
dim(R)# 10 10  1000
mR <- apply(R, 1:2, mean)# ~= E[ Wish(S, 20) ] = 20 * S
stopifnot(all.equal(mR, 20*S, tol = .009))

## See Details, the variance is
Va <- 20*(S^2 + tcrossprod(diag(S)))
vR <- apply(R, 1:2, var)
stopifnot(all.equal(vR, Va, tol = 1/16))
\end{ExampleCode}
\end{Examples}
\HeaderA{scatter.smooth}{Scatter Plot with Smooth Curve Fitted by Loess}{scatter.smooth}
\aliasA{loess.smooth}{scatter.smooth}{loess.smooth}
\keyword{smooth}{scatter.smooth}
%
\begin{Description}\relax
Plot and add a smooth curve computed by \code{loess} to a scatter plot.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
scatter.smooth(x, y = NULL, span = 2/3, degree = 1,
    family = c("symmetric", "gaussian"),
    xlab = NULL, ylab = NULL,
    ylim = range(y, prediction$y, na.rm = TRUE),
    evaluation = 50, ...)

loess.smooth(x, y, span = 2/3, degree = 1,
    family = c("symmetric", "gaussian"), evaluation = 50, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x,y}] the \code{x} and \code{y} arguments provide the x and y
coordinates for the plot.  Any reasonable way of defining the
coordinates is acceptable.  See the function \code{\LinkA{xy.coords}{xy.coords}}
for details.
\item[\code{span}] smoothness parameter for \code{loess}.
\item[\code{degree}] degree of local polynomial used.
\item[\code{family}] if \code{"gaussian"} fitting is by least-squares, and if
\code{family="symmetric"} a re-descending M estimator is used.
\item[\code{xlab}] label for x axis.
\item[\code{ylab}] label for y axis.
\item[\code{ylim}] the y limits of the plot.
\item[\code{evaluation}] number of points at which to evaluate the smooth
curve.
\item[\code{...}] For \code{scatter.smooth}, graphical parameters.
For \code{loess.smooth}, control parameters passed to
\code{\LinkA{loess.control}{loess.control}}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{loess.smooth} is an auxiliary function which evaluates the
\code{loess} smooth at \code{evaluation} equally spaced points
covering the range of \code{x}.
\end{Details}
%
\begin{Value}
For \code{scatter.smooth}, none.

For \code{loess.smooth}, a list with two components, \code{x} (the
grid of evaluation points) and \code{y} (the smoothed values at the
grid points).
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{loess}{loess}}; \code{\LinkA{smoothScatter}{smoothScatter}} for scatter plots
with smoothed \emph{density} color representation.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

with(cars, scatter.smooth(speed, dist))
\end{ExampleCode}
\end{Examples}
\HeaderA{screeplot}{Screeplots}{screeplot}
\methaliasA{screeplot.default}{screeplot}{screeplot.default}
\keyword{multivariate}{screeplot}
%
\begin{Description}\relax
\code{screeplot.default} plots the variances against the number of the
principal component. This is also the \code{plot} method for classes
\code{"princomp"} and \code{"prcomp"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## Default S3 method:
screeplot(x, npcs = min(10, length(x$sdev)),
          type = c("barplot", "lines"),
          main = deparse(substitute(x)), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object containing a \code{sdev} component, such as that
returned by \code{\LinkA{princomp}{princomp}()} and \code{\LinkA{prcomp}{prcomp}()}.
\item[\code{npcs}] the number of components to be plotted.
\item[\code{type}] the type of plot.
\item[\code{main, ...}] graphics parameters.
\end{ldescription}
\end{Arguments}
%
\begin{References}\relax
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venables, W. N. and B. D. Ripley (2002).
\emph{Modern Applied Statistics with S}, Springer-Verlag.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{princomp}{princomp}} and \code{\LinkA{prcomp}{prcomp}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## The variances of the variables in the
## USArrests data vary by orders of magnitude, so scaling is appropriate
(pc.cr <- princomp(USArrests, cor = TRUE))  # inappropriate
screeplot(pc.cr)

fit <- princomp(covmat=Harman74.cor)
screeplot(fit)
screeplot(fit, npcs=24, type="lines")
\end{ExampleCode}
\end{Examples}
\HeaderA{sd}{Standard Deviation}{sd}
\keyword{univar}{sd}
%
\begin{Description}\relax
This function computes the standard deviation of the values in
\code{x}.
If \code{na.rm} is \code{TRUE} then missing values are removed before
computation proceeds.


\end{Description}
%
\begin{Usage}
\begin{verbatim}
sd(x, na.rm = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector or an \R{} object which is coercible to one
by \code{as.vector}.  Earlier versions of \R{} allowed matrices or
data frames for \code{x}, see below.
\item[\code{na.rm}] logical.  Should missing values be removed?
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Like \code{\LinkA{var}{var}} this uses denominator \eqn{n - 1}{}.

The standard deviation of a zero-length vector (after removal of
\code{NA}s if \code{na.rm = TRUE}) is not defined and gives an error.
The standard deviation of a length-one vector is \code{NA}.

In earlier versions \R{}, for a \code{\LinkA{data.frame}{data.frame} dfrm},
\code{sd(dfrm)} worked directly.  This is deprecated now, and you are
expected to use \code{\LinkA{sapply}{sapply}(dfrm, sd)} instead.
\end{Details}
%
\begin{SeeAlso}\relax
\code{\LinkA{var}{var}} for its square, and \code{\LinkA{mad}{mad}}, the most
robust alternative.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
sd(1:2) ^ 2
\end{ExampleCode}
\end{Examples}
\HeaderA{se.contrast}{Standard Errors for Contrasts in Model Terms}{se.contrast}
\methaliasA{se.contrast.aov}{se.contrast}{se.contrast.aov}
\methaliasA{se.contrast.aovlist}{se.contrast}{se.contrast.aovlist}
\keyword{models}{se.contrast}
%
\begin{Description}\relax
Returns the standard errors for one or more contrasts in an \code{aov}
object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
se.contrast(object, ...)
## S3 method for class 'aov'
se.contrast(object, contrast.obj,
           coef = contr.helmert(ncol(contrast))[, 1],
           data = NULL, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] A suitable fit, usually from \code{aov}.
\item[\code{contrast.obj}] The contrasts for which standard errors are
requested.  This can be specified via a list or via a matrix.  A
single contrast can be specified by a list of logical vectors giving
the cells to be contrasted.  Multiple contrasts should be specified
by a matrix, each column of which is a numerical contrast vector
(summing to zero).

\item[\code{coef}] used when \code{contrast.obj} is a list; it should be a
vector of the same length as the list with zero sum.  The default
value is the first Helmert contrast, which contrasts the first and
second cell means specified by the list.
\item[\code{data}] The data frame used to evaluate \code{contrast.obj}.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Contrasts are usually used to test if certain means are
significantly different; it can be easier to use \code{se.contrast}
than compute them directly from the coefficients.

In multistratum models, the contrasts can appear in more than one
stratum, in which case the standard errors are computed in the lowest
stratum and adjusted for efficiencies and comparisons between
strata. (See the comments in the note in the help for
\code{\LinkA{aov}{aov}} about using orthogonal contrasts.)  Such standard
errors are often conservative.

Suitable matrices for use with \code{coef} can be found by
calling \code{\LinkA{contrasts}{contrasts}} and indexing the columns by a factor.
\end{Details}
%
\begin{Value}
A vector giving the standard errors for each contrast.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{contrasts}{contrasts}}, \code{\LinkA{model.tables}{model.tables}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## From Venables and Ripley (2002) p.165.
N <- c(0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,0)
P <- c(1,1,0,0,0,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,0,1,1,0)
K <- c(1,0,0,1,0,1,1,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0)
yield <- c(49.5,62.8,46.8,57.0,59.8,58.5,55.5,56.0,62.8,55.8,69.5,
55.0, 62.0,48.8,45.5,44.2,52.0,51.5,49.8,48.8,57.2,59.0,53.2,56.0)

npk <- data.frame(block = gl(6,4), N = factor(N), P = factor(P),
                  K = factor(K), yield = yield)
## Set suitable contrasts.
options(contrasts=c("contr.helmert", "contr.poly"))
npk.aov1 <- aov(yield ~ block + N + K, data=npk)
se.contrast(npk.aov1, list(N == "0", N == "1"), data=npk)
# or via a matrix
cont <- matrix(c(-1,1), 2, 1, dimnames=list(NULL, "N"))
se.contrast(npk.aov1, cont[N, , drop=FALSE]/12, data=npk)

## test a multi-stratum model
npk.aov2 <- aov(yield ~ N + K + Error(block/(N + K)), data=npk)
se.contrast(npk.aov2, list(N == "0", N == "1"))


## an example looking at an interaction contrast
## Dataset from R.E. Kirk (1995)
## 'Experimental Design: procedures for the behavioral sciences'
score <- c(12, 8,10, 6, 8, 4,10,12, 8, 6,10,14, 9, 7, 9, 5,11,12,
            7,13, 9, 9, 5,11, 8, 7, 3, 8,12,10,13,14,19, 9,16,14)
A <- gl(2, 18, labels=c("a1", "a2"))
B <- rep(gl(3, 6, labels=c("b1", "b2", "b3")), 2)
fit <- aov(score ~ A*B)
cont <- c(1, -1)[A] * c(1, -1, 0)[B]
sum(cont)       # 0
sum(cont*score) # value of the contrast
se.contrast(fit, as.matrix(cont))
(t.stat <- sum(cont*score)/se.contrast(fit, as.matrix(cont)))
summary(fit, split=list(B=1:2), expand.split = TRUE)
## t.stat^2 is the F value on the A:B: C1 line (with Helmert contrasts)
## Now look at all three interaction contrasts
cont <- c(1, -1)[A] * cbind(c(1, -1, 0), c(1, 0, -1), c(0, 1, -1))[B,]
se.contrast(fit, cont)  # same, due to balance.
rm(A,B,score)


## multi-stratum example where efficiencies play a role
utils::example(eff.aovlist)
fit <- aov(Yield ~ A + B * C + Error(Block), data = aovdat)
cont1 <- c(-1, 1)[A]/32  # Helmert contrasts
cont2 <- c(-1, 1)[B] * c(-1, 1)[C]/32
cont <- cbind(A=cont1, BC=cont2)
colSums(cont*Yield) # values of the contrasts
se.contrast(fit, as.matrix(cont))
## Not run: # comparison with lme
library(nlme)
fit2 <- lme(Yield ~ A + B*C, random = ~1 | Block, data = aovdat)
summary(fit2)$tTable # same estimates, similar (but smaller) se's.

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{selfStart}{Construct Self-starting Nonlinear Models}{selfStart}
\methaliasA{selfStart.default}{selfStart}{selfStart.default}
\methaliasA{selfStart.formula}{selfStart}{selfStart.formula}
\keyword{models}{selfStart}
%
\begin{Description}\relax
Construct self-starting nonlinear models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
selfStart(model, initial, parameters, template)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] a function object defining a nonlinear model or
a nonlinear formula object of the form \code{\textasciitilde{}expression}.
\item[\code{initial}] a function object, taking three arguments: \code{mCall},
\code{data}, and \code{LHS}, representing, respectively, a matched
call to the function \code{model}, a data frame in
which to interpret the variables in \code{mCall}, and the expression
from the left-hand side of the model formula in the call to \code{nls}.
This function should return initial values for the parameters in
\code{model}.
\item[\code{parameters}] a character vector specifying the terms on the right
hand side of \code{model} for which initial estimates should be
calculated.  Passed as the \code{namevec} argument to the
\code{deriv} function.
\item[\code{template}] an optional prototype for the calling sequence of the
returned object, passed as the \code{function.arg} argument to the
\code{deriv} function.  By default, a template is generated with the
covariates in \code{model} coming first and the parameters in
\code{model} coming last in the calling sequence.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This function is generic; methods functions can be written to handle
specific classes of objects.
\end{Details}
%
\begin{Value}
a function object of class \code{"selfStart"}, for the \code{formula}
method obtained by applying
\code{deriv} to the right hand side of the \code{model} formula.  An
\code{initial} attribute (defined by the \code{initial} argument) is
added to the function to calculate starting estimates for the
parameters in the model automatically.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}.
Each of the following are \code{"selfStart"} models (with examples)
\code{\LinkA{SSasymp}{SSasymp}}, \code{\LinkA{SSasympOff}{SSasympOff}}, \code{\LinkA{SSasympOrig}{SSasympOrig}},
\code{\LinkA{SSbiexp}{SSbiexp}}, \code{\LinkA{SSfol}{SSfol}},\code{\LinkA{SSfpl}{SSfpl}},
\code{\LinkA{SSgompertz}{SSgompertz}}, \code{\LinkA{SSlogis}{SSlogis}}, \code{\LinkA{SSmicmen}{SSmicmen}},
\code{\LinkA{SSweibull}{SSweibull}}

\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## self-starting logistic model

SSlogis <- selfStart(~ Asym/(1 + exp((xmid - x)/scal)),
  function(mCall, data, LHS)
  {
    xy <- sortedXyData(mCall[["x"]], LHS, data)
    if(nrow(xy) < 4) {
      stop("Too few distinct x values to fit a logistic")
    }
    z <- xy[["y"]]
    if (min(z) <= 0) { z <- z + 0.05 * max(z) } # avoid zeroes
    z <- z/(1.05 * max(z))              # scale to within unit height
    xy[["z"]] <- log(z/(1 - z))         # logit transformation
    aux <- coef(lm(x ~ z, xy))
    parameters(xy) <- list(xmid = aux[1], scal = aux[2])
    pars <- as.vector(coef(nls(y ~ 1/(1 + exp((xmid - x)/scal)),
                             data = xy, algorithm = "plinear")))
    value <- c(pars[3], pars[1], pars[2])
    names(value) <- mCall[c("Asym", "xmid", "scal")]
    value
  }, c("Asym", "xmid", "scal"))

# 'first.order.log.model' is a function object defining a first order
# compartment model
# 'first.order.log.initial' is a function object which calculates initial
# values for the parameters in 'first.order.log.model'

# self-starting first order compartment model
## Not run: 
SSfol <- selfStart(first.order.log.model, first.order.log.initial)

## End(Not run)

## Explore the self-starting models already available in R's  "stats":
pos.st <- which("package:stats" == search())
mSS <- apropos("^SS..", where=TRUE, ignore.case=FALSE)
(mSS <- unname(mSS[names(mSS) == pos.st]))
fSS <- sapply(mSS, get, pos = pos.st, mode = "function")
all(sapply(fSS, inherits, "selfStart"))# -> TRUE

## Show the argument list of each self-starting function:
str(fSS, give.attr=FALSE)
\end{ExampleCode}
\end{Examples}
\HeaderA{setNames}{Set the Names in an Object}{setNames}
\keyword{list}{setNames}
%
\begin{Description}\relax
This is a convenience function that sets the names on an object and
returns the object.  It is most useful at the end of a function
definition where one is creating the object to be returned and would
prefer not to store it under a name just so the names can be assigned.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
setNames(object, nm)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object for which a \code{names} attribute will be meaningful 
\item[\code{nm}] a character vector of names to assign to the object
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An object of the same sort as \code{object} with the new names assigned.
\end{Value}
%
\begin{Author}\relax
Douglas M. Bates and Saikat DebRoy 
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{unname}{unname}} for removing names.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
setNames( 1:3, c("foo", "bar", "baz") )
# this is just a short form of
tmp <- 1:3
names(tmp) <-  c("foo", "bar", "baz")
tmp
\end{ExampleCode}
\end{Examples}
\HeaderA{shapiro.test}{Shapiro-Wilk Normality Test}{shapiro.test}
\keyword{htest}{shapiro.test}
%
\begin{Description}\relax
Performs the Shapiro-Wilk test of normality.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
shapiro.test(x)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector of data values. Missing values are allowed,
but the number of non-missing values must be between 3 and 5000.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the Shapiro-Wilk statistic.
\item[\code{p.value}] an approximate p-value for the test.  This is
said in Royston (1995) to be adequate for \code{p.value < 0.1}.
\item[\code{method}] the character string \code{"Shapiro-Wilk normality test"}.
\item[\code{data.name}] a character string giving the name(s) of the data.
\end{ldescription}
\end{Value}
%
\begin{Source}\relax
The algorithm used is a C translation of the Fortran code described in
Royston (1995) and found at \url{http://lib.stat.cmu.edu/apstat/R94}.
The calculation of the p value is exact for \eqn{n = 3}{}, otherwise
approximations are used, separately for \eqn{4 \le n \le 11}{} and
\eqn{n \ge 12}{}.
\end{Source}
%
\begin{References}\relax
Patrick Royston (1982)
An extension of Shapiro and Wilk's \eqn{W}{} test for normality to large
samples.
\emph{Applied Statistics}, \bold{31}, 115--124.

Patrick Royston (1982)
Algorithm AS 181: The \eqn{W}{} test for Normality.
\emph{Applied Statistics}, \bold{31}, 176--180.

Patrick Royston (1995)
Remark AS R94: A remark on Algorithm AS 181: The \eqn{W}{} test for normality.
\emph{Applied Statistics}, \bold{44}, 547--551.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{qqnorm}{qqnorm}} for producing a normal quantile-quantile plot.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
shapiro.test(rnorm(100, mean = 5, sd = 3))
shapiro.test(runif(100, min = 2, max = 4))
\end{ExampleCode}
\end{Examples}
\HeaderA{SignRank}{Distribution of the Wilcoxon Signed Rank Statistic}{SignRank}
\aliasA{dsignrank}{SignRank}{dsignrank}
\aliasA{psignrank}{SignRank}{psignrank}
\aliasA{qsignrank}{SignRank}{qsignrank}
\aliasA{rsignrank}{SignRank}{rsignrank}
\keyword{distribution}{SignRank}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the distribution of the Wilcoxon Signed Rank statistic
obtained from a sample with size \code{n}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dsignrank(x, n, log = FALSE)
psignrank(q, n, lower.tail = TRUE, log.p = FALSE)
qsignrank(p, n, lower.tail = TRUE, log.p = FALSE)
rsignrank(nn, n)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x,q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{nn}] number of observations. If \code{length(nn) > 1}, the length
is taken to be the number required.
\item[\code{n}] number(s) of observations in the sample(s).  A positive
integer, or a vector of such integers.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This distribution is obtained as follows.  Let \code{x} be a sample of
size \code{n} from a continuous distribution symmetric about the
origin.  Then the Wilcoxon signed rank statistic is the sum of the
ranks of the absolute values \code{x[i]} for which \code{x[i]} is
positive.  This statistic takes values between \eqn{0}{} and
\eqn{n(n+1)/2}{}, and its mean and variance are \eqn{n(n+1)/4}{} and
\eqn{n(n+1)(2n+1)/24}{}, respectively.

If either of the first two arguments is a vector, the recycling rule is
used to do the calculations for all combinations of the two up to
the length of the longer vector.
\end{Details}
%
\begin{Value}
\code{dsignrank} gives the density,
\code{psignrank} gives the distribution function,
\code{qsignrank} gives the quantile function, and
\code{rsignrank} generates random deviates.
\end{Value}
%
\begin{Author}\relax
Kurt Hornik; efficiency improvement by Ivo Ugrina.
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{wilcox.test}{wilcox.test}} to calculate the statistic from data, find p
values and so on.

\LinkA{Distributions}{Distributions} for standard distributions, including
\code{\LinkA{dwilcox}{dwilcox}} for the distribution of \emph{two-sample}
Wilcoxon rank sum statistic.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

par(mfrow=c(2,2))
for(n in c(4:5,10,40)) {
  x <- seq(0, n*(n+1)/2, length=501)
  plot(x, dsignrank(x,n=n), type='l', main=paste("dsignrank(x,n=",n,")"))
}

\end{ExampleCode}
\end{Examples}
\HeaderA{simulate}{Simulate Responses}{simulate}
\keyword{models}{simulate}
\keyword{datagen}{simulate}
%
\begin{Description}\relax
Simulate one or more responses from the distribution
corresponding to a fitted model object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
simulate(object, nsim = 1, seed = NULL, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object representing a fitted model.
\item[\code{nsim}] number of response vectors to simulate.  Defaults to \code{1}.
\item[\code{seed}] an object specifying if and how the random number
generator should be initialized (`seeded').\\{}
For the "lm" method, either \code{NULL} or an integer that will be
used in a call to \code{set.seed} before simulating the response
vectors.  If set, the value is saved as the \code{"seed"} attribute
of the returned value.  The default, \code{NULL} will not change the
random generator state, and return \code{\LinkA{.Random.seed}{.Random.seed}} as the
\code{"seed"} attribute, see `Value'.

\item[\code{...}] additional optional arguments.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function.  Consult the individual modeling functions
for details on how to use this function.

Package \pkg{stats} has a method for \code{"lm"} objects which is used
for \code{\LinkA{lm}{lm}} and \code{\LinkA{glm}{glm}} fits.  There is a method
for fits from \code{glm.nb} in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}, and hence the
case of negative binomial families is not covered by the \code{"lm"}
method.

The methods for linear models fitted by \code{lm} or \code{glm(family
  = "gaussian")} assume that any weights which have been supplied are
inversely proportional to the error variance.  For other GLMs the
(optional) \code{simulate} component of the \code{\LinkA{family}{family}}
object is used---there is no appropriate simulation method for
`quasi' models as they are specified only up to two moments.

For binomial and Poisson GLMs the dispersion is fixed at one.  Integer
prior weights \eqn{w_i}{} can be interpreted as meaning that
observation \eqn{i}{} is an average of \eqn{w_i}{} observations, which is
natural for binomials specified as proportions but less so for a
Poisson, for which prior weights are ignored with a warning.

For a gamma GLM the shape parameter is estimated by maximum likelihood
(using function \code{\LinkA{gamma.shape}{gamma.shape}} in package
\Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}).  The interpretation of weights is as multipliers to a
basic shape parameter, since dispersion is inversely proportional to
shape.

For an inverse gaussian GLM the model assumed is
\eqn{IG(\mu_i, \lambda w_i)}{} (see
\url{http://en.wikipedia.org/wiki/Inverse_Gaussian_distribution})
where \eqn{\lambda}{} is estimated by the inverse of the dispersion
estimate for the fit.  The variance is
\eqn{\mu_i^3/(\lambda w_i)}{} and
hence inversely proportional to the prior weights.  The simulation is
done by function \code{\LinkA{rinvGauss}{rinvGauss}} from the
\Rhref{http://CRAN.R-project.org/package=SuppDists}{\pkg{SuppDists}} package, which must be installed.
\end{Details}
%
\begin{Value}
Typically, a list of length \code{nsim} of simulated responses.  Where
appropriate the result can be a data frame (which is a special type of
list).



For the \code{"lm"} method, the result is a data frame with an
attribute \code{"seed"}.  If argument \code{seed} is \code{NULL}, the
attribute is the value of \code{\LinkA{.Random.seed}{.Random.seed}} before the
simulation was started; otherwise it is the value of the argument with
a \code{"kind"} attribute with value \code{as.list(\LinkA{RNGkind}{RNGkind}())}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{RNG}{RNG}} about random number generation in \R{},
\code{\LinkA{fitted.values}{fitted.values}} and \code{\LinkA{residuals}{residuals}} for related methods;
\code{\LinkA{glm}{glm}}, \code{\LinkA{lm}{lm}} for model fitting.

There are further examples in the \file{simulate.R} tests file in the
sources for package \pkg{stats}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:5
mod1 <- lm(c(1:3, 7, 6) ~ x)
S1 <- simulate(mod1, nsim = 4)
## repeat the simulation:
.Random.seed <- attr(S1, "seed")
identical(S1, simulate(mod1, nsim = 4))

S2 <- simulate(mod1, nsim = 200, seed = 101)
rowMeans(S2) # should be about the same as
fitted(mod1)

## repeat identically:
(sseed <- attr(S2, "seed")) # seed; RNGkind as attribute
stopifnot(identical(S2, simulate(mod1, nsim = 200, seed = sseed)))

## To be sure about the proper RNGkind, e.g., after
RNGversion("2.7.0")
## first set the RNG kind, then simulate
do.call(RNGkind, attr(sseed, "kind"))
identical(S2, simulate(mod1, nsim = 200, seed = sseed))

## Binomial GLM examples
yb1 <- matrix(c(4, 4, 5, 7, 8, 6, 6, 5, 3, 2), ncol = 2)
modb1 <- glm(yb1 ~ x, family = binomial)
S3 <- simulate(modb1, nsim = 4)
# each column of S3 is a two-column matrix.

x2 <- sort(runif(100))
yb2 <- rbinom(100, prob = plogis(2*(x2-1)), size = 1)
yb2 <- factor(1 + yb2, labels = c("failure", "success"))
modb2 <- glm(yb2 ~ x2, family = binomial)
S4 <- simulate(modb2, nsim = 4)
# each column of S4 is a factor
\end{ExampleCode}
\end{Examples}
\HeaderA{smooth}{Tukey's (Running Median) Smoothing}{smooth}
\keyword{robust}{smooth}
\keyword{smooth}{smooth}
%
\begin{Description}\relax
Tukey's smoothers, \emph{3RS3R}, \emph{3RSS}, \emph{3R}, etc.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
smooth(x, kind = c("3RS3R", "3RSS", "3RSR", "3R", "3", "S"),
       twiceit = FALSE, endrule = "Tukey", do.ends = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a vector or time series
\item[\code{kind}] a character string indicating the kind of smoother required;
defaults to \code{"3RS3R"}.
\item[\code{twiceit}] logical, indicating if the result should be `twiced'.
Twicing a smoother \eqn{S(y)}{} means \eqn{S(y) + S(y - S(y))}{}, i.e.,
adding smoothed residuals to the smoothed values.  This decreases
bias (increasing variance).
\item[\code{endrule}] a character string indicating the rule for smoothing at the
boundary.  Either \code{"Tukey"} (default) or \code{"copy"}.
\item[\code{do.ends}] logical, indicating if the 3-splitting of ties should
also happen at the boundaries (ends).  This is only used for
\code{kind = "S"}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\emph{\code{3}} is Tukey's short notation for running \code{\LinkA{median}{median}}s
of length \bold{3},
\\{}
\emph{\code{3R}} stands for \bold{R}epeated \emph{\code{3}} until
convergence, and
\\{}
\emph{\code{S}} for \bold{S}plitting of horizontal stretches of length 2 or 3.

Hence, \emph{\code{3RS3R}} is a concatenation of \code{3R}, \code{S}
and \code{3R}, \emph{\code{3RSS}} similarly,
whereas \emph{\code{3RSR}} means first \code{3R}
and then \code{(S and 3)} \bold{R}epeated until convergence -- which
can be bad.
\end{Details}
%
\begin{Value}
An object of class \code{"tukeysmooth"} (which has \code{print} and
\code{summary} methods) and is a vector or time series containing the
smoothed values with additional attributes.
\end{Value}
%
\begin{Note}\relax
S and S-PLUS use a different (somewhat better) Tukey smoother in
\code{smooth(*)}.
Note that there are other smoothing methods which provide
rather better results.  These were designed for hand calculations
and may be used mainly for didactical purposes.

Since \R{} version 1.2, \code{smooth} \emph{does} really implement
Tukey's end-point rule correctly (see argument \code{endrule}).

\code{kind = "3RSR"} has been the default till \R{}-1.1,
but it can have very bad properties, see the examples.

Note that repeated application of \code{smooth(*)} \emph{does}
smooth more, for the \code{"3RS*"} kinds.
\end{Note}
%
\begin{References}\relax
Tukey, J. W. (1977).
\emph{Exploratory Data Analysis},
Reading Massachusetts: Addison-Wesley.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{runmed}{runmed}} for running medians;
\code{\LinkA{lowess}{lowess}} and \code{\LinkA{loess}{loess}};
\code{\LinkA{supsmu}{supsmu}} and
\code{\LinkA{smooth.spline}{smooth.spline}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## see also   demo(smooth) !

x1 <- c(4, 1, 3, 6, 6, 4, 1, 6, 2, 4, 2) # very artificial
(x3R <- smooth(x1, "3R")) # 2 iterations of "3"
smooth(x3R, kind = "S")

sm.3RS <- function(x, ...)
   smooth(smooth(x, "3R", ...), "S", ...)

y <- c(1,1, 19:1)
plot(y, main = "misbehaviour of \"3RSR\"", col.main = 3)
lines(sm.3RS(y))
lines(smooth(y))
lines(smooth(y, "3RSR"), col = 3, lwd = 2)# the horror

x <- c(8:10,10, 0,0, 9,9)
plot(x, main = "breakdown of  3R  and  S  and hence  3RSS")
matlines(cbind(smooth(x,"3R"),smooth(x,"S"), smooth(x,"3RSS"),smooth(x)))

presidents[is.na(presidents)] <- 0 # silly
summary(sm3 <- smooth(presidents, "3R"))
summary(sm2 <- smooth(presidents,"3RSS"))
summary(sm  <- smooth(presidents))

all.equal(c(sm2),c(smooth(smooth(sm3, "S"), "S"))) # 3RSS  === 3R S S
all.equal(c(sm), c(smooth(smooth(sm3, "S"), "3R")))# 3RS3R === 3R S 3R

plot(presidents, main = "smooth(presidents0, *) :  3R and default 3RS3R")
lines(sm3,col = 3, lwd = 1.5)
lines(sm, col = 2, lwd = 1.25)
\end{ExampleCode}
\end{Examples}
\HeaderA{smooth.spline}{Fit a Smoothing Spline}{smooth.spline}
\keyword{smooth}{smooth.spline}
%
\begin{Description}\relax
Fits a cubic smoothing spline to the supplied data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
smooth.spline(x, y = NULL, w = NULL, df, spar = NULL,
              cv = FALSE, all.knots = FALSE, nknots = NULL,
              keep.data = TRUE, df.offset = 0, penalty = 1,
              control.spar = list(), tol = 1e-6 * IQR(x))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a vector giving the values of the predictor variable, or  a
list or a two-column matrix specifying x and y. 
\item[\code{y}] responses. If \code{y} is missing or \code{NULL}, the responses
are assumed to be specified by \code{x}, with \code{x} the index
vector.
\item[\code{w}] optional vector of weights of the same length as \code{x};
defaults to all 1.
\item[\code{df}] the desired equivalent number of degrees of freedom (trace of
the smoother matrix).
\item[\code{spar}] smoothing parameter, typically (but not necessarily) in
\eqn{(0,1]}{}.  The coefficient \eqn{\lambda}{} of the integral of the
squared second derivative in the fit (penalized log likelihood)
criterion is a monotone function of \code{spar}, see the details
below.
\item[\code{cv}] ordinary (\code{TRUE}) or `generalized' cross-validation
(GCV) when \code{FALSE}; setting it to \code{NA} skips the evaluation
of leverages and any score.
\item[\code{all.knots}] if \code{TRUE}, all distinct points in \code{x} are used as
knots.  If \code{FALSE} (default), a subset of \code{x[]} is used,
specifically \code{x[j]} where the \code{nknots} indices are evenly
spaced in \code{1:n}, see also the next argument \code{nknots}.
\item[\code{nknots}] integer giving the number of knots to use when
\code{all.knots=FALSE}.  Per default, this is less than \eqn{n}{}, the
number of unique \code{x} values for \eqn{n > 49}{}.
\item[\code{keep.data}] logical specifying if the input data should be kept
in the result.  If \code{TRUE} (as per default), fitted values and
residuals are available from the result.
\item[\code{df.offset}] allows the degrees of freedom to be increased by
\code{df.offset} in the GCV criterion.
\item[\code{penalty}] the coefficient of the penalty for degrees of freedom
in the GCV criterion.

\item[\code{control.spar}] optional list with named components controlling the
root finding when the smoothing parameter \code{spar} is computed,
i.e., missing or \code{NULL}, see below.

\bold{Note} that this is partly \emph{experimental} and may change
with general spar computation improvements!

\begin{description}

\item[low:] lower bound for \code{spar}; defaults to -1.5 (used to
implicitly default to 0 in \R{} versions earlier than 1.4).
\item[high:] upper bound for \code{spar}; defaults to +1.5.
\item[tol:] the absolute precision (\bold{tol}erance) used; defaults
to 1e-4 (formerly 1e-3).
\item[eps:] the relative precision used; defaults to 2e-8 (formerly
0.00244).
\item[trace:] logical indicating if iterations should be traced.
\item[maxit:] integer giving the maximal number of iterations;
defaults to 500.

\end{description}

Note that \code{spar} is only searched for in the interval
\eqn{[low, high]}{}.


\item[\code{tol}] A tolerance for same-ness of the \code{x} values.  The
values are binned into bins of size \code{tol} and values which
fall into the same bin are regarded as the same.
Must be strictly positive (and finite).
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Neither \code{x} nor \code{y} are allowed to containing missing or
infinite values.

The \code{x} vector should contain at least four distinct values.
`Distinct' here is controlled by \code{tol}: values which are
regarded as the same are replaced by the first of their values and the
corresponding \code{y} and \code{w} are pooled accordingly.

The computational \eqn{\lambda}{} used (as a function of
\eqn{s=spar}{}) is
\eqn{\lambda = r * 256^{3 s - 1}}{}
where
\eqn{r = tr(X' W X) / tr(\Sigma)}{},
\eqn{\Sigma}{} is the matrix given by
\eqn{\Sigma_{ij} = \int B_i''(t) B_j''(t) dt}{},
\eqn{X}{} is given by \eqn{X_{ij} = B_j(x_i)}{},
\eqn{W}{} is the diagonal matrix of weights (scaled such that
its trace is \eqn{n}{}, the original number of observations)
and \eqn{B_k(.)}{} is the \eqn{k}{}-th B-spline.

Note that with these definitions, \eqn{f_i = f(x_i)}{}, and the B-spline
basis representation \eqn{f = X c}{} (i.e., \eqn{c}{} is
the vector of spline coefficients), the penalized log likelihood is
\eqn{L = (y - f)' W (y - f) + \lambda c' \Sigma c}{}, and hence
\eqn{c}{} is the solution of the (ridge regression)
\eqn{(X' W X + \lambda \Sigma) c = X' W y}{}.

If \code{spar} is missing or \code{NULL}, the value of \code{df} is used to
determine the degree of smoothing.  If both are missing, leave-one-out
cross-validation (ordinary or `generalized' as determined by
\code{cv}) is used to determine \eqn{\lambda}{}.
Note that from the above relation,







\code{spar} is \eqn{s = s0 + 0.0601 * \bold{\log}\lambda}{},
which is intentionally \emph{different} from the S-PLUS implementation
of \code{smooth.spline} (where \code{spar} is proportional to
\eqn{\lambda}{}).  In \R{}'s (\eqn{\log \lambda}{}) scale, it makes more
sense to vary \code{spar} linearly.

Note however that currently the results may become very unreliable
for \code{spar} values smaller than about -1 or -2.  The same may
happen for values larger than 2 or so. Don't think of setting
\code{spar} or the controls \code{low} and \code{high} outside such a
safe range, unless you know what you are doing!

The `generalized' cross-validation method will work correctly when
there are duplicated points in \code{x}.  However, it is ambiguous what
leave-one-out cross-validation means with duplicated points, and the
internal code uses an approximation that involves leaving out groups
of duplicated points.  \code{cv=TRUE} is best avoided in that case.
\end{Details}
%
\begin{Value}
An object of class \code{"smooth.spline"} with components
\begin{ldescription}
\item[\code{x}] the \emph{distinct} \code{x} values in increasing order, see
the `Details' above.
\item[\code{y}] the fitted values corresponding to \code{x}.
\item[\code{w}] the weights used at the unique values of \code{x}.
\item[\code{yin}] the y values used at the unique \code{y} values.
\item[\code{data}] only if \code{keep.data = TRUE}: itself a
\code{\LinkA{list}{list}} with components \code{x}, \code{y} and \code{w}
of the same length.  These are the original \eqn{(x_i,y_i,w_i),
      i=1,\dots,n}{}, values where \code{data\$x} may have repeated values and
hence be longer than the above \code{x} component; see details.

\item[\code{lev}] (when \code{cv} was not \code{NA}) leverages, the diagonal
values of the smoother matrix.
\item[\code{cv.crit}] cross-validation score, `generalized' or true, depending
on \code{cv}.
\item[\code{pen.crit}] penalized criterion
\item[\code{crit}] the criterion value minimized in the underlying
\code{.Fortran} routine \file{sslvrg}.
\item[\code{df}] equivalent degrees of freedom used.  Note that (currently)
this value may become quite imprecise when the true \code{df} is
between and 1 and 2.

\item[\code{spar}] the value of \code{spar} computed or given.
\item[\code{lambda}] the value of \eqn{\lambda}{} corresponding to \code{spar},
see the details above.
\item[\code{iparms}] named integer(3) vector where \code{..\$ipars["iter"]}
gives number of spar computing iterations used.
\item[\code{fit}] list for use by \code{\LinkA{predict.smooth.spline}{predict.smooth.spline}}, with
components
\begin{description}

\item[knot:] the knot sequence (including the repeated boundary
knots).
\item[nk:] number of coefficients or number of `proper'
knots plus 2.
\item[coef:] coefficients for the spline basis used.
\item[min, range:] numbers giving the corresponding quantities of
\code{x}.

\end{description}


\item[\code{call}] the matched call.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
The default \code{all.knots = FALSE} and \code{nknots = NULL} entails
using only \eqn{O(n^{0.2})}{}
knots instead of \eqn{n}{} for \eqn{n > 49}{}.  This cuts speed and memory
requirements, but not drastically anymore since \R{} version 1.5.1 where
it is only \eqn{O(n_k) + O(n)}{} where \eqn{n_k}{} is
the number of knots.
In this case where not all unique \code{x} values are
used as knots, the result is not a smoothing spline in the strict
sense, but very close unless a small smoothing parameter (or large
\code{df}) is used.
\end{Note}
%
\begin{Author}\relax
\R{} implementation by B. D. Ripley and Martin Maechler
(\code{spar/lambda}, etc).
\end{Author}
%
\begin{Source}\relax
This function is based on code in the \code{GAMFIT} Fortran program by
T. Hastie and R. Tibshirani (\url{http://lib.stat.cmu.edu/general/}),
which makes use of spline code by Finbarr O'Sullivan.  Its design
parallels the \code{smooth.spline} function of Chambers \& Hastie (1992).
\end{Source}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}, Wadsworth \& Brooks/Cole.

Green, P. J. and Silverman, B. W. (1994)
\emph{Nonparametric Regression and Generalized Linear Models:
A Roughness Penalty Approach.} Chapman and Hall.

Hastie, T. J. and Tibshirani, R. J. (1990)
\emph{Generalized Additive Models.}  Chapman and Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{predict.smooth.spline}{predict.smooth.spline}} for evaluating the spline
and its derivatives.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

attach(cars)
plot(speed, dist, main = "data(cars)  &  smoothing splines")
cars.spl <- smooth.spline(speed, dist)
(cars.spl)
## This example has duplicate points, so avoid cv=TRUE

lines(cars.spl, col = "blue")
lines(smooth.spline(speed, dist, df=10), lty=2, col = "red")
legend(5,120,c(paste("default [C.V.] => df =",round(cars.spl$df,1)),
               "s( * , df = 10)"), col = c("blue","red"), lty = 1:2,
       bg='bisque')
detach()


## Residual (Tukey Anscombe) plot:
plot(residuals(cars.spl) ~ fitted(cars.spl))
abline(h = 0, col="gray")

## consistency check:
stopifnot(all.equal(cars$dist,
                    fitted(cars.spl) + residuals(cars.spl)))

##-- artificial example
y18 <- c(1:3,5,4,7:3,2*(2:5),rep(10,4))
xx  <- seq(1,length(y18), len=201)
(s2  <- smooth.spline(y18)) # GCV
(s02  <- smooth.spline(y18, spar = 0.2))
(s02. <- smooth.spline(y18, spar = 0.2, cv=NA))
plot(y18, main=deparse(s2$call), col.main=2)
lines(s2, col = "gray"); lines(predict(s2, xx), col = 2)
lines(predict(s02, xx), col = 3); mtext(deparse(s02$call), col = 3)



## The following shows the problematic behavior of 'spar' searching:
(s2  <- smooth.spline(y18, control =
                      list(trace = TRUE, tol = 1e-6, low = -1.5)))
(s2m <- smooth.spline(y18, cv = TRUE, control =
                      list(trace = TRUE, tol = 1e-6, low = -1.5)))
## both above do quite similarly (Df = 8.5 +- 0.2)
\end{ExampleCode}
\end{Examples}
\HeaderA{smoothEnds}{End Points Smoothing (for Running Medians)}{smoothEnds}
\keyword{smooth}{smoothEnds}
\keyword{robust}{smoothEnds}
%
\begin{Description}\relax
Smooth end points of a vector \code{y} using subsequently smaller
medians and Tukey's end point rule at the very end. (of odd span),
\end{Description}
%
\begin{Usage}
\begin{verbatim}
smoothEnds(y, k = 3)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{y}] dependent variable to be smoothed (vector).
\item[\code{k}] width of largest median window; must be odd.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{smoothEnds} is used to only do the `end point smoothing',
i.e., change at most the observations closer to the beginning/end
than half the window \code{k}.  The first and last value are computed using
\emph{Tukey's end point rule}, i.e.,
\code{sm[1] = median(y[1], sm[2], 3*sm[2] - 2*sm[3])}.
\end{Details}
%
\begin{Value}
vector of smoothed values, the same length as \code{y}.
\end{Value}
%
\begin{Author}\relax
Martin Maechler
\end{Author}
%
\begin{References}\relax
John W. Tukey (1977)
\emph{Exploratory Data Analysis}, Addison.

Velleman, P.F., and Hoaglin, D.C. (1981)
\emph{ABC of EDA (Applications, Basics, and Computing of Exploratory
Data Analysis)}; Duxbury.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{runmed}{runmed}(*, endrule = "median")} which calls
\code{smoothEnds()}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

y <- ys <- (-20:20)^2
y [c(1,10,21,41)] <-  c(100, 30, 400, 470)
s7k <- runmed(y,7, endrule = "keep")
s7. <- runmed(y,7, endrule = "const")
s7m <- runmed(y,7)
col3 <- c("midnightblue","blue","steelblue")
plot(y, main = "Running Medians -- runmed(*, k=7, end.rule = X)")
lines(ys, col = "light gray")
matlines(cbind(s7k,s7.,s7m), lwd= 1.5, lty = 1, col = col3)
legend(1,470, paste("endrule",c("keep","constant","median"),sep=" = "),
       col = col3, lwd = 1.5, lty = 1)

stopifnot(identical(s7m, smoothEnds(s7k, 7)))
\end{ExampleCode}
\end{Examples}
\HeaderA{sortedXyData}{Create a \code{sortedXyData} Object}{sortedXyData}
\methaliasA{sortedXyData.default}{sortedXyData}{sortedXyData.default}
\keyword{manip}{sortedXyData}
%
\begin{Description}\relax
This is a constructor function for the class of \code{sortedXyData}
objects.  These objects are mostly used in the \code{initial}
function for a self-starting nonlinear regression model, which will be
of the \code{selfStart} class.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
sortedXyData(x, y, data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}]  a numeric vector or an expression that will evaluate in
\code{data} to a numeric vector 
\item[\code{y}]  a numeric vector or an expression that will evaluate in
\code{data} to a numeric vector 
\item[\code{data}]  an optional data frame in which to evaluate expressions
for \code{x} and \code{y}, if they are given as expressions 
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A \code{sortedXyData} object. This is a data frame with exactly
two numeric columns, named \code{x} and \code{y}.  The rows are
sorted so the \code{x} column is in increasing order.  Duplicate
\code{x} values are eliminated by averaging the corresponding \code{y}
values.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{selfStart}{selfStart}}, \code{\LinkA{NLSstClosestX}{NLSstClosestX}},
\code{\LinkA{NLSstLfAsymptote}{NLSstLfAsymptote}}, \code{\LinkA{NLSstRtAsymptote}{NLSstRtAsymptote}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
DNase.2 <- DNase[ DNase$Run == "2", ]
sortedXyData( expression(log(conc)), expression(density), DNase.2 )
\end{ExampleCode}
\end{Examples}
\HeaderA{spec.ar}{Estimate Spectral Density of a Time Series from AR Fit}{spec.ar}
\keyword{ts}{spec.ar}
%
\begin{Description}\relax
Fits an AR model to \code{x} (or uses the existing fit) and computes
(and by default plots) the spectral density of the fitted model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
spec.ar(x, n.freq, order = NULL, plot = TRUE, na.action = na.fail,
        method = "yule-walker", ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A univariate (not yet:or multivariate) time series or the
result of a fit by \code{\LinkA{ar}{ar}}.
\item[\code{n.freq}] The number of points at which to plot.
\item[\code{order}] The order of the AR model to be fitted.  If omitted,
the order is chosen by AIC.
\item[\code{plot}] Plot the periodogram?
\item[\code{na.action}] \code{NA} action function.
\item[\code{method}] \code{method} for \code{\LinkA{ar}{ar}} fit.
\item[\code{...}] Graphical arguments passed to \code{\LinkA{plot.spec}{plot.spec}}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An object of class \code{"spec"}.
The result is returned invisibly if \code{plot} is true.
\end{Value}
%
\begin{Section}{Warning}
Some authors, for example Thomson (1990), warn strongly
that AR spectra can be misleading.
\end{Section}
%
\begin{Note}\relax
The multivariate case is not yet implemented.
\end{Note}
%
\begin{References}\relax
Thompson, D.J. (1990) Time series analysis of Holocene climate data.
\emph{Phil. Trans. Roy. Soc. A} \bold{330}, 601--616.

Venables, W.N. and Ripley, B.D. (2002) \emph{Modern Applied
Statistics with S.} Fourth edition. Springer. (Especially
page 402.)
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ar}{ar}}, \code{\LinkA{spectrum}{spectrum}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

spec.ar(lh)

spec.ar(ldeaths)
spec.ar(ldeaths, method="burg")

spec.ar(log(lynx))
spec.ar(log(lynx), method="burg", add=TRUE, col="purple")
spec.ar(log(lynx), method="mle", add=TRUE, col="forest green")
spec.ar(log(lynx), method="ols", add=TRUE, col="blue")
\end{ExampleCode}
\end{Examples}
\HeaderA{spec.pgram}{Estimate Spectral Density of a Time Series by a Smoothed Periodogram}{spec.pgram}
\keyword{ts}{spec.pgram}
%
\begin{Description}\relax
\code{spec.pgram} calculates the periodogram using a fast Fourier
transform, and optionally smooths the result with a series of
modified Daniell smoothers (moving averages giving half weight to
the end values).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
spec.pgram(x, spans = NULL, kernel, taper = 0.1,
           pad = 0, fast = TRUE, demean = FALSE, detrend = TRUE,
           plot = TRUE, na.action = na.fail, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] univariate or multivariate time series.
\item[\code{spans}] vector of odd integers giving the widths of modified
Daniell smoothers to be used to smooth the periodogram.
\item[\code{kernel}] alternatively, a kernel smoother of class
\code{"tskernel"}.
\item[\code{taper}] specifies the proportion of data to taper.  A split
cosine bell taper is applied to this proportion of the data at the
beginning and end of the series.
\item[\code{pad}] proportion of data to pad. Zeros are added to the end of
the series to increase its length by the proportion \code{pad}.
\item[\code{fast}] logical; if \code{TRUE}, pad the series to a highly composite
length.
\item[\code{demean}] logical. If \code{TRUE}, subtract the mean of the
series.
\item[\code{detrend}] logical. If \code{TRUE}, remove a linear trend from
the series. This will also remove the mean.
\item[\code{plot}] plot the periodogram?
\item[\code{na.action}] \code{NA} action function.
\item[\code{...}] graphical arguments passed to \code{plot.spec}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The raw periodogram is not a consistent estimator of the spectral density,
but adjacent values are asymptotically independent. Hence a consistent
estimator can be derived by smoothing the raw periodogram, assuming that
the spectral density is smooth.

The series will be automatically padded with zeros until the series
length is a highly composite number in order to help the Fast Fourier
Transform. This is controlled by the \code{fast} and not the \code{pad}
argument.

The periodogram at zero is in theory zero as the mean of the series
is removed (but this may be affected by tapering): it is replaced by
an interpolation of adjacent values during smoothing, and no value
is returned for that frequency.
\end{Details}
%
\begin{Value}
A list object of class \code{"spec"} (see \code{\LinkA{spectrum}{spectrum}})
with the following additional components:
\begin{ldescription}
\item[\code{kernel}] The \code{kernel} argument, or the kernel constructed
from \code{spans}.
\item[\code{df}] The distribution of the spectral density estimate can be
approximated by a (scaled) chi square distribution with \code{df} degrees
of freedom.
\item[\code{bandwidth}] The equivalent bandwidth of the kernel smoother as
defined by Bloomfield (1976, page 201).
\item[\code{taper}] The value of the \code{taper} argument.
\item[\code{pad}] The value of the \code{pad} argument.
\item[\code{detrend}] The value of the \code{detrend} argument.
\item[\code{demean}] The value of the \code{demean} argument.

\end{ldescription}
The result is returned invisibly if \code{plot} is true.
\end{Value}
%
\begin{Author}\relax
Originally Martyn Plummer; kernel smoothing by Adrian Trapletti,
synthesis by B.D. Ripley
\end{Author}
%
\begin{References}\relax
Bloomfield, P. (1976) \emph{Fourier Analysis of Time Series: An
Introduction.} Wiley.

Brockwell, P.J. and Davis, R.A. (1991) \emph{Time Series: Theory and
Methods.} Second edition. Springer.

Venables, W.N. and Ripley, B.D. (2002) \emph{Modern Applied
Statistics with S.} Fourth edition. Springer. (Especially
pp. 392--7.)
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{spectrum}{spectrum}}, \code{\LinkA{spec.taper}{spec.taper}},
\code{\LinkA{plot.spec}{plot.spec}}, \code{\LinkA{fft}{fft}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Examples from Venables & Ripley
spectrum(ldeaths)
spectrum(ldeaths, spans = c(3,5))
spectrum(ldeaths, spans = c(5,7))
spectrum(mdeaths, spans = c(3,3))
spectrum(fdeaths, spans = c(3,3))

## bivariate example
mfdeaths.spc <- spec.pgram(ts.union(mdeaths, fdeaths), spans = c(3,3))
# plots marginal spectra: now plot coherency and phase
plot(mfdeaths.spc, plot.type = "coherency")
plot(mfdeaths.spc, plot.type = "phase")

## now impose a lack of alignment
mfdeaths.spc <- spec.pgram(ts.intersect(mdeaths, lag(fdeaths, 4)),
   spans = c(3,3), plot = FALSE)
plot(mfdeaths.spc, plot.type = "coherency")
plot(mfdeaths.spc, plot.type = "phase")

stocks.spc <- spectrum(EuStockMarkets, kernel("daniell", c(30,50)),
                       plot = FALSE)
plot(stocks.spc, plot.type = "marginal") # the default type
plot(stocks.spc, plot.type = "coherency")
plot(stocks.spc, plot.type = "phase")

sales.spc <- spectrum(ts.union(BJsales, BJsales.lead),
                      kernel("modified.daniell", c(5,7)))
plot(sales.spc, plot.type = "coherency")
plot(sales.spc, plot.type = "phase")
\end{ExampleCode}
\end{Examples}
\HeaderA{spec.taper}{Taper a Time Series by a Cosine Bell}{spec.taper}
\keyword{ts}{spec.taper}
%
\begin{Description}\relax
Apply a cosine-bell taper to a time series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
spec.taper(x, p = 0.1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A univariate or multivariate time series
\item[\code{p}] The proportion to be tapered at each end of the series,
either a scalar (giving the proportion for all series)
or a vector of the length of the number of series (giving the
proportion for each series..
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The cosine-bell taper is applied to the first and last \code{p[i]}
observations of time series \code{x[, i]}.
\end{Details}
%
\begin{Value}
A new time series object.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{spec.pgram}{spec.pgram}}, \code{\LinkA{cpgram}{cpgram}}
\end{SeeAlso}
\HeaderA{spectrum}{Spectral Density Estimation}{spectrum}
\aliasA{spec}{spectrum}{spec}
\keyword{ts}{spectrum}
%
\begin{Description}\relax
The \code{spectrum} function estimates the spectral density of a
time series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
spectrum(x, ..., method = c("pgram", "ar"))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A univariate or multivariate time series.
\item[\code{method}] String specifying the method used to estimate the
spectral density.  Allowed methods are \code{"pgram"} (the default)
and \code{"ar"}.
\item[\code{...}] Further arguments to specific spec methods or
\code{plot.spec}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{spectrum} is a wrapper function which calls the methods
\code{\LinkA{spec.pgram}{spec.pgram}} and \code{\LinkA{spec.ar}{spec.ar}}.

The spectrum here is defined with scaling \code{1/\LinkA{frequency}{frequency}(x)},
following S-PLUS.  This makes the spectral density a density over the
range \code{(-frequency(x)/2, +frequency(x)/2]}, whereas a more common
scaling is \eqn{2\pi}{} and range \eqn{(-0.5, 0.5]}{} (e.g., Bloomfield)
or 1 and range \eqn{(-\pi, \pi]}{}.

If available, a confidence interval will be plotted by
\code{plot.spec}: this is asymmetric, and the width of the centre
mark indicates the equivalent bandwidth.
\end{Details}
%
\begin{Value}
An object of class \code{"spec"}, which is a list containing at
least the following components:
\begin{ldescription}
\item[\code{freq}] vector of frequencies at which the spectral
density is estimated. (Possibly approximate Fourier frequencies.)
The units are the reciprocal of cycles per unit time (and not per
observation spacing): see `Details' below.

\item[\code{spec}] Vector (for univariate series) or matrix (for multivariate
series) of estimates of the spectral density at frequencies
corresponding to \code{freq}.

\item[\code{coh}] \code{NULL} for univariate series. For multivariate time
series, a matrix containing the \emph{squared} coherency
between different
series. Column \eqn{ i + (j - 1) * (j - 2)/2}{} of \code{coh}
contains the squared coherency between columns \eqn{i}{} and \eqn{j}{}
of \code{x}, where \eqn{i < j}{}.

\item[\code{phase}] \code{NULL} for univariate series. For multivariate
time series a matrix containing the cross-spectrum phase between
different series. The format is the same as \code{coh}.

\item[\code{series}] The name of the time series.

\item[\code{snames}] For multivariate input, the names of the component series.

\item[\code{method}] The method used to calculate the spectrum.

\end{ldescription}
The result is returned invisibly if \code{plot} is true.
\end{Value}
%
\begin{Note}\relax
The default plot for objects of class \code{"spec"} is quite complex,
including an error bar and default title, subtitle and axis
labels.  The defaults can all be overridden by supplying the
appropriate graphical parameters.
\end{Note}
%
\begin{Author}\relax
Martyn Plummer, B.D. Ripley
\end{Author}
%
\begin{References}\relax
Bloomfield, P. (1976) \emph{Fourier Analysis of Time Series: An
Introduction.} Wiley.

Brockwell, P. J. and Davis, R. A. (1991) \emph{Time Series: Theory and
Methods.} Second edition. Springer.

Venables, W. N. and Ripley, B. D. (2002) \emph{Modern Applied
Statistics with S-PLUS.} Fourth edition. Springer. (Especially
pages 392--7.)
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{spec.ar}{spec.ar}},
\code{\LinkA{spec.pgram}{spec.pgram}};
\code{\LinkA{plot.spec}{plot.spec}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

## Examples from Venables & Ripley
## spec.pgram
par(mfrow=c(2,2))
spectrum(lh)
spectrum(lh, spans=3)
spectrum(lh, spans=c(3,3))
spectrum(lh, spans=c(3,5))

spectrum(ldeaths)
spectrum(ldeaths, spans=c(3,3))
spectrum(ldeaths, spans=c(3,5))
spectrum(ldeaths, spans=c(5,7))
spectrum(ldeaths, spans=c(5,7), log="dB", ci=0.8)

# for multivariate examples see the help for spec.pgram

## spec.ar
spectrum(lh, method="ar")
spectrum(ldeaths, method="ar")
\end{ExampleCode}
\end{Examples}
\HeaderA{splinefun}{Interpolating Splines}{splinefun}
\aliasA{spline}{splinefun}{spline}
\aliasA{splinefunH}{splinefun}{splinefunH}
\keyword{math}{splinefun}
\keyword{dplot}{splinefun}
%
\begin{Description}\relax
Perform cubic (or Hermite) spline interpolation of given data points,
returning either a list of points obtained by the interpolation or a
\emph{function} performing the interpolation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
splinefun(x, y = NULL,
          method = c("fmm", "periodic", "natural", "monoH.FC", "hyman"),
          ties = mean)

spline(x, y = NULL, n = 3*length(x), method = "fmm",
       xmin = min(x), xmax = max(x), xout, ties = mean)

splinefunH(x, y, m)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] vectors giving the coordinates of the points to be
interpolated.  Alternatively a single plotting structure can be
specified: see \code{\LinkA{xy.coords}{xy.coords}}.

\code{y} must be increasing or decreasing for \code{method = "hyman"}.

\item[\code{m}] (for \code{splinefunH()}): vector of \emph{slopes}
\eqn{m_i}{} at the points \eqn{(x_i,y_i)}{}; these
together determine the \bold{H}ermite ``spline'' which is
piecewise cubic, (only) \emph{once} differentiable continuously.
\item[\code{method}] specifies the type of spline to be used.  Possible
values are \code{"fmm"}, \code{"natural"}, \code{"periodic"},
\code{"monoH.FC"} and \code{"hyman"}.
\item[\code{n}] if \code{xout} is left unspecified, interpolation takes place
at \code{n} equally spaced points spanning the interval
[\code{xmin}, \code{xmax}].
\item[\code{xmin, xmax}] left-hand and right-hand endpoint of the
interpolation interval (when \code{xout} is unspecified).
\item[\code{xout}] an optional set of values specifying where interpolation
is to take place.
\item[\code{ties}] Handling of tied \code{x} values.  Either a function
with a single vector argument returning a single number result or
the string \code{"ordered"}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The inputs can contain missing values which are deleted, so at least
one complete \code{(x, y)} pair is required.
If \code{method = "fmm"}, the spline used is that of Forsythe, Malcolm
and Moler (an exact cubic is fitted through the four points at each
end of the data, and this is used to determine the end conditions).
Natural splines are used when \code{method = "natural"}, and periodic
splines when \code{method = "periodic"}.

The method \code{"monoH.FC"} computes a \emph{monotone} Hermite spline
according to the method of Fritsch and Carlson.  It does so by
determining slopes such that the Hermite spline, determined by
\eqn{(x_i,y_i,m_i)}{}, is monotone (increasing or
decreasing) \bold{iff} the data are.

Method \code{"hyman"} computes a \emph{monotone} cubic spline using
Hyman filtering of an \code{method = "fmm"} fit for strictly monotonic
inputs.  (Added in \R{} 2.16.0.)

These interpolation splines can also be used for extrapolation, that is
prediction at points outside the range of \code{x}.  Extrapolation
makes little sense for \code{method = "fmm"}; for natural splines it
is linear using the slope of the interpolating curve at the nearest
data point.
\end{Details}
%
\begin{Value}
\code{spline} returns a list containing components \code{x} and
\code{y} which give the ordinates where interpolation took place and
the interpolated values.

\code{splinefun} returns a function with formal arguments \code{x} and
\code{deriv}, the latter defaulting to zero.  This function
can be used to evaluate the interpolating cubic spline
(\code{deriv} = 0), or its derivatives (\code{deriv} = 1, 2, 3) at the
points \code{x}, where the spline function interpolates the data
points originally specified.  This is often more useful than
\code{spline}.
\end{Value}
%
\begin{Section}{Warning}
The value returned by \code{splinefun} contains references to the code
in the current version of \R{}: it is not intended to be saved and
loaded into a different \R{} session.
\end{Section}
%
\begin{Author}\relax
R Core Team.

Simon Wood for the original code for Hyman filtering.
\end{Author}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.

Dougherty, R. L., Edelman, A. and Hyman, J. M. (1989)
Positivity-, monotonicity-, or convexity-preserving cubic and quintic
Hermite interpolation.
\emph{Mathematics of Computation} \bold{52}, 471--494.

Forsythe, G. E., Malcolm, M. A. and Moler, C. B. (1977)
\emph{Computer Methods for Mathematical Computations}.  Wiley.

Fritsch, F. N. and Carlson, R. E. (1980)
Monotone piecewise cubic interpolation, \emph{SIAM Journal on
Numerical Analysis} \bold{17}, 238--246.

Hyman, J. M. (1983)
Accurate monotonicity preserving cubic interpolation.
\emph{SIAM J. Sci. Stat. Comput.} \bold{4}, 645--654.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{approx}{approx}} and \code{\LinkA{approxfun}{approxfun}} for constant and
linear interpolation.

Package \pkg{splines}, especially \code{\LinkA{interpSpline}{interpSpline}}
and \code{\LinkA{periodicSpline}{periodicSpline}} for interpolation splines.
That package also generates spline bases that can be used for
regression splines.

\code{\LinkA{smooth.spline}{smooth.spline}} for smoothing splines.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

op <- par(mfrow = c(2,1), mgp = c(2,.8,0), mar = .1+c(3,3,3,1))
n <- 9
x <- 1:n
y <- rnorm(n)
plot(x, y, main = paste("spline[fun](.) through", n, "points"))
lines(spline(x, y))
lines(spline(x, y, n = 201), col = 2)

y <- (x-6)^2
plot(x, y, main = "spline(.) -- 3 methods")
lines(spline(x, y, n = 201), col = 2)
lines(spline(x, y, n = 201, method = "natural"), col = 3)
lines(spline(x, y, n = 201, method = "periodic"), col = 4)
legend(6,25, c("fmm","natural","periodic"), col=2:4, lty=1)

y <- sin((x-0.5)*pi)
f <- splinefun(x, y)
ls(envir = environment(f))
splinecoef <- get("z", envir = environment(f))
curve(f(x), 1, 10, col = "green", lwd = 1.5)
points(splinecoef, col = "purple", cex = 2)
curve(f(x, deriv=1), 1, 10, col = 2, lwd = 1.5)
curve(f(x, deriv=2), 1, 10, col = 2, lwd = 1.5, n = 401)
curve(f(x, deriv=3), 1, 10, col = 2, lwd = 1.5, n = 401)
par(op)

## Manual spline evaluation --- demo the coefficients :
.x <- splinecoef$x
u <- seq(3,6, by = 0.25)
(ii <- findInterval(u, .x))
dx <- u - .x[ii]
f.u <- with(splinecoef,
            y[ii] + dx*(b[ii] + dx*(c[ii] + dx* d[ii])))
stopifnot(all.equal(f(u), f.u))

## An example with ties (non-unique  x values):
set.seed(1); x <- round(rnorm(30), 1); y <- sin(pi * x) + rnorm(30)/10
plot(x,y, main="spline(x,y)  when x has ties")
lines(spline(x,y, n= 201), col = 2)
## visualizes the non-unique ones:
tx <- table(x); mx <- as.numeric(names(tx[tx > 1]))
ry <- matrix(unlist(tapply(y, match(x,mx), range, simplify=FALSE)),
             ncol=2, byrow=TRUE)
segments(mx, ry[,1], mx, ry[,2], col = "blue", lwd = 2)

## An example of  monotone  interpolation
n <- 20
set.seed(11)
x. <- sort(runif(n)) ; y. <- cumsum(abs(rnorm(n)))
plot(x.,y.)
curve(splinefun(x.,y.)(x), add = TRUE, col = 2, n = 1001)
curve(splinefun(x.,y., method = "monoH.FC")(x), add = TRUE, col = 3, n = 1001)
curve(splinefun(x.,y., method = "hyman")(x), add = TRUE, col = 4, n = 1001)
legend("topleft",
       paste0("splinefun( \"", c("fmm", "monoH.FC", "hyman"), "\" )"),
       col = 2:4, lty = 1)

## and one from Fritsch and Carlson (1980), Dougherty et al (1989)
x. <- c(7.09, 8.09, 8.19, 8.7, 9.2, 10, 12, 15, 20)
f <- c(0, 2.76429e-5, 4.37498e-2, 0.169183, 0.469428, 0.943740,
       0.998636, 0.999919, 0.999994)
plot(x., f, ylim = c(-0.2, 1.2))
curve(splinefun(x., f)(x), add = TRUE, col = 2, n = 1001)
curve(splinefun(x., f, method = "hyman")(x), add = TRUE, col = 4, n = 1001)
\end{ExampleCode}
\end{Examples}
\HeaderA{SSasymp}{Self-Starting Nls Asymptotic Regression Model}{SSasymp}
\keyword{models}{SSasymp}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the asymptotic regression
function and its gradient.  It has an \code{initial} attribute that
will evaluate initial estimates of the parameters \code{Asym}, \code{R0},
and \code{lrc} for a given set of data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSasymp(input, Asym, R0, lrc)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input}] a numeric vector of values at which to evaluate the model.
\item[\code{Asym}] a numeric parameter representing the horizontal asymptote on
the right side (very large values of \code{input}).
\item[\code{R0}] a numeric parameter representing the response when
\code{input} is zero.
\item[\code{lrc}] a numeric parameter representing the natural logarithm of
the rate constant.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression \code{Asym+(R0-Asym)*exp(-exp(lrc)*input)}.  If all of
the arguments \code{Asym}, \code{R0}, and \code{lrc} are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

Lob.329 <- Loblolly[ Loblolly$Seed == "329", ]
SSasymp( Lob.329$age, 100, -8.5, -3.2 )  # response only
Asym <- 100
resp0 <- -8.5
lrc <- -3.2
SSasymp( Lob.329$age, Asym, resp0, lrc ) # response and gradient
getInitial(height ~ SSasymp( age, Asym, resp0, lrc), data = Lob.329)
## Initial values are in fact the converged values
fm1 <- nls(height ~ SSasymp( age, Asym, resp0, lrc), data = Lob.329)
summary(fm1)

\end{ExampleCode}
\end{Examples}
\HeaderA{SSasympOff}{Self-Starting Nls Asymptotic Regression Model with an Offset}{SSasympOff}
\keyword{models}{SSasympOff}
%
\begin{Description}\relax
This \code{selfStart} model evaluates an alternative parametrization
of the asymptotic
regression function and the gradient with respect to those parameters.
It has an \code{initial}
attribute that creates initial estimates of the parameters
\code{Asym}, \code{lrc}, and \code{c0}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSasympOff(input, Asym, lrc, c0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input}] a numeric vector of values at which to evaluate the model.
\item[\code{Asym}] a numeric parameter representing the horizontal asymptote on
the right side (very large values of \code{input}).
\item[\code{lrc}] a numeric parameter representing the natural logarithm of
the rate constant.
\item[\code{c0}] a numeric parameter representing the \code{input} for which the
response is zero.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression \code{Asym*(1 - exp(-exp(lrc)*(input - c0)))}.  If all of
the arguments \code{Asym}, \code{lrc}, and \code{c0} are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}};
\code{example(SSasympOff)} gives graph showing the \code{SSasympOff}
parametrization, where \eqn{\phi_1}{} is \code{Asymp},
\eqn{\phi_3}{} is \code{c0}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
CO2.Qn1 <- CO2[CO2$Plant == "Qn1", ]
SSasympOff(CO2.Qn1$conc, 32, -4, 43)  # response only
Asym <- 32; lrc <- -4; c0 <- 43
SSasympOff(CO2.Qn1$conc, Asym, lrc, c0) # response and gradient
getInitial(uptake ~ SSasympOff(conc, Asym, lrc, c0), data = CO2.Qn1)
## Initial values are in fact the converged values
fm1 <- nls(uptake ~ SSasympOff(conc, Asym, lrc, c0), data = CO2.Qn1)
summary(fm1)

\end{ExampleCode}
\end{Examples}
\HeaderA{SSasympOrig}{Self-Starting Nls Asymptotic Regression Model through the Origin}{SSasympOrig}
\keyword{models}{SSasympOrig}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the asymptotic regression
function through the origin and its gradient.  It has an
\code{initial} attribute that will evaluate initial estimates of the
parameters \code{Asym} and \code{lrc} for a given set of data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSasympOrig(input, Asym, lrc)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input}] a numeric vector of values at which to evaluate the model.
\item[\code{Asym}] a numeric parameter representing the horizontal asymptote.
\item[\code{lrc}] a numeric parameter representing the natural logarithm of
the rate constant.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression \code{Asym*(1 - exp(-exp(lrc)*input))}.  If all of
the arguments \code{Asym} and \code{lrc} are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

Lob.329 <- Loblolly[ Loblolly$Seed == "329", ]
SSasympOrig(Lob.329$age, 100, -3.2)  # response only
Asym <- 100; lrc <- -3.2
SSasympOrig(Lob.329$age, Asym, lrc) # response and gradient
getInitial(height ~ SSasympOrig(age, Asym, lrc), data = Lob.329)
## Initial values are in fact the converged values
fm1 <- nls(height ~ SSasympOrig(age, Asym, lrc), data = Lob.329)
summary(fm1)


\end{ExampleCode}
\end{Examples}
\HeaderA{SSbiexp}{Self-Starting Nls Biexponential model}{SSbiexp}
\keyword{models}{SSbiexp}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the biexponential model function
and its gradient.  It has an \code{initial} attribute that 
creates initial estimates of the parameters \code{A1}, \code{lrc1},
\code{A2}, and \code{lrc2}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSbiexp(input, A1, lrc1, A2, lrc2)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input}] a numeric vector of values at which to evaluate the model.
\item[\code{A1}] a numeric parameter representing the multiplier of the first
exponential.
\item[\code{lrc1}] a numeric parameter representing the natural logarithm of
the rate constant of the first exponential.
\item[\code{A2}] a numeric parameter representing the multiplier of the second
exponential.
\item[\code{lrc2}] a numeric parameter representing the natural logarithm of
the rate constant of the second exponential.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression
\code{A1*exp(-exp(lrc1)*input)+A2*exp(-exp(lrc2)*input)}.
If all of the arguments \code{A1}, \code{lrc1}, \code{A2}, and
\code{lrc2} are names of objects, the gradient matrix with respect to
these names is attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
Indo.1 <- Indometh[Indometh$Subject == 1, ]
SSbiexp( Indo.1$time, 3, 1, 0.6, -1.3 )  # response only
A1 <- 3; lrc1 <- 1; A2 <- 0.6; lrc2 <- -1.3
SSbiexp( Indo.1$time, A1, lrc1, A2, lrc2 ) # response and gradient
print(getInitial(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = Indo.1),
      digits = 5)
## Initial values are in fact the converged values
fm1 <- nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = Indo.1)
summary(fm1)

\end{ExampleCode}
\end{Examples}
\HeaderA{SSD}{SSD Matrix and Estimated Variance Matrix in Multivariate Models}{SSD}
\aliasA{estVar}{SSD}{estVar}
\keyword{models}{SSD}
\keyword{multivariate}{SSD}
%
\begin{Description}\relax
Functions to compute matrix of residual sums of squares and products,
or the estimated variance matrix for multivariate linear models.
\end{Description}
%
\begin{Usage}
\begin{verbatim}

# S3 method for class 'mlm'
SSD(object, ...)

# S3 methods for class 'SSD' and 'mlm'
estVar(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] \code{object} of class \code{"mlm"}, or \code{"SSD"} in
the case of \code{estVar}.
\item[\code{...}] Unused
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{SSD()} returns a list of class \code{"SSD"} containing the
following components
\begin{ldescription}
\item[\code{SSD}] The residual sums of squares and products matrix
\item[\code{df}] Degrees of freedom
\item[\code{call}] Copied from \code{object}

\end{ldescription}
\code{estVar} returns a matrix with the estimated variances and
covariances.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{mauchly.test}{mauchly.test}}, \code{\LinkA{anova.mlm}{anova.mlm}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
# Lifted from Baron+Li:
# "Notes on the use of R for psychology experiments and questionnaires"
# Maxwell and Delaney, p. 497
reacttime <- matrix(c(
420, 420, 480, 480, 600, 780,
420, 480, 480, 360, 480, 600,
480, 480, 540, 660, 780, 780,
420, 540, 540, 480, 780, 900,
540, 660, 540, 480, 660, 720,
360, 420, 360, 360, 480, 540,
480, 480, 600, 540, 720, 840,
480, 600, 660, 540, 720, 900,
540, 600, 540, 480, 720, 780,
480, 420, 540, 540, 660, 780),
ncol = 6, byrow = TRUE,
dimnames=list(subj=1:10,
              cond=c("deg0NA", "deg4NA", "deg8NA",
                     "deg0NP", "deg4NP", "deg8NP")))

mlmfit <- lm(reacttime~1)
SSD(mlmfit)
estVar(mlmfit)
\end{ExampleCode}
\end{Examples}
\HeaderA{SSfol}{Self-Starting Nls First-order Compartment Model}{SSfol}
\keyword{models}{SSfol}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the first-order compartment
function and its gradient.  It has an \code{initial} attribute that 
creates initial estimates of the parameters \code{lKe}, \code{lKa},
and \code{lCl}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSfol(Dose, input, lKe, lKa, lCl)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{Dose}] a numeric value representing the initial dose.
\item[\code{input}] a numeric vector at which to evaluate the model.
\item[\code{lKe}] a numeric parameter representing the natural logarithm of
the elimination rate constant.
\item[\code{lKa}] a numeric parameter representing the natural logarithm of
the absorption rate constant.
\item[\code{lCl}] a numeric parameter representing the natural logarithm of
the clearance.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}, which is the
value of the expression
\code{Dose * exp(lKe+lKa-lCl) * (exp(-exp(lKe)*input)-exp(-exp(lKa)*input)) / (exp(lKa)-exp(lKe))}.

If all of the arguments \code{lKe}, \code{lKa}, and \code{lCl} are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
Theoph.1 <- Theoph[ Theoph$Subject == 1, ]
SSfol(Theoph.1$Dose, Theoph.1$Time, -2.5, 0.5, -3) # response only
lKe <- -2.5; lKa <- 0.5; lCl <- -3
SSfol(Theoph.1$Dose, Theoph.1$Time, lKe, lKa, lCl) # response and gradient
getInitial(conc ~ SSfol(Dose, Time, lKe, lKa, lCl), data = Theoph.1)
## Initial values are in fact the converged values
fm1 <- nls(conc ~ SSfol(Dose, Time, lKe, lKa, lCl), data = Theoph.1)
summary(fm1)
\end{ExampleCode}
\end{Examples}
\HeaderA{SSfpl}{Self-Starting Nls Four-Parameter Logistic Model}{SSfpl}
\keyword{models}{SSfpl}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the four-parameter logistic
function and its gradient.  It has an \code{initial} attribute that
will evaluate initial estimates of the parameters \code{A}, \code{B},
\code{xmid}, and \code{scal} for a given set of data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSfpl(input, A, B, xmid, scal)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input}] a numeric vector of values at which to evaluate the model.
\item[\code{A}] a numeric parameter representing the horizontal asymptote on
the left side (very small values of \code{input}).
\item[\code{B}] a numeric parameter representing the horizontal asymptote on
the right side (very large values of \code{input}).
\item[\code{xmid}] a numeric parameter representing the \code{input} value at the
inflection point of the curve.  The value of \code{SSfpl} will be
midway between \code{A} and \code{B} at \code{xmid}.
\item[\code{scal}] a numeric scale parameter on the \code{input} axis.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression \code{A+(B-A)/(1+exp((xmid-input)/scal))}.  If all of
the arguments \code{A}, \code{B}, \code{xmid}, and \code{scal} are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
Chick.1 <- ChickWeight[ChickWeight$Chick == 1, ]
SSfpl(Chick.1$Time, 13, 368, 14, 6)  # response only
A <- 13; B <- 368; xmid <- 14; scal <- 6
SSfpl(Chick.1$Time, A, B, xmid, scal) # response and gradient
print(getInitial(weight ~ SSfpl(Time, A, B, xmid, scal), data = Chick.1),
      digits = 5)
## Initial values are in fact the converged values
fm1 <- nls(weight ~ SSfpl(Time, A, B, xmid, scal), data = Chick.1)
summary(fm1)

\end{ExampleCode}
\end{Examples}
\HeaderA{SSgompertz}{Self-Starting Nls Gompertz Growth Model}{SSgompertz}
\keyword{models}{SSgompertz}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the Gompertz growth model
and its gradient.  It has an \code{initial} attribute that
creates initial estimates of the parameters \code{Asym},
\code{b2}, and \code{b3}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSgompertz(x, Asym, b2, b3)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector of values at which to evaluate the model.
\item[\code{Asym}] a numeric parameter representing the asymptote.
\item[\code{b2}] a numeric parameter related to the value of the function at
\code{x = 0}
\item[\code{b3}] a numeric parameter related to the scale the \code{x} axis.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression \code{Asym*exp(-b2*b3\textasciicircum{}x)}.  If all of
the arguments \code{Asym}, \code{b2}, and \code{b3} are
names of objects the gradient matrix with respect to these names is attached as
an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
DNase.1 <- subset(DNase, Run == 1)
SSgompertz(log(DNase.1$conc), 4.5, 2.3, 0.7)   # response only
Asym <- 4.5; b2 <- 2.3; b3 <- 0.7
SSgompertz(log(DNase.1$conc), Asym, b2, b3) # response and gradient
print(getInitial(density ~ SSgompertz(log(conc), Asym, b2, b3),
                 data = DNase.1), digits = 5)
## Initial values are in fact the converged values
fm1 <- nls(density ~ SSgompertz(log(conc), Asym, b2, b3),
           data = DNase.1)
summary(fm1)
\end{ExampleCode}
\end{Examples}
\HeaderA{SSlogis}{Self-Starting Nls Logistic Model}{SSlogis}
\keyword{models}{SSlogis}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the logistic
function and its gradient.  It has an \code{initial} attribute that
creates initial estimates of the parameters \code{Asym},
\code{xmid}, and \code{scal}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSlogis(input, Asym, xmid, scal)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input}] a numeric vector of values at which to evaluate the model.
\item[\code{Asym}] a numeric parameter representing the asymptote.
\item[\code{xmid}] a numeric parameter representing the \code{x} value at the
inflection point of the curve.  The value of \code{SSlogis} will be
\code{Asym/2} at \code{xmid}.
\item[\code{scal}] a numeric scale parameter on the \code{input} axis.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression \code{Asym/(1+exp((xmid-input)/scal))}.  If all of
the arguments \code{Asym}, \code{xmid}, and \code{scal} are
names of objects the gradient matrix with respect to these names is attached as
an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

Chick.1 <- ChickWeight[ChickWeight$Chick == 1, ]
SSlogis(Chick.1$Time, 368, 14, 6)  # response only
Asym <- 368; xmid <- 14; scal <- 6
SSlogis(Chick.1$Time, Asym, xmid, scal) # response and gradient
getInitial(weight ~ SSlogis(Time, Asym, xmid, scal), data = Chick.1)
## Initial values are in fact the converged values
fm1 <- nls(weight ~ SSlogis(Time, Asym, xmid, scal), data = Chick.1)
summary(fm1)

\end{ExampleCode}
\end{Examples}
\HeaderA{SSmicmen}{Self-Starting Nls Michaelis-Menten Model}{SSmicmen}
\keyword{models}{SSmicmen}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the Michaelis-Menten model and
its gradient.  It has an \code{initial} attribute that
will evaluate initial estimates of the parameters \code{Vm} and \code{K}
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSmicmen(input, Vm, K)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input}] a numeric vector of values at which to evaluate the model.
\item[\code{Vm}] a numeric parameter representing the maximum value of the response.
\item[\code{K}] a numeric parameter representing the \code{input} value at
which half the maximum response is attained.  In the field of enzyme
kinetics this is called the Michaelis parameter.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
a numeric vector of the same length as \code{input}.  It is the value of
the expression \code{Vm*input/(K+input)}.  If both
the arguments \code{Vm} and \code{K} are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
JosÃ© Pinheiro and Douglas Bates
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
PurTrt <- Puromycin[ Puromycin$state == "treated", ]
SSmicmen(PurTrt$conc, 200, 0.05)  # response only
Vm <- 200; K <- 0.05
SSmicmen(PurTrt$conc, Vm, K)      # response and gradient
print(getInitial(rate ~ SSmicmen(conc, Vm, K), data = PurTrt), digits=3)
## Initial values are in fact the converged values
fm1 <- nls(rate ~ SSmicmen(conc, Vm, K), data = PurTrt)
summary(fm1)
## Alternative call using the subset argument
fm2 <- nls(rate ~ SSmicmen(conc, Vm, K), data = Puromycin,
           subset = state == "treated")
summary(fm2)

\end{ExampleCode}
\end{Examples}
\HeaderA{SSweibull}{Self-Starting Nls Weibull Growth Curve Model}{SSweibull}
\keyword{models}{SSweibull}
%
\begin{Description}\relax
This \code{selfStart} model evaluates the Weibull model for growth
curve data and its gradient.  It has an \code{initial} attribute that
will evaluate initial estimates of the parameters \code{Asym}, \code{Drop},
\code{lrc}, and \code{pwr} for a given set of data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
SSweibull(x, Asym, Drop, lrc, pwr)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a numeric vector of values at which to evaluate the model.
\item[\code{Asym}] a numeric parameter representing the horizontal asymptote on
the right side (very small values of \code{x}).
\item[\code{Drop}] a numeric parameter representing the change from
\code{Asym} to the \code{y} intercept.
\item[\code{lrc}] a numeric parameter representing the natural logarithm of
the rate constant.
\item[\code{pwr}] a numeric parameter representing the power to which \code{x}
is raised.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This model is a generalization of the \code{\LinkA{SSasymp}{SSasymp}} model in
that it reduces to \code{SSasymp} when \code{pwr} is unity.

\end{Details}
%
\begin{Value}
a numeric vector of the same length as \code{x}.  It is the value of
the expression \code{Asym-Drop*exp(-exp(lrc)*x\textasciicircum{}pwr)}.  If all of
the arguments \code{Asym}, \code{Drop}, \code{lrc}, and \code{pwr} are
names of objects, the gradient matrix with respect to these names is
attached as an attribute named \code{gradient}.
\end{Value}
%
\begin{Author}\relax
Douglas Bates
\end{Author}
%
\begin{References}\relax
Ratkowsky, David A. (1983), \emph{Nonlinear Regression Modeling},
Dekker. (section 4.4.5)
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{nls}{nls}}, \code{\LinkA{selfStart}{selfStart}}, \code{\LinkA{SSasymp}{SSasymp}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
Chick.6 <- subset(ChickWeight, (Chick == 6) & (Time > 0))
SSweibull(Chick.6$Time, 160, 115, -5.5, 2.5)   # response only
Asym <- 160; Drop <- 115; lrc <- -5.5; pwr <- 2.5
SSweibull(Chick.6$Time, Asym, Drop, lrc, pwr)  # response and gradient
getInitial(weight ~ SSweibull(Time, Asym, Drop, lrc, pwr), data = Chick.6)
## Initial values are in fact the converged values
fm1 <- nls(weight ~ SSweibull(Time, Asym, Drop, lrc, pwr), data = Chick.6)
summary(fm1)
\end{ExampleCode}
\end{Examples}
\HeaderA{start}{Encode the Terminal Times of Time Series}{start}
\aliasA{end}{start}{end}
\keyword{ts}{start}
%
\begin{Description}\relax
Extract and encode the times the first and last observations were
taken. Provided only for compatibility with S version 2.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
start(x, ...)
end(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a univariate or multivariate time-series, or a vector or matrix.
\item[\code{...}] extra arguments for future methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These are generic functions, which will use the
\code{\LinkA{tsp}{tsp}} attribute of \code{x} if it exists.
Their default methods decode the start time from the original time
units, so that for a monthly series \code{1995.5} is represented
as \code{c(1995, 7)}. For a series of frequency \code{f}, time
\code{n+i/f} is presented as \code{c(n, i+1)} (even for \code{i = 0}
and \code{f = 1}).
\end{Details}
%
\begin{Section}{Warning}
The representation used by \code{start} and \code{end} has no
meaning unless the frequency is supplied.
\end{Section}
%
\begin{SeeAlso}\relax
\code{\LinkA{ts}{ts}}, \code{\LinkA{time}{time}}, \code{\LinkA{tsp}{tsp}}.
\end{SeeAlso}
\HeaderA{stat.anova}{GLM Anova Statistics}{stat.anova}
\keyword{regression}{stat.anova}
\keyword{models}{stat.anova}
%
\begin{Description}\relax
This is a utility function, used in \code{lm} and
\code{glm} methods for \code{\LinkA{anova}{anova}(..., test != NULL)}
and should not be used by the average user.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
stat.anova(table, test = c("Rao","LRT", "Chisq", "F", "Cp"),
           scale, df.scale, n)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{table}] numeric matrix as results from
\code{\LinkA{anova.glm}{anova.glm}(..., test=NULL)}.
\item[\code{test}] a character string, matching one of \code{"Rao"}, \code{"LRT"},\code{"Chisq"},
\code{"F"} or \code{"Cp"}.
\item[\code{scale}] a residual mean square or other scale estimate to be used
as the denominator in an F test.
\item[\code{df.scale}] degrees of freedom corresponding to \code{scale}.
\item[\code{n}] number of observations.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A matrix which is the original \code{table}, augmented by a column
of test statistics, depending on the \code{test} argument.
\end{Value}
%
\begin{References}\relax
Hastie, T. J. and Pregibon, D. (1992)
\emph{Generalized linear models.}
Chapter 6 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{anova.lm}{anova.lm}}, \code{\LinkA{anova.glm}{anova.glm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
##-- Continued from '?glm':

print(ag <- anova(glm.D93))
stat.anova(ag$table, test = "Cp",
           scale = sum(resid(glm.D93, "pearson")^2)/4,
           df.scale = 4, n = 9)
\end{ExampleCode}
\end{Examples}
\HeaderA{stats-deprecated}{Deprecated Functions in Package \pkg{stats}}{stats.Rdash.deprecated}
\keyword{misc}{stats-deprecated}
%
\begin{Description}\relax
These functions are provided for compatibility with older versions of
\R{} only, and may be defunct as soon as the next release.
\end{Description}
%
\begin{Details}\relax

There are currently no deprecated functions in this package.


\end{Details}
%
\begin{SeeAlso}\relax
\code{\LinkA{Deprecated}{Deprecated}}
\end{SeeAlso}
\HeaderA{step}{Choose a model by AIC in a Stepwise Algorithm}{step}
\keyword{models}{step}
%
\begin{Description}\relax
Select a formula-based model by AIC.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
step(object, scope, scale = 0,
     direction = c("both", "backward", "forward"), 
     trace = 1, keep = NULL, steps = 1000, k = 2, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] 
an object representing a model of an appropriate class (mainly
\code{"lm"} and \code{"glm"}).
This is used as the initial model in the stepwise search.

\item[\code{scope}] 
defines the range of models examined in the stepwise search.
This should be either a single formula, or a list containing
components \code{upper} and \code{lower}, both formulae.  See the
details for how to specify the formulae and how they are used.

\item[\code{scale}] 
used in the definition of the AIC statistic for selecting the models,
currently only for \code{\LinkA{lm}{lm}}, \code{\LinkA{aov}{aov}} and
\code{\LinkA{glm}{glm}} models.  The default value, \code{0}, indicates
the scale should be estimated: see \code{\LinkA{extractAIC}{extractAIC}}.

\item[\code{direction}] 
the mode of stepwise search, can be one of \code{"both"},
\code{"backward"}, or \code{"forward"}, with a default of \code{"both"}. 
If the \code{scope} argument is missing the default for
\code{direction} is \code{"backward"}.

\item[\code{trace}] 
if positive, information is printed during the running of \code{step}.
Larger values may give more detailed information.

\item[\code{keep}] 
a filter function whose input is a fitted model object and the 
associated \code{AIC} statistic, and whose output is arbitrary. 
Typically \code{keep} will select a subset of the components of 
the object and return them. The default is not to keep anything.

\item[\code{steps}] 
the maximum number of steps to be considered.  The default is 1000
(essentially as many as required).  It is typically used to stop the
process early.

\item[\code{k}] 
the multiple of the number of degrees of freedom used for the penalty.
Only \code{k = 2} gives the genuine AIC: \code{k = log(n)} is sometimes
referred to as BIC or SBC.

\item[\code{...}] 
any additional arguments to \code{\LinkA{extractAIC}{extractAIC}}.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{step} uses \code{\LinkA{add1}{add1}} and \code{\LinkA{drop1}{drop1}}
repeatedly; it will work for any method for which they work, and that
is determined by having a valid method for \code{\LinkA{extractAIC}{extractAIC}}.
When the additive constant can be chosen so that AIC is equal to
Mallows' \eqn{C_p}{}, this is done and the tables are labelled
appropriately.

The set of models searched is determined by the \code{scope} argument.
The right-hand-side of its \code{lower} component is always included
in the model, and right-hand-side of the model is included in the
\code{upper} component.  If \code{scope} is a single formula, it
specifies the \code{upper} component, and the \code{lower} model is
empty.  If \code{scope} is missing, the initial model is used as the
\code{upper} model.

Models specified by \code{scope} can be templates to update
\code{object} as used by \code{\LinkA{update.formula}{update.formula}}.  So using
\code{.} in a \code{scope} formula means `what is
already there', with \code{.\textasciicircum{}2} indicating all interactions of
existing terms.

There is a potential problem in using \code{\LinkA{glm}{glm}} fits with a
variable \code{scale}, as in that case the deviance is not simply
related to the maximized log-likelihood.  The \code{"glm"} method for
function \code{\LinkA{extractAIC}{extractAIC}} makes the
appropriate adjustment for a \code{gaussian} family, but may need to be
amended for other cases.  (The \code{binomial} and \code{poisson}
families have fixed \code{scale} by default and do not correspond
to a particular maximum-likelihood problem for variable \code{scale}.)
\end{Details}
%
\begin{Value}
the stepwise-selected model is returned, with up to two additional
components.  There is an \code{"anova"} component corresponding to the
steps taken in the search, as well as a \code{"keep"} component if the
\code{keep=} argument was supplied in the call. The
\code{"Resid. Dev"} column of the analysis of deviance table refers
to a constant minus twice the maximized log likelihood: it will be a
deviance only in cases where a saturated model is well-defined
(thus excluding \code{lm}, \code{aov} and \code{survreg} fits,
for example).
\end{Value}
%
\begin{Section}{Warning}
The model fitting must apply the models to the same dataset. This
may be a problem if there are missing values and \R{}'s default of
\code{na.action = na.omit} is used.  We suggest you remove the
missing values first.

Calls to the function \code{\LinkA{nobs}{nobs}} are used to check that the
number of observations involved in the fitting process remains unchanged.
\end{Section}
%
\begin{Note}\relax
This function differs considerably from the function in S, which uses a
number of approximations and does not in general compute the correct AIC.

This is a minimal implementation.  Use \code{\LinkA{stepAIC}{stepAIC}}
in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} for a wider range of object classes.
\end{Note}
%
\begin{Author}\relax
B. D. Ripley: \code{step} is a slightly simplified version of
\code{\LinkA{stepAIC}{stepAIC}} in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}} (Venables \&
Ripley, 2002 and earlier editions).

The idea of a \code{step} function follows that described in Hastie \&
Pregibon (1992); but the implementation in \R{} is more general.
\end{Author}
%
\begin{References}\relax
Hastie, T. J. and Pregibon, D. (1992)
\emph{Generalized linear models.}
Chapter 6 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.

Venables, W. N. and Ripley, B. D. (2002)
\emph{Modern Applied Statistics with S.}
New York: Springer (4th ed).
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{stepAIC}{stepAIC}} in \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}, \code{\LinkA{add1}{add1}},
\code{\LinkA{drop1}{drop1}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

## following on from example(lm)

step(lm.D9)  

summary(lm1 <- lm(Fertility ~ ., data = swiss))
slm1 <- step(lm1)
summary(slm1)
slm1$anova
\end{ExampleCode}
\end{Examples}
\HeaderA{stepfun}{Step Function Class}{stepfun}
\aliasA{as.stepfun}{stepfun}{as.stepfun}
\aliasA{is.stepfun}{stepfun}{is.stepfun}
\aliasA{knots}{stepfun}{knots}
\aliasA{print.stepfun}{stepfun}{print.stepfun}
\aliasA{summary.stepfun}{stepfun}{summary.stepfun}
\keyword{dplot}{stepfun}
%
\begin{Description}\relax
Given the vectors \eqn{(x_1, \ldots, x_n)}{} and
\eqn{(y_0,y_1,\ldots, y_n)}{} (one value
more!), \code{stepfun(x,y,...)} returns an interpolating
`step' function, say \code{fn}. I.e., \eqn{fn(t) =
    c}{}\eqn{_i}{} (constant) for \eqn{t \in (x_i, x_{i+1})}{} and at the abscissa values, if (by default)
\code{right = FALSE}, \eqn{fn(x_i) = y_i}{} and for
\code{right = TRUE}, \eqn{fn(x_i) = y_{i-1}}{}, for
\eqn{i=1,\ldots,n}{}.

The value of the constant \eqn{c_i}{} above depends on the
`continuity' parameter \code{f}.
For the default, \code{right = FALSE, f = 0},
\code{fn} is a \emph{cadlag} function, i.e., continuous at right,
limit (`the point') at left.
In general, \eqn{c_i}{} is interpolated in between the
neighbouring \eqn{y}{} values,
\eqn{c_i= (1-f) y_i + f\cdot y_{i+1}}{}.
Therefore, for non-0 values of \code{f}, \code{fn} may no longer be a proper
step function, since it can be discontinuous from both sides, unless
\code{right = TRUE, f = 1} which is right-continuous.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
stepfun(x, y, f = as.numeric(right), ties = "ordered",
        right = FALSE)

is.stepfun(x)
knots(Fn, ...)
as.stepfun(x, ...)

## S3 method for class 'stepfun'
print(x, digits = getOption("digits") - 2, ...)

## S3 method for class 'stepfun'
summary(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric vector giving the knots or jump locations of the step
function for \code{stepfun()}.  For the other functions, \code{x} is
as \code{object} below.
\item[\code{y}] numeric vector one longer than \code{x}, giving the heights of
the function values \emph{between} the x values.
\item[\code{f}] a number between 0 and 1, indicating how interpolation outside
the given x values should happen.  See \code{\LinkA{approxfun}{approxfun}}.
\item[\code{ties}] Handling of tied \code{x} values. Either a function or
the string \code{"ordered"}.  See  \code{\LinkA{approxfun}{approxfun}}.
\item[\code{right}] logical, indicating if the intervals should be closed on
the right (and open on the left) or vice versa.

\item[\code{Fn, object}] an \R{} object inheriting from \code{"stepfun"}.
\item[\code{digits}] number of significant digits to use, see \code{\LinkA{print}{print}}.
\item[\code{...}] potentially further arguments (required by the generic).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A function of class \code{"stepfun"}, say \code{fn}.

There are methods available for summarizing (\code{"summary(.)"}),
representing (\code{"print(.)"}) and plotting  (\code{"plot(.)"}, see
\code{\LinkA{plot.stepfun}{plot.stepfun}}) \code{"stepfun"} objects.

The \code{\LinkA{environment}{environment}} of \code{fn} contains all the
information needed;
\begin{ldescription}
\item[\code{"x","y"}] the original arguments
\item[\code{"n"}] number of knots (x values)
\item[\code{"f"}] continuity parameter
\item[\code{"yleft", "yright"}] the function values \emph{outside} the knots
\item[\code{"method"}] (always \code{== "constant"}, from
\code{\LinkA{approxfun}{approxfun}(.)}).
\end{ldescription}
The knots are also available via \code{\LinkA{knots}{knots}(fn)}.
\end{Value}
%
\begin{Author}\relax
Martin Maechler, \email{maechler@stat.math.ethz.ch} with some basic
code from Thomas Lumley.
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{ecdf}{ecdf}} for empirical distribution functions as
special step functions and \code{\LinkA{plot.stepfun}{plot.stepfun}} for \emph{plotting}
step functions.

\code{\LinkA{approxfun}{approxfun}} and \code{\LinkA{splinefun}{splinefun}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
y0 <- c(1.,2.,4.,3.)
sfun0  <- stepfun(1:3, y0, f = 0)
sfun.2 <- stepfun(1:3, y0, f = .2)
sfun1  <- stepfun(1:3, y0, f = 1)
sfun1c <- stepfun(1:3, y0, right=TRUE)# hence f=1
sfun0
summary(sfun0)
summary(sfun.2)

## look at the internal structure:
unclass(sfun0)
ls(envir = environment(sfun0))

x0 <- seq(0.5,3.5, by = 0.25)
rbind(x=x0, f.f0 = sfun0(x0), f.f02= sfun.2(x0),
      f.f1 = sfun1(x0), f.f1c = sfun1c(x0))
## Identities :
stopifnot(identical(y0[-1], sfun0 (1:3)),# right = FALSE
          identical(y0[-4], sfun1c(1:3)))# right = TRUE
\end{ExampleCode}
\end{Examples}
\HeaderA{stl}{Seasonal Decomposition of Time Series by Loess}{stl}
\keyword{ts}{stl}
%
\begin{Description}\relax
Decompose a time series into seasonal, trend and irregular components
using \code{loess}, acronym STL.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
stl(x, s.window, s.degree = 0,
    t.window = NULL, t.degree = 1,
    l.window = nextodd(period), l.degree = t.degree,
    s.jump = ceiling(s.window/10),
    t.jump = ceiling(t.window/10),
    l.jump = ceiling(l.window/10),
    robust = FALSE,
    inner = if(robust)  1 else 2,
    outer = if(robust) 15 else 0,
    na.action = na.fail)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] univariate time series to be decomposed.
This should be an object of class \code{"ts"} with a frequency
greater than one.
\item[\code{s.window}] either the character string \code{"periodic"} or the span (in
lags) of the loess window for seasonal extraction, which should
be odd.  This has no default.
\item[\code{s.degree}] degree of locally-fitted polynomial in seasonal
extraction.  Should be zero or one.
\item[\code{t.window}] the span (in lags) of the loess window for trend
extraction, which should be odd.  If \code{NULL}, the default,
\code{nextodd(ceiling((1.5*period) / (1-(1.5/s.window))))}, is taken.
\item[\code{t.degree}] degree of locally-fitted polynomial in trend
extraction.  Should be zero or one.
\item[\code{l.window}] the span (in lags) of the loess window of the low-pass
filter used for each subseries.  Defaults to the smallest odd
integer greater than or equal to \code{frequency(x)} which is
recommended since it prevents competition between the trend and
seasonal components.  If not an odd integer its given value is
increased to the next odd one.
\item[\code{l.degree}] degree of locally-fitted polynomial for the subseries
low-pass filter.  Must be 0 or 1.
\item[\code{s.jump, t.jump, l.jump}] integers at least one to increase speed of
the respective smoother.  Linear interpolation happens between every
\code{*.jump}th value.
\item[\code{robust}] logical indicating if robust fitting be used in the
\code{loess} procedure.
\item[\code{inner}] integer; the number of `inner' (backfitting)
iterations; usually very few (2) iterations suffice.
\item[\code{outer}] integer; the number of `outer' robustness
iterations.
\item[\code{na.action}] action on missing values.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The seasonal component is found by \emph{loess} smoothing the
seasonal sub-series (the series of all January values, \ldots); if
\code{s.window = "periodic"} smoothing is effectively replaced by
taking the mean. The seasonal values are removed, and the remainder
smoothed to find the trend. The overall level is removed from the
seasonal component and added to the trend component. This process is
iterated a few times.  The \code{remainder} component is the
residuals from the seasonal plus trend fit.

Several methods for the resulting class \code{"stl"} objects, see,
\code{\LinkA{plot.stl}{plot.stl}}.
\end{Details}
%
\begin{Value}
\code{stl} returns an object of class \code{"stl"} with components
\begin{ldescription}
\item[\code{time.series}] a multiple time series with columns
\code{seasonal}, \code{trend} and \code{remainder}.
\item[\code{weights}] the final robust weights (all one if fitting is not
done robustly).
\item[\code{call}] the matched call.
\item[\code{win}] integer (length 3 vector) with the spans used for the \code{"s"},
\code{"t"}, and \code{"l"} smoothers.
\item[\code{deg}] integer (length 3) vector with the polynomial degrees for
these smoothers.
\item[\code{jump}] integer (length 3) vector with the `jumps' (skips)
used for these smoothers.
\item[\code{ni}] number of \bold{i}nner iterations
\item[\code{no}] number of \bold{o}uter robustness iterations
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
This is similar to but not identical to the \code{stl} function in
S-PLUS. The \code{remainder} component given by S-PLUS is the sum of
the \code{trend} and \code{remainder} series from this function.
\end{Note}
%
\begin{Author}\relax
B.D. Ripley; Fortran code by Cleveland \emph{et al.} (1990) from
\file{netlib}.
\end{Author}
%
\begin{References}\relax
R. B. Cleveland, W. S. Cleveland, J.E.  McRae, and I. Terpenning (1990)
STL:  A  Seasonal-Trend  Decomposition  Procedure Based on Loess.
\emph{Journal of Official Statistics}, \bold{6}, 3--73.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{plot.stl}{plot.stl}} for \code{stl} methods;
\code{\LinkA{loess}{loess}} in package \pkg{stats} (which is not actually
used in \code{stl}).

\code{\LinkA{StructTS}{StructTS}} for different kind of decomposition.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

plot(stl(nottem, "per"))
plot(stl(nottem, s.window = 4, t.window = 50, t.jump = 1))

plot(stllc <- stl(log(co2), s.window=21))
summary(stllc)
## linear trend, strict period.
plot(stl(log(co2), s.window="per", t.window=1000))

## Two STL plotted side by side :
        stmd <- stl(mdeaths, s.window = "per") # non-robust
summary(stmR <- stl(mdeaths, s.window = "per", robust = TRUE))
op <- par(mar = c(0, 4, 0, 3), oma = c(5, 0, 4, 0), mfcol = c(4, 2))
plot(stmd, set.pars=NULL, labels = NULL,
     main = "stl(mdeaths, s.w = \"per\",  robust = FALSE / TRUE )")
plot(stmR, set.pars=NULL)
# mark the 'outliers' :
(iO <- which(stmR $ weights  < 1e-8)) # 10 were considered outliers
sts <- stmR$time.series
points(time(sts)[iO], 0.8* sts[,"remainder"][iO], pch = 4, col = "red")
par(op)# reset
\end{ExampleCode}
\end{Examples}
\HeaderA{stlmethods}{Methods for STL Objects}{stlmethods}
\aliasA{plot.stl}{stlmethods}{plot.stl}
\keyword{ts}{stlmethods}
%
\begin{Description}\relax
Methods for objects of class \code{stl}, typically the result of
\code{\LinkA{stl}{stl}}.  The \code{plot} method does a multiple figure plot
with some flexibility.

There are also (non-visible) \code{print} and \code{summary} methods.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'stl'
plot(x, labels = colnames(X),
     set.pars = list(mar = c(0, 6, 0, 6), oma = c(6, 0, 4, 0),
                     tck = -0.01, mfrow = c(nplot, 1)),
     main = NULL, range.bars = TRUE, ...,
     col.range = "light gray")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] \code{\LinkA{stl}{stl}} object.
\item[\code{labels}] character of length 4 giving the names of the component
time-series.
\item[\code{set.pars}] settings for \code{\LinkA{par}{par}(.)} when setting up the plot.
\item[\code{main}] plot main title.
\item[\code{range.bars}] logical indicating if each plot should have a bar at
its right side which are of equal heights in user coordinates.
\item[\code{...}] further arguments passed to or from other methods.
\item[\code{col.range}] colour to be used for the range bars, if plotted.
Note this appears after \code{...} and so cannot be abbreviated.
\end{ldescription}
\end{Arguments}
%
\begin{SeeAlso}\relax
\code{\LinkA{plot.ts}{plot.ts}} and \code{\LinkA{stl}{stl}}, particularly for
examples.
\end{SeeAlso}
\HeaderA{StructTS}{Fit Structural Time Series}{StructTS}
\aliasA{predict.StructTS}{StructTS}{predict.StructTS}
\aliasA{print.StructTS}{StructTS}{print.StructTS}
\keyword{ts}{StructTS}
%
\begin{Description}\relax
Fit a structural model for a time series by maximum likelihood.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
StructTS(x, type = c("level", "trend", "BSM"), init = NULL,
         fixed = NULL, optim.control = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a univariate numeric time series. Missing values are allowed.

\item[\code{type}] the class of structural model.  If omitted, a BSM is used
for a time series with \code{frequency(x) > 1}, and a local trend
model otherwise.

\item[\code{init}] initial values of the variance parameters.

\item[\code{fixed}] optional numeric vector of the same length as the total
number of parameters.  If supplied, only \code{NA} entries in
\code{fixed} will be varied.  Probably most useful for setting
variances to zero.

\item[\code{optim.control}] List of control parameters for
\code{\LinkA{optim}{optim}}.  Method \code{"L-BFGS-B"} is used.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\emph{Structural time series} models are (linear Gaussian) state-space
models for (univariate) time series based on a decomposition of the
series into a number of components. They are specified by a set of
error variances, some of which may be zero.

The simplest model is the \emph{local level} model specified by
\code{type = "level"}.  This has an underlying level \eqn{\mu_t}{} which
evolves by
\deqn{\mu_{t+1} = \mu_t + \xi_t,  \qquad \xi_t \sim N(0, \sigma^2_\xi)}{}
The observations are
\deqn{x_t = \mu_t + \epsilon_t, \qquad \epsilon_t \sim  N(0, \sigma^2_\epsilon)}{}
There are two parameters, \eqn{\sigma^2_\xi}{}
and \eqn{\sigma^2_\epsilon}{}.  It is an ARIMA(0,1,1) model,
but with restrictions on the parameter set.

The \emph{local linear trend model}, \code{type = "trend"}, has the same
measurement equation, but with a time-varying slope in the dynamics for
\eqn{\mu_t}{}, given by
\deqn{
   \mu_{t+1} = \mu_t + \nu_t + \xi_t, \qquad  \xi_t \sim N(0, \sigma^2_\xi)
 }{}
\deqn{
   \nu_{t+1} = \nu_t + \zeta_t, \qquad \zeta_t \sim N(0, \sigma^2_\zeta)
 }{}
with three variance parameters.  It is not uncommon to find
\eqn{\sigma^2_\zeta = 0}{} (which reduces to the local
level model) or \eqn{\sigma^2_\xi = 0}{}, which ensures a
smooth trend.  This is a restricted ARIMA(0,2,2) model.

The \emph{basic structural model}, \code{type = "BSM"}, is a local
trend model with an additional seasonal component. Thus the measurement
equation is
\deqn{x_t = \mu_t + \gamma_t + \epsilon_t, \qquad \epsilon_t \sim  N(0, \sigma^2_\epsilon)}{}
where \eqn{\gamma_t}{} is a seasonal component with dynamics
\deqn{
   \gamma_{t+1} = -\gamma_t + \cdots + \gamma_{t-s+2} + \omega_t, \qquad
   \omega_t \sim N(0, \sigma^2_\omega)
 }{}
The boundary case \eqn{\sigma^2_\omega = 0}{} corresponds
to a deterministic (but arbitrary) seasonal pattern.  (This is
sometimes known as the `dummy variable' version of the BSM.)
\end{Details}
%
\begin{Value}
A list of class \code{"StructTS"} with components:

\begin{ldescription}
\item[\code{coef}] the estimated variances of the components.
\item[\code{loglik}] the maximized log-likelihood.  Note that as all these
models are non-stationary this includes a diffuse prior for some
observations and hence is not comparable to \code{\LinkA{arima}{arima}}
nor different types of structural models.
\item[\code{loglik0}] the maximized log-likelihood with the constant used
prior to \R{} 2.16.0, for backwards compatibility.
\item[\code{data}] the time series \code{x}.
\item[\code{residuals}] the standardized residuals.
\item[\code{fitted}] a multiple time series with one component for the level,
slope and seasonal components, estimated contemporaneously (that is
at time \eqn{t}{} and not at the end of the series).
\item[\code{call}] the matched call.
\item[\code{series}] the name of the series \code{x}.
\item[\code{code}] the \code{convergence} code returned by \code{\LinkA{optim}{optim}}.
\item[\code{model, model0}] Lists representing the Kalman Filter used in the
fitting.  See \code{\LinkA{KalmanLike}{KalmanLike}}.  \code{model0} is the
initial state of the filter, \code{model} its final state.
\item[\code{xtsp}] the \code{tsp} attributes of \code{x}.
\end{ldescription}
\end{Value}
%
\begin{Note}\relax
Optimization of structural models is a lot harder than many of the
references admit.  For example, the \code{\LinkA{AirPassengers}{AirPassengers}} data
are considered in Brockwell \& Davis (1996): their solution appears to
be a local maximum, but nowhere near as good a fit as that produced by
\code{StructTS}.  It is quite common to find fits with one or more
variances zero, and this can include \eqn{\sigma^2_\epsilon}{}.
\end{Note}
%
\begin{References}\relax
Brockwell, P. J. \& Davis, R. A. (1996).
\emph{Introduction to Time Series and Forecasting}.
Springer, New York.
Sections 8.2 and 8.5.

Durbin, J. and Koopman, S. J. (2001) \emph{Time Series Analysis by
State Space Methods.}  Oxford University Press.

Harvey, A. C. (1989)
\emph{Forecasting, Structural Time Series Models and the Kalman Filter}.
Cambridge University Press.

Harvey, A. C. (1993) \emph{Time Series Models}.
2nd Edition, Harvester Wheatsheaf.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{KalmanLike}{KalmanLike}}, \code{\LinkA{tsSmooth}{tsSmooth}};
\code{\LinkA{stl}{stl}} for different kind of (seasonal) decomposition.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## see also JohnsonJohnson, Nile and AirPassengers
require(graphics)

trees <- window(treering, start=0)
(fit <- StructTS(trees, type = "level"))
plot(trees)
lines(fitted(fit), col = "green")
tsdiag(fit)

(fit <- StructTS(log10(UKgas), type = "BSM"))
par(mfrow = c(4, 1)) # to give appropriate aspect ratio for next plot.
plot(log10(UKgas))
plot(cbind(fitted(fit), resids=resid(fit)), main = "UK gas consumption")

## keep some parameters fixed; trace optimizer:
StructTS(log10(UKgas), type = "BSM", fixed = c(0.1,0.001,NA,NA),
         optim.control = list(trace=TRUE))
\end{ExampleCode}
\end{Examples}
\HeaderA{summary.aov}{Summarize an Analysis of Variance Model}{summary.aov}
\aliasA{print.summary.aov}{summary.aov}{print.summary.aov}
\aliasA{print.summary.aovlist}{summary.aov}{print.summary.aovlist}
\aliasA{summary.aovlist}{summary.aov}{summary.aovlist}
\keyword{models}{summary.aov}
\keyword{regression}{summary.aov}
%
\begin{Description}\relax
Summarize an analysis of variance model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'aov'
summary(object, intercept = FALSE, split,
        expand.split = TRUE, keep.zero.df = TRUE, ...)

## S3 method for class 'aovlist'
summary(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] An object of class \code{"aov"} or \code{"aovlist"}.
\item[\code{intercept}] logical: should intercept terms be included?
\item[\code{split}] an optional named list, with names corresponding to terms
in the model.  Each component is itself a list with integer
components giving contrasts whose contributions are to be summed.
\item[\code{expand.split}] logical: should the split apply also to
interactions involving the factor?
\item[\code{keep.zero.df}] logical: should terms with no degrees of freedom
be included?
\item[\code{...}] Arguments to be passed to or from other methods,
for \code{summary.aovlist} including those for \code{summary.aov}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An object of class \code{c("summary.aov", "listof")} or
\code{"summary.aovlist"} respectively.

For fits with a single stratum the result will be a list of
ANOVA tables, one for each response (even if there is only one response):
the tables are of class \code{"anova"} inheriting from class
\code{"data.frame"}.  They have columns \code{"Df"}, \code{"Sum Sq"},
\code{"Mean Sq"}, as well as \code{"F value"} and \code{"Pr(>F)"} if
there are non-zero residual degrees of freedom.  There is a row for
each term in the model, plus one for \code{"Residuals"} if there
are any.

For multistratum fits the return value is a list of such summaries,
one for each stratum.
\end{Value}
%
\begin{Note}\relax
The use of \code{expand.split = TRUE} is little tested: it is always
possible to set it to \code{FALSE} and specify exactly all
the splits required.
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{aov}{aov}}, \code{\LinkA{summary}{summary}}, \code{\LinkA{model.tables}{model.tables}},
\code{\LinkA{TukeyHSD}{TukeyHSD}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## For a simple example see example(aov)

# Cochran and Cox (1957, p.164)
# 3x3 factorial with ordered factors, each is average of 12. 
CC <- data.frame(
    y = c(449, 413, 326, 409, 358, 291, 341, 278, 312)/12,
    P = ordered(gl(3, 3)), N = ordered(gl(3, 1, 9))
)
CC.aov <- aov(y ~ N * P, data = CC , weights = rep(12, 9))
summary(CC.aov)

# Split both main effects into linear and quadratic parts.
summary(CC.aov, split = list(N = list(L = 1, Q = 2),
                             P = list(L = 1, Q = 2)))

# Split only the interaction
summary(CC.aov, split = list("N:P" = list(L.L = 1, Q = 2:4)))

# split on just one var
summary(CC.aov, split = list(P = list(lin = 1, quad = 2)))
summary(CC.aov, split = list(P = list(lin = 1, quad = 2)),
        expand.split=FALSE)
\end{ExampleCode}
\end{Examples}
\HeaderA{summary.glm}{Summarizing Generalized Linear Model Fits}{summary.glm}
\aliasA{print.summary.glm}{summary.glm}{print.summary.glm}
\keyword{models}{summary.glm}
\keyword{regression}{summary.glm}
%
\begin{Description}\relax
These functions are all \code{\LinkA{methods}{methods}} for class \code{glm} or
\code{summary.glm} objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'glm'
summary(object, dispersion = NULL, correlation = FALSE,
        symbolic.cor = FALSE, ...)

## S3 method for class 'summary.glm'
print(x, digits = max(3, getOption("digits") - 3),
      symbolic.cor = x$symbolic.cor,
      signif.stars = getOption("show.signif.stars"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object of class \code{"glm"}, usually, a result of a
call to \code{\LinkA{glm}{glm}}.
\item[\code{x}] an object of class \code{"summary.glm"}, usually, a result of a
call to \code{summary.glm}.
\item[\code{dispersion}] the dispersion parameter for the family used.
Either a single numerical value or \code{NULL} (the default), when
it is inferred from \code{object} (see `Details').
\item[\code{correlation}] logical; if \code{TRUE}, the correlation matrix of
the estimated parameters is returned and printed.
\item[\code{digits}] the number of significant digits to use when printing.
\item[\code{symbolic.cor}] logical. If \code{TRUE}, print the correlations in
a symbolic form (see \code{\LinkA{symnum}{symnum}}) rather than as numbers.
\item[\code{signif.stars}] logical. If \code{TRUE}, `significance stars'
are printed for each coefficient.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{print.summary.glm} tries to be smart about formatting the
coefficients, standard errors, etc. and additionally gives
`significance stars' if \code{signif.stars} is \code{TRUE}.
The \code{coefficients} component of the result gives the estimated
coefficients and their estimated standard errors, together with their
ratio.  This third column is labelled \code{t ratio} if the
dispersion is estimated, and \code{z ratio} if the dispersion is known
(or fixed by the family).  A fourth column gives the two-tailed
p-value corresponding to the t or z ratio based on a Student t or
Normal reference distribution.  (It is possible that the dispersion is
not known and there are no residual degrees of freedom from which to
estimate it.  In that case the estimate is \code{NaN}.)

Aliased coefficients are omitted in the returned object but restored
by the \code{print} method.

Correlations are printed to two decimal places (or symbolically): to
see the actual correlations print \code{summary(object)\$correlation}
directly.

The dispersion of a GLM is not used in the fitting process, but it is
needed to find standard errors.
If \code{dispersion} is not supplied or \code{NULL},
the dispersion is taken as \code{1} for the \code{binomial} and
\code{Poisson} families, and otherwise estimated by the residual
Chisquared statistic (calculated from cases with non-zero weights)
divided by the residual degrees of freedom.

\code{summary} can be used with Gaussian \code{glm} fits to handle the
case of a linear regression with known error variance, something not
handled by \code{\LinkA{summary.lm}{summary.lm}}.
\end{Details}
%
\begin{Value}
\code{summary.glm} returns an object of class \code{"summary.glm"}, a
list with components

\begin{ldescription}
\item[\code{call}] the component from \code{object}.
\item[\code{family}] the component from \code{object}.
\item[\code{deviance}] the component from \code{object}.
\item[\code{contrasts}] the component from \code{object}.
\item[\code{df.residual}] the component from \code{object}.
\item[\code{null.deviance}] the component from \code{object}.
\item[\code{df.null}] the component from \code{object}.
\item[\code{deviance.resid}] the deviance residuals:
see \code{\LinkA{residuals.glm}{residuals.glm}}.
\item[\code{coefficients}] the matrix of coefficients, standard errors,
z-values and p-values.  Aliased coefficients are omitted.
\item[\code{aliased}] named logical vector showing if the original
coefficients are aliased.
\item[\code{dispersion}] either the supplied argument or the inferred/estimated
dispersion if the latter is \code{NULL}.
\item[\code{df}] a 3-vector of the rank of the model and the number of
residual degrees of freedom, plus number of non-aliased coefficients.
\item[\code{cov.unscaled}] the unscaled (\code{dispersion = 1}) estimated covariance
matrix of the estimated coefficients.
\item[\code{cov.scaled}] ditto, scaled by \code{dispersion}.
\item[\code{correlation}] (only if \code{correlation} is true.)  The estimated
correlations of the estimated coefficients.
\item[\code{symbolic.cor}] (only if \code{correlation} is true.)  The value
of the argument \code{symbolic.cor}.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{glm}{glm}}, \code{\LinkA{summary}{summary}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## For examples see example(glm)
\end{ExampleCode}
\end{Examples}
\HeaderA{summary.lm}{Summarizing Linear Model Fits}{summary.lm}
\aliasA{print.summary.lm}{summary.lm}{print.summary.lm}
\aliasA{summary.mlm}{summary.lm}{summary.mlm}
\keyword{regression}{summary.lm}
\keyword{models}{summary.lm}
%
\begin{Description}\relax
\code{summary} method for class \code{"lm"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'lm'
summary(object, correlation = FALSE, symbolic.cor = FALSE, ...)

## S3 method for class 'summary.lm'
print(x, digits = max(3, getOption("digits") - 3),
      symbolic.cor = x$symbolic.cor,
      signif.stars = getOption("show.signif.stars"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object of class \code{"lm"}, usually, a result of a
call to \code{\LinkA{lm}{lm}}.
\item[\code{x}] an object of class \code{"summary.lm"}, usually, a result of a
call to \code{summary.lm}.
\item[\code{correlation}] logical; if \code{TRUE}, the correlation matrix of
the estimated parameters is returned and printed.
\item[\code{digits}] the number of significant digits to use when printing.
\item[\code{symbolic.cor}] logical. If \code{TRUE}, print the correlations in
a symbolic form (see \code{\LinkA{symnum}{symnum}}) rather than as numbers.
\item[\code{signif.stars}] logical. If \code{TRUE}, `significance stars'
are printed for each coefficient.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{print.summary.lm} tries to be smart about formatting the
coefficients, standard errors, etc. and additionally gives
`significance stars' if \code{signif.stars} is \code{TRUE}.

Correlations are printed to two decimal places (or symbolically): to
see the actual correlations print \code{summary(object)\$correlation}
directly.
\end{Details}
%
\begin{Value}
The function \code{summary.lm} computes and returns a list of summary
statistics of the fitted linear model given in \code{object}, using
the components (list elements) \code{"call"} and \code{"terms"}
from its argument, plus
\begin{ldescription}
\item[\code{residuals}] the \emph{weighted} residuals, the usual residuals
rescaled by the square root of the weights specified in the call to
\code{lm}.
\item[\code{coefficients}] a \eqn{p \times 4}{} matrix with columns for
the estimated coefficient, its standard error, t-statistic and
corresponding (two-sided) p-value.  Aliased coefficients are omitted.
\item[\code{aliased}] named logical vector showing if the original
coefficients are aliased.
\item[\code{sigma}] the square root of the estimated variance of the random
error
\deqn{\hat\sigma^2 = \frac{1}{n-p}\sum_i{w_i R_i^2},}{}
where \eqn{R_i}{} is the \eqn{i}{}-th residual, \code{residuals[i]}.
\item[\code{df}] degrees of freedom, a 3-vector \eqn{(p, n-p, p*)}{}, the last
being the number of non-aliased coefficients.
\item[\code{fstatistic}] (for models including non-intercept terms)
a 3-vector with the value of the F-statistic with
its numerator and denominator degrees of freedom.
\item[\code{r.squared}] \eqn{R^2}{}, the `fraction of variance explained by
the model',
\deqn{R^2 = 1 - \frac{\sum_i{R_i^2}}{\sum_i(y_i- y^*)^2},}{}
where \eqn{y^*}{} is the mean of \eqn{y_i}{} if there is an
intercept and zero otherwise.
\item[\code{adj.r.squared}] the above \eqn{R^2}{} statistic
`\emph{adjusted}', penalizing for higher \eqn{p}{}.
\item[\code{cov.unscaled}] a \eqn{p \times p}{} matrix of (unscaled)
covariances of the \eqn{\hat\beta_j}{}, \eqn{j=1, \dots, p}{}.
\item[\code{correlation}] the correlation matrix corresponding to the above
\code{cov.unscaled}, if \code{correlation = TRUE} is specified.
\item[\code{symbolic.cor}] (only if \code{correlation} is true.)  The value
of the argument \code{symbolic.cor}.
\item[\code{na.action}] from \code{object}, if present there.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
The model fitting function \code{\LinkA{lm}{lm}}, \code{\LinkA{summary}{summary}}.

Function \code{\LinkA{coef}{coef}} will extract the matrix of coefficients
with standard errors, t-statistics and p-values.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

##-- Continuing the  lm(.) example:
coef(lm.D90)# the bare coefficients
sld90 <- summary(lm.D90 <- lm(weight ~ group -1))# omitting intercept
sld90
coef(sld90)# much more
\end{ExampleCode}
\end{Examples}
\HeaderA{summary.manova}{Summary Method for Multivariate Analysis of Variance}{summary.manova}
\aliasA{print.summary.manova}{summary.manova}{print.summary.manova}
\keyword{models}{summary.manova}
%
\begin{Description}\relax
A \code{summary} method for class \code{"manova"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'manova'
summary(object,
        test = c("Pillai", "Wilks", "Hotelling-Lawley", "Roy"),
        intercept = FALSE, tol = 1e-7, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] An object of class \code{"manova"} or an \code{aov}
object with multiple responses.
\item[\code{test}] The name of the test statistic to be used.  Partial
matching is used so the name can be abbreviated.
\item[\code{intercept}] logical.  If \code{TRUE}, the intercept term is
included in the table.
\item[\code{tol}] tolerance to be used in deciding if the residuals are
rank-deficient: see \code{\LinkA{qr}{qr}}.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \code{summary.manova} method uses a multivariate test statistic
for the summary table.  Wilks' statistic is most popular in the
literature, but the default Pillai--Bartlett statistic is recommended
by Hand and Taylor (1987).

The table gives a transformation of the test statistic which has
approximately an F distribution.  The approximations used follow
S-PLUS and SAS (the latter apart from some cases of the
Hotelling--Lawley statistic), but many other distributional
approximations exist: see Anderson (1984) and Krzanowski and Marriott
(1994) for further references.  All four approximate F statistics are
the same when the term being tested has one degree of freedom, but in
other cases that for the Roy statistic is an upper bound.

The tolerance \code{tol} is applied to the QR decomposition of the
residual correlation matrix (unless some response has essentially zero
residuals, when it is unscaled).  Thus the default value guards
against very highly correlated responses: it can be reduced but doing
so will allow rather inaccurate results and it will normally be better
to transform the responses to remove the high correlation.
\end{Details}
%
\begin{Value}
An object of class \code{"summary.manova"}.  If there is a positive
residual degrees of freedom, this is a list with components
\begin{ldescription}
\item[\code{row.names}] The names of the terms, the row names of the
\code{stats} table if present.
\item[\code{SS}] A named list of sums of squares and product matrices.
\item[\code{Eigenvalues}] A matrix of eigenvalues.
\item[\code{stats}] A matrix of the statistics, approximate F value,
degrees of freedom and P value.
\end{ldescription}
otherwise components \code{row.names}, \code{SS} and \code{Df}
(degrees of freedom) for the terms (and not the residuals).
\end{Value}
%
\begin{References}\relax
Anderson, T. W. (1994) \emph{An Introduction to Multivariate
Statistical Analysis.} Wiley.

Hand, D. J. and Taylor, C. C.  (1987)
\emph{Multivariate Analysis of Variance and Repeated Measures.}
Chapman and Hall.

Krzanowski, W. J. (1988) \emph{Principles of Multivariate Analysis. A
User's Perspective.} Oxford.

Krzanowski, W. J. and Marriott, F. H. C. (1994) \emph{Multivariate
Analysis. Part I: Distributions, Ordination and Inference.} Edward Arnold.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{manova}{manova}}, \code{\LinkA{aov}{aov}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

## Example on producing plastic film from Krzanowski (1998, p. 381)
tear <- c(6.5, 6.2, 5.8, 6.5, 6.5, 6.9, 7.2, 6.9, 6.1, 6.3,
          6.7, 6.6, 7.2, 7.1, 6.8, 7.1, 7.0, 7.2, 7.5, 7.6)
gloss <- c(9.5, 9.9, 9.6, 9.6, 9.2, 9.1, 10.0, 9.9, 9.5, 9.4,
           9.1, 9.3, 8.3, 8.4, 8.5, 9.2, 8.8, 9.7, 10.1, 9.2)
opacity <- c(4.4, 6.4, 3.0, 4.1, 0.8, 5.7, 2.0, 3.9, 1.9, 5.7,
             2.8, 4.1, 3.8, 1.6, 3.4, 8.4, 5.2, 6.9, 2.7, 1.9)
Y <- cbind(tear, gloss, opacity)
rate <- factor(gl(2,10), labels=c("Low", "High"))
additive <- factor(gl(2, 5, length=20), labels=c("Low", "High"))

fit <- manova(Y ~ rate * additive)
summary.aov(fit)           # univariate ANOVA tables
summary(fit, test="Wilks") # ANOVA table of Wilks' lambda
summary(fit)               # same F statistics as single-df terms
\end{ExampleCode}
\end{Examples}
\HeaderA{summary.nls}{Summarizing Non-Linear Least-Squares Model Fits}{summary.nls}
\aliasA{print.summary.nls}{summary.nls}{print.summary.nls}
\keyword{regression}{summary.nls}
\keyword{models}{summary.nls}
%
\begin{Description}\relax
\code{summary} method for class \code{"nls"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'nls'
summary(object, correlation = FALSE, symbolic.cor = FALSE, ...)

## S3 method for class 'summary.nls'
print(x, digits = max(3, getOption("digits") - 3),
      symbolic.cor = x$symbolic.cor,
      signif.stars = getOption("show.signif.stars"), ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object of class \code{"nls"}.
\item[\code{x}] an object of class \code{"summary.nls"}, usually the result of a
call to \code{summary.nls}.
\item[\code{correlation}] logical; if \code{TRUE}, the correlation matrix of
the estimated parameters is returned and printed.
\item[\code{digits}] the number of significant digits to use when printing.
\item[\code{symbolic.cor}] logical.  If \code{TRUE}, print the correlations in
a symbolic form (see \code{\LinkA{symnum}{symnum}}) rather than as numbers.
\item[\code{signif.stars}] logical.  If \code{TRUE}, `significance stars'
are printed for each coefficient.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The distribution theory used to find the distribution of the
standard errors and of the residual standard error (for t ratios) is
based on linearization and is approximate, maybe very approximate.

\code{print.summary.nls} tries to be smart about formatting the
coefficients, standard errors, etc. and additionally gives
`significance stars' if \code{signif.stars} is \code{TRUE}.

Correlations are printed to two decimal places (or symbolically): to
see the actual correlations print \code{summary(object)\$correlation}
directly.
\end{Details}
%
\begin{Value}
The function \code{summary.nls} computes and returns a list of summary
statistics of the fitted model given in \code{object}, using
the component  \code{"formula"} from its argument, plus
\begin{ldescription}
\item[\code{residuals}] the \emph{weighted} residuals, the usual residuals
rescaled by the square root of the weights specified in the call to
\code{nls}.
\item[\code{coefficients}] a \eqn{p \times 4}{} matrix with columns for
the estimated coefficient, its standard error, t-statistic and
corresponding (two-sided) p-value.
\item[\code{sigma}] the square root of the estimated variance of the random
error
\deqn{\hat\sigma^2 = \frac{1}{n-p}\sum_i{R_i^2},}{}
where \eqn{R_i}{} is the \eqn{i}{}-th weighted residual.
\item[\code{df}] degrees of freedom, a 2-vector \eqn{(p, n-p)}{}.  (Here and
elsewhere \eqn{n}{} omits observations with zero weights.)
\item[\code{cov.unscaled}] a \eqn{p \times p}{} matrix of (unscaled)
covariances of the parameter estimates.
\item[\code{correlation}] the correlation matrix corresponding to the above
\code{cov.unscaled}, if \code{correlation = TRUE} is specified and
there are a non-zero number of residual degrees of freedom.
\item[\code{symbolic.cor}] (only if \code{correlation} is true.)  The value
of the argument \code{symbolic.cor}.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
The model fitting function \code{\LinkA{nls}{nls}}, \code{\LinkA{summary}{summary}}.

Function \code{\LinkA{coef}{coef}} will extract the matrix of coefficients
with standard errors, t-statistics and p-values.
\end{SeeAlso}
\HeaderA{summary.princomp}{Summary method for Principal Components Analysis}{summary.princomp}
\aliasA{print.summary.princomp}{summary.princomp}{print.summary.princomp}
\keyword{multivariate}{summary.princomp}
%
\begin{Description}\relax
The \code{\LinkA{summary}{summary}} method for class \code{"princomp"}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'princomp'
summary(object, loadings = FALSE, cutoff = 0.1, ...)

## S3 method for class 'summary.princomp'
print(x, digits = 3, loadings = x$print.loadings,
      cutoff = x$cutoff, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object of class \code{"princomp"}, as
from \code{princomp()}.
\item[\code{loadings}] logical. Should loadings be included?
\item[\code{cutoff}] numeric. Loadings below this cutoff in absolute value
are shown as blank in the output.
\item[\code{x}] an object of class "summary.princomp".
\item[\code{digits}] the number of significant digits to be used in listing
loadings.
\item[\code{...}] arguments to be passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
\code{object} with additional components \code{cutoff} and
\code{print.loadings}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{princomp}{princomp}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
summary(pc.cr <- princomp(USArrests, cor=TRUE))
print(summary(princomp(USArrests, cor=TRUE),
              loadings = TRUE, cutoff = 0.2), digits = 2)
\end{ExampleCode}
\end{Examples}
\HeaderA{supsmu}{Friedman's SuperSmoother}{supsmu}
\keyword{smooth}{supsmu}
%
\begin{Description}\relax
Smooth the (x, y) values by Friedman's `super smoother'.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
supsmu(x, y, wt, span = "cv", periodic = FALSE, bass = 0)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] x values for smoothing
\item[\code{y}] y values for smoothing
\item[\code{wt}] case weights, by default all equal
\item[\code{span}] the fraction of the observations in the span of the running
lines smoother, or \code{"cv"} to choose this by leave-one-out
cross-validation.
\item[\code{periodic}] if \code{TRUE}, the x values are assumed to be in
\code{[0, 1]} and of period 1.
\item[\code{bass}] controls the smoothness of the fitted curve. Values of up
to 10 indicate increasing smoothness.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
\code{supsmu} is a running lines smoother which chooses between three
spans for the lines. The running lines smoothers are symmetric, with
\code{k/2} data points each side of the predicted point, and values of
\code{k} as \code{0.5 * n}, \code{0.2 * n} and \code{0.05 * n}, where
\code{n} is the number of data points.  If \code{span} is specified,
a single smoother with span \code{span * n} is used.

The best of the three smoothers is chosen by cross-validation for each
prediction. The best spans are then smoothed by a running lines
smoother and the final prediction chosen by linear interpolation. 

The FORTRAN code says: ``For small samples (\code{n < 40}) or if
there are substantial serial correlations between observations close
in x-value, then a pre-specified fixed span smoother (\code{span >
      0}) should be used.  Reasonable span values are 0.2 to 0.4.''

Cases with non-finite values of \code{x}, \code{y} or \code{wt} are
dropped, with a warning.
\end{Details}
%
\begin{Value}
A list with components
\begin{ldescription}
\item[\code{x}] the input values in increasing order with duplicates removed.
\item[\code{y}] the corresponding y values on the fitted curve.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Friedman, J. H. (1984)
SMART User's Guide.
Laboratory for Computational Statistics, Stanford University Technical
Report No. 1.

Friedman, J. H. (1984)
A variable span scatterplot smoother.
Laboratory for Computational Statistics, Stanford University Technical
Report No. 5.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ppr}{ppr}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

with(cars, {
    plot(speed, dist)
    lines(supsmu(speed, dist))
    lines(supsmu(speed, dist, bass = 7), lty = 2)
    })
\end{ExampleCode}
\end{Examples}
\HeaderA{symnum}{Symbolic Number Coding}{symnum}
\keyword{utilities}{symnum}
\keyword{character}{symnum}
%
\begin{Description}\relax
Symbolically encode a given numeric or logical vector or array.
Particularly useful for visualization of structured matrices,
e.g., correlation, sparse, or logical ones.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
symnum(x, cutpoints = c(0.3, 0.6, 0.8, 0.9, 0.95),
       symbols = if(numeric.x) c(" ", ".", ",", "+", "*", "B")
                 else c(".", "|"),
       legend = length(symbols) >= 3,
       na = "?", eps = 1e-5, numeric.x = is.numeric(x),
       corr = missing(cutpoints) && numeric.x,
       show.max = if(corr) "1", show.min = NULL,
       abbr.colnames = has.colnames,
       lower.triangular = corr && is.numeric(x) && is.matrix(x),
       diag.lower.tri   = corr && !is.null(show.max))
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric or logical vector or array.
\item[\code{cutpoints}] numeric vector whose values \code{cutpoints[j]}
\eqn{= c_j}{}  (\emph{after} augmentation, see \code{corr}
below) are used for intervals.
\item[\code{symbols}] character vector, one shorter than (the
\emph{augmented}, see \code{corr} below) \code{cutpoints}.
\code{symbols[j]}\eqn{= s_j}{} are used as `code' for
the (half open) interval \eqn{(c_j,c_{j+1}]}{}.

When \code{numeric.x} is \code{FALSE}, i.e., by default when
argument \code{x} is \code{logical}, the default is
\code{c(".","|")} (graphical 0 / 1 s).
\item[\code{legend}] logical indicating if a \code{"legend"} attribute is
desired.
\item[\code{na}] character or logical. How \code{\LinkA{NA}{NA}s} are coded.  If
\code{na == FALSE}, \code{NA}s are coded invisibly, \emph{including}
the \code{"legend"} attribute below, which otherwise mentions NA
coding.
\item[\code{eps}] absolute precision to be used at left and right boundary.
\item[\code{numeric.x}] logical indicating if \code{x} should be treated as numbers,
otherwise as logical.
\item[\code{corr}] logical.  If \code{TRUE}, \code{x} contains correlations.
The cutpoints are augmented by \code{0} and \code{1} and
\code{abs(x)} is coded.
\item[\code{show.max}] if \code{TRUE}, or of mode \code{character}, the
maximal cutpoint is coded especially.
\item[\code{show.min}] if \code{TRUE}, or of mode \code{character}, the
minimal cutpoint is coded especially.
\item[\code{abbr.colnames}] logical, integer or \code{NULL} indicating how
column names should be abbreviated (if they are); if \code{NULL}
(or \code{FALSE} and \code{x} has no column names),
the column names will all be empty, i.e., \code{""}; otherwise if
\code{abbr.colnames} is false, they are left unchanged.  If
\code{TRUE} or integer, existing column names will be abbreviated to
\code{\LinkA{abbreviate}{abbreviate}(*, minlength = abbr.colnames)}.
\item[\code{lower.triangular}] logical.  If \code{TRUE} and \code{x} is a
matrix, only the \emph{lower triangular} part of the matrix is coded
as non-blank.
\item[\code{diag.lower.tri}] logical.  If \code{lower.triangular} \emph{and}
this are \code{TRUE}, the \emph{diagonal} part of the matrix is
shown.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An atomic character object of class \code{\LinkA{noquote}{noquote}} and the same
dimensions as \code{x}.

If \code{legend} is \code{TRUE} (as by default when there are more
than two classes), the result has an attribute \code{"legend"}
containing a legend of the returned character codes, in the form
\deqn{c_1 s_1 c_2 s_2 \dots s_n c_{n+1}}{}
where \eqn{c_j}{}\code{ = cutpoints[j]} and
\eqn{s_j}{}\code{ = symbols[j]}.
\end{Value}
%
\begin{Note}\relax
The optional (mostly logical) arguments all try to use smart defaults.
Specifying them explicitly may lead to considerably improved output in
many cases.
\end{Note}
%
\begin{Author}\relax
Martin Maechler \email{maechler@stat.math.ethz.ch}
\end{Author}
%
\begin{SeeAlso}\relax
\code{\LinkA{as.character}{as.character}}; \code{\LinkA{image}{image}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ii <- 0:8; names(ii) <- ii
symnum(ii, cut= 2*(0:4), sym = c(".", "-", "+", "$"))
symnum(ii, cut= 2*(0:4), sym = c(".", "-", "+", "$"), show.max=TRUE)

symnum(1:12 %% 3 == 0)# --> "|" = TRUE, "." = FALSE  for logical

## Pascal's Triangle modulo 2 -- odd and even numbers:
N <- 38
pascal <- t(sapply(0:N, function(n) round(choose(n, 0:N - (N-n)%/%2))))
rownames(pascal) <- rep("", 1+N) # <-- to improve "graphic"
symnum(pascal %% 2, symbols = c(" ", "A"), numeric = FALSE)

##-- Symbolic correlation matrices:
symnum(cor(attitude), diag = FALSE)
symnum(cor(attitude), abbr.= NULL)
symnum(cor(attitude), abbr.= FALSE)
symnum(cor(attitude), abbr.= 2)

symnum(cor(rbind(1, rnorm(25), rnorm(25)^2)))
symnum(cor(matrix(rexp(30, 1), 5, 18))) # <<-- PATTERN ! --
symnum(cm1 <- cor(matrix(rnorm(90) ,  5, 18))) # < White Noise SMALL n
symnum(cm1, diag=FALSE)
symnum(cm2 <- cor(matrix(rnorm(900), 50, 18))) # < White Noise "BIG" n
symnum(cm2, lower=FALSE)

## NA's:
Cm <- cor(matrix(rnorm(60),  10, 6)); Cm[c(3,6), 2] <- NA
symnum(Cm, show.max=NULL)

## Graphical P-values (aka "significance stars"):
pval <- rev(sort(c(outer(1:6, 10^-(1:3)))))
symp <- symnum(pval, corr=FALSE,
               cutpoints = c(0,  .001,.01,.05, .1, 1),
               symbols = c("***","**","*","."," "))
noquote(cbind(P.val = format(pval), Signif= symp))
\end{ExampleCode}
\end{Examples}
\HeaderA{t.test}{Student's t-Test}{t.test}
\methaliasA{t.test.default}{t.test}{t.test.default}
\methaliasA{t.test.formula}{t.test}{t.test.formula}
\keyword{htest}{t.test}
%
\begin{Description}\relax
Performs one and two sample t-tests on vectors of data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
t.test(x, ...)

## Default S3 method:
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)

## S3 method for class 'formula'
t.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a (non-empty) numeric vector of data values.
\item[\code{y}] an optional (non-empty) numeric vector of data values.
\item[\code{alternative}] a character string specifying the alternative
hypothesis, must be one of \code{"two.sided"} (default),
\code{"greater"} or \code{"less"}.  You can specify just the initial
letter.
\item[\code{mu}] a number indicating the true value of the mean (or
difference in means if you are performing a two sample test).
\item[\code{paired}] a logical indicating whether you want a paired
t-test.
\item[\code{var.equal}] a logical variable indicating whether to treat the
two variances as being equal. If \code{TRUE} then the pooled
variance is used to estimate the variance otherwise the Welch
(or Satterthwaite) approximation to the degrees of freedom is used.
\item[\code{conf.level}] confidence level of the interval.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
is a numeric variable giving the data values and \code{rhs} a factor
with two levels giving the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The formula interface is only applicable for the 2-sample tests.

\code{alternative = "greater"} is the alternative that \code{x} has a
larger mean than \code{y}.

If \code{paired} is \code{TRUE} then both \code{x} and \code{y} must
be specified and they must be the same length.  Missing values are
silently removed (in pairs if \code{paired} is \code{TRUE}).  If
\code{var.equal} is \code{TRUE} then the pooled estimate of the
variance is used.  By default, if \code{var.equal} is \code{FALSE}
then the variance is estimated separately for both groups and the
Welch modification to the degrees of freedom is used.

If the input data are effectively constant (compared to the larger of the
two means) an error is generated.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the t-statistic.
\item[\code{parameter}] the degrees of freedom for the t-statistic.
\item[\code{p.value}] the p-value for the test.
\item[\code{conf.int}] a confidence interval for the mean appropriate to the
specified alternative hypothesis.
\item[\code{estimate}] the estimated mean or difference in means depending on
whether it was a one-sample test or a two-sample test.
\item[\code{null.value}] the specified hypothesized value of the mean or mean
difference depending on whether it was a one-sample test or a
two-sample test.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] a character string indicating what type of t-test was
performed.
\item[\code{data.name}] a character string giving the name(s) of the data.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{prop.test}{prop.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

t.test(1:10,y=c(7:20))      # P = .00001855
t.test(1:10,y=c(7:20, 200)) # P = .1245    -- NOT significant anymore

## Classical example: Student's sleep data
plot(extra ~ group, data = sleep)
## Traditional interface
with(sleep, t.test(extra[group == 1], extra[group == 2]))
## Formula interface
t.test(extra ~ group, data = sleep)
\end{ExampleCode}
\end{Examples}
\HeaderA{TDist}{The Student t Distribution}{TDist}
\aliasA{dt}{TDist}{dt}
\aliasA{pt}{TDist}{pt}
\aliasA{qt}{TDist}{qt}
\aliasA{rt}{TDist}{rt}
\keyword{distribution}{TDist}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the t distribution with \code{df} degrees of freedom
(and optional non-centrality parameter \code{ncp}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dt(x, df, ncp, log = FALSE)
pt(q, df, ncp, lower.tail = TRUE, log.p = FALSE)
qt(p, df, ncp, lower.tail = TRUE, log.p = FALSE)
rt(n, df, ncp)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{df}] degrees of freedom (\eqn{> 0}{}, maybe non-integer).  \code{df
      = Inf} is allowed.
\item[\code{ncp}] non-centrality parameter \eqn{\delta}{};
currently except for \code{rt()}, only for \code{abs(ncp) <= 37.62}.
If omitted, use the central t distribution.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \eqn{t}{} distribution with \code{df} \eqn{= \nu}{} degrees of
freedom has density
\deqn{
    f(x) = \frac{\Gamma ((\nu+1)/2)}{\sqrt{\pi \nu} \Gamma (\nu/2)}
    (1 + x^2/\nu)^{-(\nu+1)/2}%
  }{}
for all real \eqn{x}{}.
It has mean \eqn{0}{} (for \eqn{\nu > 1}{}) and
variance \eqn{\frac{\nu}{\nu-2}}{} (for \eqn{\nu > 2}{}).

The general \emph{non-central} \eqn{t}{}
with parameters \eqn{(\nu, \delta)}{} \code{= (df, ncp)}
is defined as the distribution of
\eqn{T_{\nu}(\delta) := (U + \delta)/\sqrt{V/\nu}}{}
where \eqn{U}{} and \eqn{V}{}  are independent random
variables, \eqn{U \sim {\cal N}(0,1)}{} and
\eqn{V \sim \chi^2_\nu}{} (see \LinkA{Chisquare}{Chisquare}).

The most used applications are power calculations for \eqn{t}{}-tests:\\{}
Let \eqn{T = \frac{\bar{X} - \mu_0}{S/\sqrt{n}}}{}
where
\eqn{\bar{X}}{} is the \code{\LinkA{mean}{mean}} and \eqn{S}{} the sample standard
deviation (\code{\LinkA{sd}{sd}}) of \eqn{X_1, X_2, \dots, X_n}{} which are
i.i.d. \eqn{{\cal N}(\mu, \sigma^2)}{}
Then \eqn{T}{} is distributed as non-central \eqn{t}{} with
\code{df}\eqn{{} = n-1}{}
degrees of freedom and \bold{n}on-\bold{c}entrality \bold{p}arameter
\code{ncp}\eqn{{} = (\mu - \mu_0) \sqrt{n}/\sigma}{}.
\end{Details}
%
\begin{Value}
\code{dt} gives the density,
\code{pt} gives the distribution function,
\code{qt} gives the quantile function, and
\code{rt} generates random deviates.

Invalid arguments will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Note}\relax
Supplying \code{ncp = 0} uses the algorithm for the non-central
distribution, which is not the same algorithm used if \code{ncp} is
omitted.  This is to give consistent behaviour in extreme cases with
values of \code{ncp} very near zero.

The code for non-zero \code{ncp} is principally intended to be used
for moderate values of \code{ncp}: it will not be highly accurate,
especially in the tails, for large values.
\end{Note}
%
\begin{Source}\relax
The central \code{dt} is computed via an accurate formula
provided by Catherine Loader (see the reference in \code{\LinkA{dbinom}{dbinom}}).

For the non-central case of \code{dt}, C code contributed by
Claus EkstrÃ¸m based on the relationship (for
\eqn{x \neq 0}{}) to the cumulative distribution.

For the central case of \code{pt}, a normal approximation in the
tails, otherwise via \code{\LinkA{pbeta}{pbeta}}.

For the non-central case of \code{pt} based on a C translation of

Lenth, R. V. (1989). \emph{Algorithm AS 243} ---
Cumulative distribution function of the non-central \eqn{t}{} distribution,
\emph{Applied Statistics} \bold{38}, 185--189.

This computes the lower tail only, so the upper tail suffers from
cancellation and a warning will be given when this is likely to be
significant.

For central \code{qt}, a C translation of

Hill, G. W. (1970) Algorithm 396: Student's t-quantiles.
\emph{Communications of the ACM}, \bold{13(10)}, 619--620.

altered to take account of

Hill, G. W. (1981) Remark on Algorithm 396, \emph{ACM Transactions on
Mathematical Software}, \bold{7}, 250--1.

The non-central case is done by inversion.
\end{Source}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole. (Except non-central versions.)

Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 2, chapters 28 and 31.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
\code{\LinkA{df}{df}} for the F distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

1 - pt(1:5, df = 1)
qt(.975, df = c(1:10,20,50,100,1000))

tt <- seq(0,10, len=21)
ncp <- seq(0,6, len=31)
ptn <- outer(tt,ncp, function(t,d) pt(t, df = 3, ncp=d))
t.tit <- "Non-central t - Probabilities"
image(tt,ncp,ptn, zlim=c(0,1), main = t.tit)
persp(tt,ncp,ptn, zlim=0:1, r=2, phi=20, theta=200, main=t.tit,
      xlab = "t", ylab = "non-centrality parameter",
      zlab = "Pr(T <= t)")

plot(function(x) dt(x, df = 3, ncp = 2), -3, 11, ylim = c(0, 0.32),
     main="Non-central t - Density", yaxs="i")
\end{ExampleCode}
\end{Examples}
\HeaderA{termplot}{Plot Regression Terms}{termplot}
\keyword{hplot}{termplot}
\keyword{regression}{termplot}
%
\begin{Description}\relax
Plots regression terms against their predictors, optionally with
standard errors and partial residuals added.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
termplot(model, data = NULL, envir = environment(formula(model)),
         partial.resid = FALSE, rug = FALSE,
         terms = NULL, se = FALSE,
         xlabs = NULL, ylabs = NULL, main = NULL,
         col.term = 2, lwd.term = 1.5,
         col.se = "orange", lty.se = 2, lwd.se = 1,
         col.res = "gray", cex = 1, pch = par("pch"),
         col.smth = "darkred", lty.smth = 2, span.smth = 2/3,
         ask = dev.interactive() && nb.fig < n.tms,
         use.factor.levels = TRUE, smooth = NULL, ylim = "common",
         ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model}] fitted model object
\item[\code{data}] data frame in which variables in \code{model} can be
found
\item[\code{envir}] environment in which variables in \code{model} can be found
\item[\code{partial.resid}] logical; should partial residuals be plotted?
\item[\code{rug}] add \LinkA{rug}{rug}plots (jittered 1-d histograms) to the axes?
\item[\code{terms}] which terms to plot (default \code{NULL} means all
terms); a \code{\LinkA{character}{character}} vector, passed to
\code{\LinkA{predict}{predict}(.., term = "terms", terms = *)}.
\item[\code{se}] plot pointwise standard errors?
\item[\code{xlabs}] vector of labels for the x axes
\item[\code{ylabs}] vector of labels for the y axes
\item[\code{main}] logical, or vector of main titles;  if \code{TRUE}, the
model's call is taken as main title, \code{NULL} or \code{FALSE} mean
no titles.
\item[\code{col.term, lwd.term}] color and line width for the `term curve',
see \code{\LinkA{lines}{lines}}.
\item[\code{col.se, lty.se, lwd.se}] color, line type and line width for the
`twice-standard-error curve' when \code{se = TRUE}.
\item[\code{col.res, cex, pch}] color, plotting character expansion and type
for partial residuals, when \code{partial.resid = TRUE}, see
\code{\LinkA{points}{points}}.
\item[\code{ask}] logical; if \code{TRUE}, the user is \emph{ask}ed before
each plot, see \code{\LinkA{par}{par}(ask=.)}.
\item[\code{use.factor.levels}] Should x-axis ticks use factor levels or
numbers for factor terms?
\item[\code{smooth}] \code{NULL} or a function with the same arguments as
\code{\LinkA{panel.smooth}{panel.smooth}} to draw a smooth through the partial
residuals for non-factor terms
\item[\code{lty.smth, col.smth, span.smth}] Passed to \code{smooth}
\item[\code{ylim}] an optional range for the y axis, or \code{"common"} when
a range sufficient for all the plot will be computed, or
\code{"free"} when limits are computed for each plot.
\item[\code{...}] other graphical parameters.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \code{model} object must have a \code{predict} method that accepts
\code{type = terms}, e.g., \code{\LinkA{glm}{glm}} in the \pkg{stats} package,
\code{\LinkA{coxph}{coxph}} and \code{\LinkA{survreg}{survreg}} in
the \Rhref{http://CRAN.R-project.org/package=survival}{\pkg{survival}} package.

For the \code{partial.resid = TRUE} option \code{model} must have a
\code{\LinkA{residuals}{residuals}} method that accepts \code{type = "partial"},
which \code{\LinkA{lm}{lm}} and \code{\LinkA{glm}{glm}} do.

The \code{data} argument should rarely be needed, but in some cases
\code{termplot} may be unable to reconstruct the original data
frame. Using \code{na.action=na.exclude} makes these problems less likely.

Nothing sensible happens for interaction terms, and they may cause errors.
\end{Details}
%
\begin{Value}
The number of terms, invisibly.
\end{Value}
%
\begin{SeeAlso}\relax
For (generalized) linear models, \code{\LinkA{plot.lm}{plot.lm}} and
\code{\LinkA{predict.glm}{predict.glm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

had.splines <- "package:splines" %in% search()
if(!had.splines) rs <- require(splines)
x <- 1:100
z <- factor(rep(LETTERS[1:4],25))
y <- rnorm(100, sin(x/10)+as.numeric(z))
model <- glm(y ~ ns(x,6) + z)

par(mfrow=c(2,2)) ## 2 x 2 plots for same model :
termplot(model, main = paste("termplot( ", deparse(model$call)," ...)"))
termplot(model, rug=TRUE)
termplot(model, partial.resid=TRUE, se = TRUE, main = TRUE)
termplot(model, partial.resid=TRUE, smooth=panel.smooth, span.smth=1/4)
if(!had.splines && rs) detach("package:splines")
\end{ExampleCode}
\end{Examples}
\HeaderA{terms}{Model Terms}{terms}
\aliasA{labels.terms}{terms}{labels.terms}
\aliasA{print.terms}{terms}{print.terms}
\methaliasA{terms.aovlist}{terms}{terms.aovlist}
\methaliasA{terms.default}{terms}{terms.default}
\methaliasA{terms.terms}{terms}{terms.terms}
\keyword{models}{terms}
%
\begin{Description}\relax
The function \code{terms} is a generic function
which can be used to extract \emph{terms} objects
from various kinds of \R{} data objects.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
terms(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] object used to select a method to dispatch.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
There are methods for classes \code{"aovlist"}, and \code{"terms"}
\code{"formula"} (see \code{\LinkA{terms.formula}{terms.formula}}):
the default method just extracts the \code{terms} component of the
object, or failing that a \code{"terms"} attribute (as used by
\code{\LinkA{model.frame}{model.frame}}).

There are \code{\LinkA{print}{print}} and \code{\LinkA{labels}{labels}} methods for
class \code{"terms"}: the latter prints the term labels (see
\code{\LinkA{terms.object}{terms.object}}).
\end{Details}
%
\begin{Value}
An object of class \code{c("terms", "formula")} which contains the
\emph{terms} representation of a symbolic model. See
\code{\LinkA{terms.object}{terms.object}} for its structure.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical models.}
Chapter 2 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{terms.object}{terms.object}}, \code{\LinkA{terms.formula}{terms.formula}},
\code{\LinkA{lm}{lm}}, \code{\LinkA{glm}{glm}}, \code{\LinkA{formula}{formula}}.
\end{SeeAlso}
\HeaderA{terms.formula}{Construct a terms Object from a Formula}{terms.formula}
\keyword{models}{terms.formula}
%
\begin{Description}\relax
This function takes a formula and some optional arguments and
constructs a terms object. The terms object can then be used to
construct a \code{\LinkA{model.matrix}{model.matrix}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'formula'
terms(x, specials = NULL, abb = NULL, data = NULL, neg.out = TRUE,
      keep.order = FALSE, simplify = FALSE, ...,
      allowDotAsName = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a formula.
\item[\code{specials}] which functions in the formula should be marked as
special in the \code{terms} object?  A character vector or \code{NULL}.
\item[\code{abb}] Not implemented in \R{}.
\item[\code{data}] a data frame from which the meaning of the special symbol
\code{.} can be inferred. It is unused if there is no \code{.} in
the formula.
\item[\code{neg.out}] Not implemented in \R{}.
\item[\code{keep.order}] a logical value indicating whether the terms should
keep their positions. If \code{FALSE} the terms are reordered so
that main effects come first, followed by the interactions,
all second-order, all third-order and so on.  Effects of a given
order are kept in the order specified.
\item[\code{simplify}] should the formula be expanded and simplified, the
pre-1.7.0 behaviour?
\item[\code{...}] further arguments passed to or from other methods.
\item[\code{allowDotAsName}] normally \code{.} in a formula refers to the
remaining variables contained in \code{data}.  Exceptionally,
\code{.} can be treated as a name for non-standard uses of formulae.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Not all of the options work in the same way that they do in S and not
all are implemented.
\end{Details}
%
\begin{Value}
A \code{\LinkA{terms.object}{terms.object}} object is returned.  The object itself is
the re-ordered (unless \code{keep.order = TRUE}) formula.  In all
cases variables within an interaction term in the formula are
re-ordered by the ordering of the \code{"variables"} attribute, which
is the order in which the variables occur in the formula.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{terms}{terms}}, \code{\LinkA{terms.object}{terms.object}}
\end{SeeAlso}
\HeaderA{terms.object}{Description of Terms Objects}{terms.object}
\keyword{models}{terms.object}
%
\begin{Description}\relax
An object of class \code{\LinkA{terms}{terms}} holds information about a
model.  Usually the model was specified in terms of a
\code{\LinkA{formula}{formula}} and that formula was used to determine the terms
object.
\end{Description}
%
\begin{Value}
The object itself is simply the formula supplied to the call of
\code{\LinkA{terms.formula}{terms.formula}}.  The object has a number of attributes
and they are used to construct the model frame:

\begin{ldescription}
\item[\code{factors}] A matrix of variables by terms showing which variables
appear in which terms.  The entries are 0 if the variable does not
occur in the term, 1 if it does occur and should be coded by
contrasts, and 2 if it occurs and should be coded via dummy
variables for all levels (as when an intercept or lower-order term
is missing).  If there are no terms other than an intercept and offsets,
this is \code{numeric(0)}.
\item[\code{term.labels}] A character vector containing the labels for each
of the terms in the model, except for offsets.  Note that these are
after possible re-ordering of terms.

Non-syntactic names will be quoted by backticks: this makes it
easier to re-construct the formula from the term labels.

\item[\code{variables}] A call to \code{list} of the variables in the model.
\item[\code{intercept}] Either 0, indicating no intercept is to be fit, or 1
indicating that an intercept is to be fit.
\item[\code{order}] A vector of the same length as \code{term.labels}
indicating the order of interaction for each term.
\item[\code{response}] The index of the variable (in variables) of the
response (the left hand side of the formula).  Zero, if there is no
response.
\item[\code{offset}] If the model contains \code{\LinkA{offset}{offset}} terms there
is an \code{offset} attribute indicating which variable(s) are offsets
\item[\code{specials}] If a \code{specials} argument was given to
\code{\LinkA{terms.formula}{terms.formula}} there is a \code{specials} attribute, a
pairlist of vectors (one for each specified special function) giving
numeric indices of the arguments of the list returned as the
\code{variables} attribute which contain these special functions.
\item[\code{dataClasses}] optional.  A named character vector giving the classes
(as given by \code{\LinkA{.MFclass}{.MFclass}}) of the variables used in a fit.

\end{ldescription}
The object has class \code{c("terms", "formula")}.
\end{Value}
%
\begin{Note}\relax
These objects are different from those found in S.  In particular
there is no \code{formula} attribute: instead the object is itself a
formula.  (Thus, the mode of a terms object is different.)

Examples of the \code{specials} argument can be seen in the
\code{\LinkA{aov}{aov}} and \code{\LinkA{coxph}{coxph}} functions, the
latter from package \Rhref{http://CRAN.R-project.org/package=survival}{\pkg{survival}}.
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{terms}{terms}}, \code{\LinkA{formula}{formula}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## use of specials (as used for gam() in packages mgcv and gam)
(tf <- terms(y ~ x + x:z + s(x), specials = "s"))
## Note that the "factors" attribute has variables as row names
## and term labels as column names, both as character vectors.
attr(tf, "specials")    # index 's' variable(s)
rownames(attr(tf, "factors"))[attr(tf, "specials")$s]

## we can keep the order by
terms(y ~ x + x:z + s(x), specials = "s", keep.order = TRUE)
\end{ExampleCode}
\end{Examples}
\HeaderA{time}{Sampling Times of Time Series}{time}
\aliasA{cycle}{time}{cycle}
\aliasA{deltat}{time}{deltat}
\aliasA{frequency}{time}{frequency}
\methaliasA{time.default}{time}{time.default}
\keyword{ts}{time}
%
\begin{Description}\relax
\code{time} creates the vector of times at which a time series was sampled.

\code{cycle} gives the positions in the cycle of each observation.

\code{frequency} returns the number of samples per unit time and
\code{deltat} the time interval between observations (see
\code{\LinkA{ts}{ts}}).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
time(x, ...)
## Default S3 method:
time(x, offset=0, ...)

cycle(x, ...)
frequency(x, ...)
deltat(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a univariate or multivariate time-series, or a vector or matrix.
\item[\code{offset}] can be used to indicate when sampling took place
in the time unit. \code{0} (the default) indicates the start
of the unit, \code{0.5} the middle and \code{1} the end of
the interval.
\item[\code{...}] extra arguments for future methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These are all generic functions, which will use the
\code{\LinkA{tsp}{tsp}} attribute of \code{x} if it exists. \code{time}
and \code{cycle} have methods for class \code{\LinkA{ts}{ts}} that coerce
the result to that class.
\end{Details}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ts}{ts}}, \code{\LinkA{start}{start}}, \code{\LinkA{tsp}{tsp}},
\code{\LinkA{window}{window}}.

\code{\LinkA{date}{date}} for clock time, \code{\LinkA{system.time}{system.time}}
for CPU usage.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

cycle(presidents)
# a simple series plot
plot(as.vector(time(presidents)), as.vector(presidents), type="l")
\end{ExampleCode}
\end{Examples}
\HeaderA{toeplitz}{Form Symmetric Toeplitz Matrix}{toeplitz}
\keyword{ts}{toeplitz}
%
\begin{Description}\relax
Forms a symmetric Toeplitz matrix given its first row.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
toeplitz(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] the first row to form the Toeplitz matrix.
\item[\code{...}] potential further arguments (for methods); none here.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
The Toeplitz matrix.
\end{Value}
%
\begin{Author}\relax
A. Trapletti
\end{Author}
%
\begin{Examples}
\begin{ExampleCode}
x <- 1:5
toeplitz (x)
\end{ExampleCode}
\end{Examples}
\HeaderA{ts}{Time-Series Objects}{ts}
\aliasA{as.ts}{ts}{as.ts}
\methaliasA{as.ts.default}{ts}{as.ts.default}
\aliasA{cbind.ts}{ts}{cbind.ts}
\aliasA{is.mts}{ts}{is.mts}
\aliasA{is.ts}{ts}{is.ts}
\aliasA{Ops.ts}{ts}{Ops.ts}
\aliasA{t.ts}{ts}{t.ts}
\aliasA{[.ts}{ts}{[.ts}
\keyword{ts}{ts}
%
\begin{Description}\relax
The function \code{ts} is used to create time-series objects.

\code{as.ts} and \code{is.ts} coerce an object to a time-series and
test whether an object is a time series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ts(data = NA, start = 1, end = numeric(), frequency = 1,
   deltat = 1, ts.eps = getOption("ts.eps"), class = , names = )
as.ts(x, ...)
is.ts(x)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] a numeric vector or matrix of the observed time-series
values. A data frame will be coerced to a numeric matrix via
\code{data.matrix}.
\item[\code{start}] the time of the first observation.  Either a single
number or a vector of two integers, which specify a natural time
unit and a (1-based) number of samples into the time unit.  See
the examples for the use of the second form.
\item[\code{end}] the time of the last observation, specified in the same way
as \code{start}.
\item[\code{frequency}] the number of observations per unit of time.
\item[\code{deltat}] the fraction of the sampling period between successive
observations; e.g., 1/12 for monthly data.  Only one of
\code{frequency} or \code{deltat} should be provided.
\item[\code{ts.eps}] time series comparison tolerance.  Frequencies are
considered equal if their absolute difference is less than
\code{ts.eps}.
\item[\code{class}] class to be given to the result, or none if \code{NULL}
or \code{"none"}.  The default is \code{"ts"} for a single series,
\code{c("mts", "ts")} for multiple series.
\item[\code{names}] a character vector of names for the series in a multiple
series: defaults to the colnames of \code{data}, or \code{Series 1},
\code{Series 2}, \dots.
\item[\code{x}] an arbitrary \R{} object.
\item[\code{...}] arguments passed to methods (unused for the default method).
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The function \code{ts} is used to create time-series objects.  These
are vector or matrices with class of \code{"ts"} (and additional
attributes) which represent data which has been sampled at equispaced
points in time.  In the matrix case, each column of the matrix
\code{data} is assumed to contain a single (univariate) time series.
Time series must have at least one observation, and although they need
not be numeric there is very limited support for non-numeric series.

Class \code{"ts"} has a number of methods.  In particular arithmetic
will attempt to align time axes, and subsetting to extract subsets of
series can be used (e.g., \code{EuStockMarkets[, "DAX"]}).  However,
subsetting the first (or only) dimension will return a matrix or
vector, as will matrix subsetting.  Subassignment can be used to
replace values but not to extend a series (see \code{\LinkA{window}{window}}).
There is a method for \code{\LinkA{t}{t}} that transposes the series as a
matrix (a one-column matrix if a vector) and hence returns a result
that does not inherit from class \code{"ts"}.

The value of argument \code{frequency} is used when the series is
sampled an integral number of times in each unit time interval.  For
example, one could use a value of \code{7} for \code{frequency} when
the data are sampled daily, and the natural time period is a week, or
\code{12} when the data are sampled monthly and the natural time
period is a year.  Values of \code{4} and \code{12} are assumed in
(e.g.) \code{print} methods to imply a quarterly and monthly series
respectively.

\code{as.ts} is generic.  Its default method will use the
\code{\LinkA{tsp}{tsp}} attribute of the object if it has one to set the
start and end times and frequency.

\code{is.ts} tests if an object is a time series.  It is generic: you
can write methods to handle specific classes of objects,
see \LinkA{InternalMethods}{InternalMethods}.
\end{Details}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{tsp}{tsp}},
\code{\LinkA{frequency}{frequency}},
\code{\LinkA{start}{start}},
\code{\LinkA{end}{end}},
\code{\LinkA{time}{time}},
\code{\LinkA{window}{window}};
\code{\LinkA{print.ts}{print.ts}}, the print method for time series objects;
\code{\LinkA{plot.ts}{plot.ts}}, the plot method for time series objects.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

ts(1:10, frequency = 4, start = c(1959, 2)) # 2nd Quarter of 1959
print( ts(1:10, frequency = 7, start = c(12, 2)), calendar = TRUE)
# print.ts(.)
## Using July 1954 as start date:
gnp <- ts(cumsum(1 + round(rnorm(100), 2)),
          start = c(1954, 7), frequency = 12)
plot(gnp) # using 'plot.ts' for time-series plot

## Multivariate
z <- ts(matrix(rnorm(300), 100, 3), start=c(1961, 1), frequency=12)
class(z)
plot(z)
plot(z, plot.type="single", lty=1:3)

## A phase plot:
plot(nhtemp, c(nhtemp[-1], NA), cex = .8, col="blue",
     main = "Lag plot of New Haven temperatures")
## a clearer way to do this would be
## Not run: 
plot(nhtemp, lag(nhtemp, 1), cex = .8, col="blue",
     main = "Lag plot of New Haven temperatures")

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{ts-methods}{Methods for Time Series Objects}{ts.Rdash.methods}
\aliasA{diff.ts}{ts-methods}{diff.ts}
\aliasA{na.omit.ts}{ts-methods}{na.omit.ts}
\keyword{ts}{ts-methods}
%
\begin{Description}\relax
Methods for objects of class \code{"ts"}, typically the result of
\code{\LinkA{ts}{ts}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'ts'
diff(x, lag = 1, differences = 1, ...)

## S3 method for class 'ts'
na.omit(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object of class \code{"ts"} containing the values to be
differenced.
\item[\code{lag}] an integer indicating which lag to use.
\item[\code{differences}] an integer indicating the order of the difference.
\item[\code{object}] a univariate or multivariate time series.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \code{na.omit} method omits initial and final segments with
missing values in one or more of the series.  `Internal'
missing values will lead to failure.
\end{Details}
%
\begin{Value}
For the \code{na.omit} method, a time series without missing values.
The class of \code{object} will be preserved.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{diff}{diff}};
\code{\LinkA{na.omit}{na.omit}}, \code{\LinkA{na.fail}{na.fail}},
\code{\LinkA{na.contiguous}{na.contiguous}}.
\end{SeeAlso}
\HeaderA{ts.plot}{Plot Multiple Time Series}{ts.plot}
\keyword{ts}{ts.plot}
%
\begin{Description}\relax
Plot several time series on a common plot. Unlike
\code{\LinkA{plot.ts}{plot.ts}} the series can have a different time bases,
but they should have the same frequency.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ts.plot(..., gpars = list())
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{...}] one or more univariate or multivariate time series.
\item[\code{gpars}] list of named graphics parameters to be passed to the
plotting functions.  Those commonly used can be supplied directly in
\code{...}.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
None.
\end{Value}
%
\begin{Note}\relax
Although this can be used for a single time series, \code{plot} is
easier to use and is preferred.
\end{Note}
%
\begin{SeeAlso}\relax
\code{\LinkA{plot.ts}{plot.ts}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

ts.plot(ldeaths, mdeaths, fdeaths,
        gpars=list(xlab="year", ylab="deaths", lty=c(1:3)))
\end{ExampleCode}
\end{Examples}
\HeaderA{ts.union}{Bind Two or More Time Series}{ts.union}
\aliasA{ts.intersect}{ts.union}{ts.intersect}
\keyword{ts}{ts.union}
%
\begin{Description}\relax
Bind time series which have a common frequency. \code{ts.union} pads
with \code{NA}s to the total time coverage, \code{ts.intersect}
restricts to the time covered by all the series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ts.intersect(..., dframe = FALSE)
ts.union(..., dframe = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{...}] two or more univariate or multivariate time series, or
objects which can coerced to time series.
\item[\code{dframe}] logical; if \code{TRUE} return the result as a data
frame.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
As a special case, \code{...} can contain vectors or matrices of the
same length as the combined time series of the time series present, as
well as those of a single row.
\end{Details}
%
\begin{Value}
A time series object if \code{dframe} is \code{FALSE}, otherwise a
data frame.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{cbind}{cbind}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
ts.union(mdeaths, fdeaths)
cbind(mdeaths, fdeaths) # same as the previous line
ts.intersect(window(mdeaths, 1976), window(fdeaths, 1974, 1978))

sales1 <- ts.union(BJsales, lead = BJsales.lead)
ts.intersect(sales1, lead3 = lag(BJsales.lead, -3))
\end{ExampleCode}
\end{Examples}
\HeaderA{tsdiag}{Diagnostic Plots for Time-Series Fits}{tsdiag}
\methaliasA{tsdiag.Arima}{tsdiag}{tsdiag.Arima}
\methaliasA{tsdiag.arima0}{tsdiag}{tsdiag.arima0}
\methaliasA{tsdiag.StructTS}{tsdiag}{tsdiag.StructTS}
\keyword{ts}{tsdiag}
%
\begin{Description}\relax
A generic function to plot time-series diagnostics.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tsdiag(object, gof.lag, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a fitted time-series model
\item[\code{gof.lag}] the maximum number of lags for a Portmanteau
goodness-of-fit test
\item[\code{...}] further arguments to be passed to particular methods
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function. It will generally plot the residuals,
often standardized, the autocorrelation function of the residuals, and
the p-values of a Portmanteau test for all lags up to \code{gof.lag}.

The methods for \code{\LinkA{arima}{arima}} and \code{\LinkA{StructTS}{StructTS}} objects
plots residuals scaled by the estimate of their (individual) variance,
and use the Ljung--Box version of the portmanteau test.
\end{Details}
%
\begin{Value}
None. Diagnostics are plotted.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{arima}{arima}}, \code{\LinkA{StructTS}{StructTS}}, \code{\LinkA{Box.test}{Box.test}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: require(graphics)

fit <- arima(lh, c(1,0,0))
tsdiag(fit)

## see also examples(arima)

(fit <- StructTS(log10(JohnsonJohnson), type="BSM"))
tsdiag(fit)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{tsp}{Tsp Attribute of Time-Series-like Objects}{tsp}
\aliasA{hasTsp}{tsp}{hasTsp}
\aliasA{tsp<\Rdash}{tsp}{tsp<.Rdash.}
\keyword{ts}{tsp}
%
\begin{Description}\relax
\code{tsp} returns the \code{tsp} attribute (or \code{NULL}).
It is included for compatibility with S version 2. \code{tsp<-}
sets the \code{tsp} attribute. \code{hasTsp} ensures \code{x} has a
\code{tsp} attribute, by adding one if needed.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tsp(x)
tsp(x) <- value
hasTsp(x)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a vector or matrix or univariate or multivariate time-series.
\item[\code{value}] a numeric vector of length 3 or \code{NULL}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The \code{tsp} attribute was previously described here as
\code{c(start(x), end(x), frequency(x))}, but this is incorrect.
It gives the start time \emph{in time units}, the end time and
the frequency.

Assignments are checked for consistency.    

Assigning \code{NULL} which removes the \code{tsp} attribute
\emph{and} any \code{"ts"} (or \code{"mts"}) class of \code{x}.
\end{Details}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{ts}{ts}}, \code{\LinkA{time}{time}}, \code{\LinkA{start}{start}}.
\end{SeeAlso}
\HeaderA{tsSmooth}{Use Fixed-Interval Smoothing on Time Series}{tsSmooth}
\methaliasA{tsSmooth.StructTS}{tsSmooth}{tsSmooth.StructTS}
\keyword{ts}{tsSmooth}
%
\begin{Description}\relax
Performs fixed-interval smoothing on a univariate time series via a
state-space model.  Fixed-interval smoothing gives the best estimate
of the state at each time point based on the whole observed series.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tsSmooth(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] a time-series fit.  Currently only class
\code{"\LinkA{StructTS}{StructTS}"} is supported
\item[\code{...}] possible arguments for future methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A time series, with as many dimensions as the state space and results
at each time point of the original series.  (For seasonal models, only
the current seasonal component is returned.)
\end{Value}
%
\begin{Author}\relax
 B. D. Ripley 
\end{Author}
%
\begin{References}\relax
Durbin, J. and Koopman, S. J. (2001) \emph{Time Series Analysis by
State Space Methods.}  Oxford University Press.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{KalmanSmooth}{KalmanSmooth}}, \code{\LinkA{StructTS}{StructTS}}.

For examples consult \code{\LinkA{AirPassengers}{AirPassengers}},
\code{\LinkA{JohnsonJohnson}{JohnsonJohnson}} and \code{\LinkA{Nile}{Nile}}.
\end{SeeAlso}
\HeaderA{Tukey}{The Studentized Range Distribution}{Tukey}
\aliasA{ptukey}{Tukey}{ptukey}
\aliasA{qtukey}{Tukey}{qtukey}
\keyword{distribution}{Tukey}
%
\begin{Description}\relax
Functions of the distribution of the studentized range, \eqn{R/s}{},
where \eqn{R}{} is the range of a standard normal sample and
\eqn{df \times s^2}{} is independently distributed as
chi-squared with \eqn{df}{} degrees of freedom, see \code{\LinkA{pchisq}{pchisq}}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ptukey(q, nmeans, df, nranges = 1, lower.tail = TRUE, log.p = FALSE)
qtukey(p, nmeans, df, nranges = 1, lower.tail = TRUE, log.p = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{nmeans}] sample size for range (same for each group).
\item[\code{df}] degrees of freedom for \eqn{s}{} (see below).
\item[\code{nranges}] number of \emph{groups} whose \bold{maximum} range is
considered.
\item[\code{log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \eqn{n_g =}{}\code{nranges} is greater than one, \eqn{R}{} is
the \emph{maximum} of \eqn{n_g}{} groups of \code{nmeans}
observations each.
\end{Details}
%
\begin{Value}
\code{ptukey} gives the distribution function and \code{qtukey} its
inverse, the quantile function.
\end{Value}
%
\begin{Note}\relax
A Legendre 16-point formula is used for the integral of \code{ptukey}.
The computations are relatively expensive, especially for
\code{qtukey} which uses a simple secant method for finding the
inverse of \code{ptukey}.
\code{qtukey} will be accurate to the 4th decimal place.
\end{Note}
%
\begin{Source}\relax
\code{qtukey} is in part adapted from Odeh and Evans (1974).
\end{Source}
%
\begin{References}\relax
Copenhaver, Margaret Diponzio and Holland, Burt S. (1988)
Multiple comparisons of simple effects in
the two-way analysis of variance with fixed effects.
\emph{Journal of Statistical Computation and Simulation}, \bold{30}, 1--15.

Odeh, R. E.  and  Evans, J. O. (1974)
Algorithm AS 70: Percentage Points of the Normal Distribution.
\emph{Applied Statistics} \bold{23}, 96--97. 
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for standard distributions, including
\code{\LinkA{pnorm}{pnorm}} and \code{\LinkA{qnorm}{qnorm}} for the corresponding
functions for the normal distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
if(interactive())
  curve(ptukey(x, nm=6, df=5), from=-1, to=8, n=101)
(ptt <- ptukey(0:10, 2, df= 5))
(qtt <- qtukey(.95, 2, df= 2:11))
## The precision may be not much more than about 8 digits:
summary(abs(.95 - ptukey(qtt,2, df = 2:11)))
\end{ExampleCode}
\end{Examples}
\HeaderA{TukeyHSD}{Compute Tukey Honest Significant Differences}{TukeyHSD}
\aliasA{plot.TukeyHSD}{TukeyHSD}{plot.TukeyHSD}
\aliasA{print.TukeyHSD}{TukeyHSD}{print.TukeyHSD}
\methaliasA{TukeyHSD.aov}{TukeyHSD}{TukeyHSD.aov}
\keyword{models}{TukeyHSD}
\keyword{design}{TukeyHSD}
%
\begin{Description}\relax
Create a set of confidence intervals on the differences between the
means of the levels of a factor with the specified family-wise
probability of coverage.  The intervals are based on the Studentized
range statistic, Tukey's `Honest Significant Difference'
method.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
TukeyHSD(x, which, ordered = FALSE, conf.level = 0.95, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A fitted model object, usually an \code{\LinkA{aov}{aov}} fit.
\item[\code{which}] A character vector listing terms in the fitted model for
which the intervals should be calculated.  Defaults to all the
terms.
\item[\code{ordered}] A logical value indicating if the levels of the factor
should be ordered according to increasing average in the sample
before taking differences.  If \code{ordered} is true then
the calculated differences in the means will all be positive.  The
significant differences will be those for which the \code{lwr} end
point is positive.
\item[\code{conf.level}] A numeric value between zero and one giving the
family-wise confidence level to use.
\item[\code{...}] Optional additional arguments.  None are used at present.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
When comparing the means for the levels of a factor in an analysis of
variance, a simple comparison using t-tests will inflate the
probability of declaring a significant difference when it is not in
fact present.  This because the intervals are calculated with a
given coverage probability for each interval but the interpretation of
the coverage is usually with respect to the entire family of
intervals.

John Tukey introduced intervals based on the range of the
sample means rather than the individual differences.  The intervals
returned by this function are based on this Studentized range
statistics.

The intervals constructed in this way would only apply exactly to
balanced designs where there are the same number of observations made
at each level of the factor.  This function incorporates an adjustment
for sample size that produces sensible intervals for mildly unbalanced
designs.

If \code{which} specifies non-factor terms these will be dropped with
a warning: if no terms are left this is a an error.
\end{Details}
%
\begin{Value}
A list of class \code{c("multicomp", "TukeyHSD")},
with one component for each term requested in \code{which}.
Each component is a matrix with columns \code{diff} giving the
difference in the observed means, \code{lwr} giving the lower
end point of the interval, \code{upr} giving the upper end point
and \code{p adj} giving the p-value after adjustment for the multiple
comparisons.

There are \code{print} and \code{plot} methods for class
\code{"TukeyHSD"}.  The \code{plot} method does not accept
\code{xlab}, \code{ylab} or \code{main} arguments and creates its own
values for each plot.
\end{Value}
%
\begin{Author}\relax
Douglas Bates
\end{Author}
%
\begin{References}\relax
Miller, R. G. (1981)
\emph{Simultaneous Statistical Inference}. Springer.

Yandell, B. S. (1997)
\emph{Practical Data Analysis for Designed Experiments}.
Chapman \& Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{aov}{aov}}, \code{\LinkA{qtukey}{qtukey}}, \code{\LinkA{model.tables}{model.tables}},
\code{\LinkA{glht}{glht}} in package \Rhref{http://CRAN.R-project.org/package=multcomp}{\pkg{multcomp}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

summary(fm1 <- aov(breaks ~ wool + tension, data = warpbreaks))
TukeyHSD(fm1, "tension", ordered = TRUE)
plot(TukeyHSD(fm1, "tension"))
\end{ExampleCode}
\end{Examples}
\HeaderA{Uniform}{The Uniform Distribution}{Uniform}
\aliasA{dunif}{Uniform}{dunif}
\aliasA{punif}{Uniform}{punif}
\aliasA{qunif}{Uniform}{qunif}
\aliasA{runif}{Uniform}{runif}
\keyword{distribution}{Uniform}
%
\begin{Description}\relax
These functions provide information about the uniform distribution
on the interval from \code{min} to \code{max}.  \code{dunif} gives the
density, \code{punif} gives the distribution function \code{qunif}
gives the quantile function and \code{runif} generates random
deviates.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dunif(x, min=0, max=1, log = FALSE)
punif(q, min=0, max=1, lower.tail = TRUE, log.p = FALSE)
qunif(p, min=0, max=1, lower.tail = TRUE, log.p = FALSE)
runif(n, min=0, max=1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x,q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{min,max}] lower and upper limits of the distribution.  Must be finite.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
If \code{min} or \code{max} are not specified they assume the default
values of \code{0} and \code{1} respectively.

The uniform distribution has density
\deqn{f(x) = \frac{1}{max-min}}{}
for \eqn{min \le x \le max}{}.

For the case of \eqn{u := min == max}{}, the limit case of
\eqn{X \equiv u}{} is assumed, although there is no density in
that case and \code{dunif} will return \code{NaN} (the error condition).

\code{runif} will not generate either of the extreme values unless
\code{max = min} or \code{max-min} is small compared to \code{min},
and in particular not for the default arguments.
\end{Details}
%
\begin{Note}\relax
The characteristics of output from pseudo-random number generators
(such as precision and periodicity) vary widely.  See
\code{\LinkA{.Random.seed}{.Random.seed}} for more information on \R{}'s random number
generation algorithms.
\end{Note}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{RNG}{RNG}} about random number generation in \R{}.

\LinkA{Distributions}{Distributions} for other standard distributions.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
u <- runif(20)

## The following relations always hold :
punif(u) == u
dunif(u) == 1

var(runif(10000))#- ~ = 1/12 = .08333
\end{ExampleCode}
\end{Examples}
\HeaderA{uniroot}{One Dimensional Root (Zero) Finding}{uniroot}
\keyword{optimize}{uniroot}
%
\begin{Description}\relax
The function \code{uniroot} searches the interval from \code{lower}
to \code{upper} for a root (i.e., zero) of the function \code{f} with
respect to its first argument.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
uniroot(f, interval, ...,
        lower = min(interval), upper = max(interval),
        f.lower = f(lower, ...), f.upper = f(upper, ...),
        tol = .Machine$double.eps^0.25, maxiter = 1000)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{f}] the function for which the root is sought.
\item[\code{interval}] a vector containing the end-points of the interval
to be searched for the root.
\item[\code{...}] additional named or unnamed arguments to be passed
to \code{f}
\item[\code{lower, upper}] the lower and upper end points of the interval to
be searched.
\item[\code{f.lower, f.upper}] the same as \code{f(upper)} and
\code{f(lower)}, respectively.  Passing these values from the caller
where they are often known is more economical as soon as \code{f()}
contains non-trivial computations.
\item[\code{tol}] the desired accuracy (convergence tolerance).
\item[\code{maxiter}] the maximum number of iterations.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Note that arguments after \code{...} must be matched exactly.

Either \code{interval} or both \code{lower} and \code{upper} must be
specified: the upper endpoint must be strictly larger than the lower
endpoint.  The function values at the endpoints must be of opposite
signs (or zero).

The function uses Fortran subroutine \file{"zeroin"} (from Netlib)
based on algorithms given in the reference below.  They assume a
continuous function (which then is known to have at least one root in
the interval).

Convergence is declared either if \code{f(x) == 0} or the change in
\code{x} for one step of the algorithm is less than \code{tol} (plus an
allowance for representation error in \code{x}).

If the algorithm does not converge in \code{maxiter} steps, a warning
is printed and the current approximation is returned.

\code{f} will be called as \code{f(\var{x}, ...)} for a numeric value
of \var{x}.

The argument passed to \code{f} has special semantics and is shared
between calls.  The function should not copy it.
\end{Details}
%
\begin{Value}
A list with four components: \code{root} and \code{f.root} give the
location of the root and the value of the function evaluated at that
point. \code{iter} and \code{estim.prec} give the number of iterations
used and an approximate estimated precision for \code{root}.  (If the
root occurs at one of the endpoints, the estimated precision is
\code{NA}.)
\end{Value}
%
\begin{Source}\relax
Based on \file{zeroin.c} in \url{http://www.netlib.org/c/brent.shar}.
\end{Source}
%
\begin{References}\relax
Brent, R. (1973)
\emph{Algorithms for Minimization without Derivatives.}
Englewood Cliffs, NJ: Prentice-Hall.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{polyroot}{polyroot}} for all complex roots of a polynomial;
\code{\LinkA{optimize}{optimize}}, \code{\LinkA{nlm}{nlm}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}

require(utils) # for str

## some platforms hit zero exactly on the first step:
## if so the estimated precision is 2/3.
f <- function (x, a) x - a
str(xmin <- uniroot(f, c(0, 1), tol = 0.0001, a = 1/3))

## handheld calculator example: fixed point of cos(.):
uniroot(function(x) cos(x) - x, lower = -pi, upper = pi, tol = 1e-9)$root

str(uniroot(function(x) x*(x^2-1) + .5, lower = -2, upper = 2,
            tol = 0.0001))
str(uniroot(function(x) x*(x^2-1) + .5, lower = -2, upper = 2,
            tol = 1e-10))

## Find the smallest value x for which exp(x) > 0 (numerically):
r <- uniroot(function(x) 1e80*exp(x) - 1e-300, c(-1000, 0), tol = 1e-15)
str(r, digits.d = 15) # around -745, depending on the platform.

exp(r$root)     # = 0, but not for r$root * 0.999...
minexp <- r$root * (1 - 10*.Machine$double.eps)
exp(minexp)     # typically denormalized
\end{ExampleCode}
\end{Examples}
\HeaderA{update}{Update and Re-fit a Model Call}{update}
\aliasA{getCall}{update}{getCall}
\methaliasA{getCall.default}{update}{getCall.default}
\methaliasA{update.default}{update}{update.default}
\keyword{models}{update}
%
\begin{Description}\relax
\code{update} will update and (by default) re-fit a model.  It does this
by extracting the call stored in the object, updating the call and (by
default) evaluating that call.  Sometimes it is useful to call
\code{update} with only one argument, for example if the data frame has
been corrected.

``Extracting the call'' in \code{update()} and similar functions
uses \code{getCall()} which itself is a (S3) generic function with a
default method that simply gets \code{x\$call}.

Because of this, \code{update()} will often work (via its default
method) on new model classes, either automatically, or by providing a
simple \code{getCall()} method for that class.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
update(object, ...)
## Default S3 method:
update(object, formula., ..., evaluate = TRUE)

getCall(x, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object, x}] An existing fit from a model function such as \code{lm},
\code{glm} and many others.
\item[\code{formula.}] Changes to the formula -- see \code{update.formula} for
details.
\item[\code{...}] Additional arguments to the call, or arguments with
changed values. Use \code{name=NULL} to remove the argument \code{name}.
\item[\code{evaluate}] If true evaluate the new call else return the call.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
If \code{evaluate = TRUE} the fitted object, otherwise the updated call.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. (1992)
\emph{Linear models.}
Chapter 4 of \emph{Statistical Models in S}
eds J. M. Chambers and T. J. Hastie, Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{update.formula}{update.formula}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
oldcon <- options(contrasts = c("contr.treatment", "contr.poly"))
## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl", "Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D9
summary(lm.D90 <- update(lm.D9, . ~ . - 1))
options(contrasts = c("contr.helmert", "contr.poly"))
update(lm.D9)
getCall(lm.D90)# "through the origin"

options(oldcon)
\end{ExampleCode}
\end{Examples}
\HeaderA{update.formula}{Model Updating}{update.formula}
\keyword{models}{update.formula}
%
\begin{Description}\relax
\code{update.formula} is used to update model formulae.
This typically involves adding or dropping terms,
but updates can be more general.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
## S3 method for class 'formula'
update(old, new, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{old}] a model formula to be updated.
\item[\code{new}] a formula giving a template which specifies how to update.
\item[\code{...}] further arguments passed to or from other methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Either or both of \code{old} and \code{new} can be objects such as
length-one character vectors which can be coerced to a formula via
\code{\LinkA{as.formula}{as.formula}}.

The function works by first identifying the \emph{left-hand side}
and \emph{right-hand side} of the \code{old} formula.
It then examines the \code{new} formula and substitutes
the \emph{lhs} of the \code{old} formula for any occurrence
of `.' on the left of \code{new}, and substitutes
the \emph{rhs} of the \code{old} formula for any occurrence
of `.' on the right of \code{new}.  The result is then
simplified \emph{via} \code{\LinkA{terms.formula}{terms.formula}(simplify = TRUE)}.
\end{Details}
%
\begin{Value}
The updated formula is returned.  The environment of the result is
that of \code{old}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{terms}{terms}}, \code{\LinkA{model.matrix}{model.matrix}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
update(y ~ x,    ~ . + x2) #> y ~ x + x2
update(y ~ x, log(.) ~ . ) #> log(y) ~ x
\end{ExampleCode}
\end{Examples}
\HeaderA{var.test}{F Test to Compare Two Variances}{var.test}
\methaliasA{var.test.default}{var.test}{var.test.default}
\methaliasA{var.test.formula}{var.test}{var.test.formula}
\keyword{htest}{var.test}
%
\begin{Description}\relax
Performs an F test to compare the variances of two samples from normal
populations.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
var.test(x, ...)

## Default S3 method:
var.test(x, y, ratio = 1,
         alternative = c("two.sided", "less", "greater"),
         conf.level = 0.95, ...)

## S3 method for class 'formula'
var.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, y}] numeric vectors of data values, or fitted linear model
objects (inheriting from class \code{"lm"}).
\item[\code{ratio}] the hypothesized ratio of the population variances of
\code{x} and \code{y}.
\item[\code{alternative}] a character string specifying the alternative
hypothesis, must be one of \code{"two.sided"} (default),
\code{"greater"} or \code{"less"}.  You can specify just the initial
letter.
\item[\code{conf.level}] confidence level for the returned confidence
interval.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
is a numeric variable giving the data values and \code{rhs} a factor
with two levels giving the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods. 
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The null hypothesis is that the ratio of the variances of the
populations from which \code{x} and \code{y} were drawn, or in the
data to which the linear models \code{x} and \code{y} were fitted, is
equal to \code{ratio}.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the F test statistic.
\item[\code{parameter}] the degrees of the freedom of the F distribution of
the test statistic.
\item[\code{p.value}] the p-value of the test.
\item[\code{conf.int}] a confidence interval for the ratio of the population
variances.
\item[\code{estimate}] the ratio of the sample variances of \code{x} and
\code{y}.
\item[\code{null.value}] the ratio of population variances under the null.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] the character string
\code{"F test to compare two variances"}.
\item[\code{data.name}] a character string giving the names of the data.
\end{ldescription}
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{bartlett.test}{bartlett.test}} for testing homogeneity of variances in
more than two samples from normal distributions;
\code{\LinkA{ansari.test}{ansari.test}} and \code{\LinkA{mood.test}{mood.test}} for two rank
based (nonparametric) two-sample tests for difference in scale.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- rnorm(50, mean = 0, sd = 2)
y <- rnorm(30, mean = 1, sd = 1)
var.test(x, y)                  # Do x and y have the same variance?
var.test(lm(x ~ 1), lm(y ~ 1))  # The same.
\end{ExampleCode}
\end{Examples}
\HeaderA{varimax}{Rotation Methods for Factor Analysis}{varimax}
\aliasA{promax}{varimax}{promax}
\keyword{multivariate}{varimax}
%
\begin{Description}\relax
These functions `rotate' loading matrices in factor analysis.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
varimax(x, normalize = TRUE, eps = 1e-5)
promax(x, m = 4)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A loadings matrix, with \eqn{p}{} rows and \eqn{k < p}{} columns
\item[\code{m}] The power used the target for \code{promax}.  Values of 2 to
4 are recommended.
\item[\code{normalize}] logical. Should Kaiser normalization be performed?
If so the rows of \code{x} are re-scaled to unit length before
rotation, and scaled back afterwards.
\item[\code{eps}] The tolerance for stopping: the relative change in the sum
of singular values.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
These seek a `rotation' of the factors \code{x \%*\% T} that
aims to clarify the structure of the loadings matrix.  The matrix
\code{T} is a rotation (possibly with reflection) for \code{varimax},
but a general linear transformation for \code{promax}, with the
variance of the factors being preserved.
\end{Details}
%
\begin{Value}
A list with components
\begin{ldescription}
\item[\code{loadings}] The `rotated' loadings matrix,
\code{x \%*\% rotmat}, of class \code{"loadings"}.
\item[\code{rotmat}] The `rotation' matrix.
\end{ldescription}
\end{Value}
%
\begin{References}\relax
Hendrickson, A. E. and White, P. O. (1964) Promax: a quick method for
rotation to orthogonal oblique structure. \emph{British Journal of
Statistical Psychology}, \bold{17}, 65--70.

Horst, P. (1965) \emph{Factor Analysis of Data Matrices.} Holt,
Rinehart and Winston.  Chapter 10.

Kaiser, H. F. (1958) The varimax criterion for analytic rotation in
factor analysis. \emph{Psychometrika} \bold{23}, 187--200.

Lawley, D. N. and Maxwell, A. E. (1971) \emph{Factor Analysis as a
Statistical Method}. Second edition. Butterworths.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{factanal}{factanal}}, \code{\LinkA{Harman74.cor}{Harman74.cor}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## varimax with normalize = TRUE is the default
fa <- factanal( ~., 2, data = swiss)
varimax(loadings(fa), normalize = FALSE)
promax(loadings(fa))
\end{ExampleCode}
\end{Examples}
\HeaderA{vcov}{Calculate Variance-Covariance Matrix for a Fitted Model Object}{vcov}
\methaliasA{vcov.glm}{vcov}{vcov.glm}
\methaliasA{vcov.gls}{vcov}{vcov.gls}
\methaliasA{vcov.lm}{vcov}{vcov.lm}
\methaliasA{vcov.lme}{vcov}{vcov.lme}
\methaliasA{vcov.summary.glm}{vcov}{vcov.summary.glm}
\methaliasA{vcov.summary.lm}{vcov}{vcov.summary.lm}
\keyword{models}{vcov}
\keyword{nonlinear}{vcov}
%
\begin{Description}\relax
Returns the variance-covariance matrix of the main parameters of
a fitted model object.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
vcov(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] 
a fitted model object, typically.  Sometimes also a
\code{\LinkA{summary}{summary}()} object of such a fitted model.

\item[\code{...}] 
additional arguments for method functions.  For the
\code{\LinkA{glm}{glm}} method this can be used to pass a
\code{dispersion} parameter.

\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function.
Functions with names beginning in \code{vcov.} will be
methods for this function.
Classes with methods for this function include:
\code{lm}, \code{mlm}, \code{glm}, \code{nls},
\code{summary.lm}, \code{summary.glm},
\code{negbin}, \code{polr}, \code{rlm} (in package \Rhref{http://CRAN.R-project.org/package=MASS}{\pkg{MASS}}),
\code{multinom} (in package \Rhref{http://CRAN.R-project.org/package=nnet}{\pkg{nnet}})
\code{gls}, \code{lme} (in package \Rhref{http://CRAN.R-project.org/package=nlme}{\pkg{nlme}}),
\code{coxph} and \code{survreg} (in package \Rhref{http://CRAN.R-project.org/package=survival}{\pkg{survival}}).

(\code{vcov()} methods for summary objects allow more
efficient and still encapsulated access when both
\code{summary(mod)} and \code{vcov(mod)} are needed.)
\end{Details}
%
\begin{Value}
A matrix of the estimated covariances between the parameter estimates
in the linear or non-linear predictor of the model.  This should have
row and column names corresponding to the parameter names given by the
\code{\LinkA{coef}{coef}} method.
\end{Value}
\HeaderA{Weibull}{The Weibull Distribution}{Weibull}
\aliasA{dweibull}{Weibull}{dweibull}
\aliasA{pweibull}{Weibull}{pweibull}
\aliasA{qweibull}{Weibull}{qweibull}
\aliasA{rweibull}{Weibull}{rweibull}
\keyword{distribution}{Weibull}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the Weibull distribution with parameters \code{shape}
and \code{scale}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dweibull(x, shape, scale = 1, log = FALSE)
pweibull(q, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
qweibull(p, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
rweibull(n, shape, scale = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{n}] number of observations. If \code{length(n) > 1}, the length
is taken to be the number required.
\item[\code{shape, scale}] shape and scale parameters, the latter defaulting to 1.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The Weibull distribution with \code{shape} parameter \eqn{a}{} and
\code{scale} parameter \eqn{\sigma}{} has density given by
\deqn{f(x) = (a/\sigma) {(x/\sigma)}^{a-1} \exp (-{(x/\sigma)}^{a})}{} for \eqn{x > 0}{}.
The cumulative distribution function is
\eqn{F(x) = 1 - \exp(-{(x/\sigma)}^a)}{}
on \eqn{x > 0}{}, the
mean is \eqn{E(X) = \sigma \Gamma(1 + 1/a)}{}, and
the \eqn{Var(X) = \sigma^2(\Gamma(1 + 2/a)-(\Gamma(1 + 1/a))^2)}{}.
\end{Details}
%
\begin{Value}
\code{dweibull} gives the density,
\code{pweibull} gives the distribution function,
\code{qweibull} gives the quantile function, and
\code{rweibull} generates random deviates.

Invalid arguments will result in return value \code{NaN}, with a warning.
\end{Value}
%
\begin{Note}\relax
The cumulative hazard \eqn{H(t) = - \log(1 - F(t))}{}
is \code{-pweibull(t, a, b, lower = FALSE, log = TRUE)} which is just
\eqn{H(t) = {(t/b)}^a}{}.
\end{Note}
%
\begin{Source}\relax
\code{[dpq]weibull} are calculated directly from the definitions.
\code{rweibull} uses inversion.
\end{Source}
%
\begin{References}\relax
Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995)
\emph{Continuous Univariate Distributions}, volume 1, chapter 21.
Wiley, New York.
\end{References}
%
\begin{SeeAlso}\relax
\LinkA{Distributions}{Distributions} for other standard distributions, including
the \LinkA{Exponential}{Exponential} which is a special case of the Weibull distribution.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
x <- c(0,rlnorm(50))
all.equal(dweibull(x, shape = 1), dexp(x))
all.equal(pweibull(x, shape = 1, scale = pi), pexp(x, rate = 1/pi))
## Cumulative hazard H():
all.equal(pweibull(x, 2.5, pi, lower.tail=FALSE, log.p=TRUE), -(x/pi)^2.5,
          tol = 1e-15)
all.equal(qweibull(x/11, shape = 1, scale = pi), qexp(x/11, rate = 1/pi))
\end{ExampleCode}
\end{Examples}
\HeaderA{weighted.mean}{Weighted Arithmetic Mean}{weighted.mean}
\methaliasA{weighted.mean.default}{weighted.mean}{weighted.mean.default}
\keyword{univar}{weighted.mean}
%
\begin{Description}\relax
Compute a weighted mean.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
weighted.mean(x, w, ...)

## Default S3 method:
weighted.mean(x, w, ..., na.rm = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] an object containing the values whose weighted mean is to be
computed.
\item[\code{w}] a numerical vector of weights the same length as \code{x} giving
the weights to use for elements of \code{x}.
\item[\code{...}] arguments to be passed to or from methods.
\item[\code{na.rm}] a logical value indicating whether \code{NA}
values in \code{x} should be stripped before the computation proceeds.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This is a generic function and methods can be defined for the first
argument \code{x}: apart from the default methods there are methods
for the date-time classes \code{"POSIXct"}, \code{"POSIXlt"},
\code{"difftime"} and \code{"Date"}.  The default method will work for
any numeric-like object for which \code{[}, multiplication, division
and \code{\LinkA{sum}{sum}} have suitable methods, including complex vectors.

If \code{w} is missing then all elements of \code{x} are given the
same weight, otherwise the weights coerced to numeric by
\code{\LinkA{as.numeric}{as.numeric}} and normalized to sum to one (if possible: if
their sum is zero or infinite the value is likely to be \code{NaN}).

Missing values in \code{w} are not handled specially and so give a
missing value as the result.  However, as from \R{} 2.11.0 zero weights
\emph{are} handled specially and the corresponding \code{x} values are
omitted from the sum.
\end{Details}
%
\begin{Value}
For the default method, a length-one numeric vector.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{mean}{mean}}
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## GPA from Siegel 1994
wt <- c(5,  5,  4,  1)/15
x <- c(3.7,3.3,3.5,2.8)
xm <- weighted.mean(x, wt)
\end{ExampleCode}
\end{Examples}
\HeaderA{weighted.residuals}{Compute Weighted Residuals}{weighted.residuals}
\keyword{regression}{weighted.residuals}
%
\begin{Description}\relax
Computed weighted residuals from a linear model fit.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
weighted.residuals(obj, drop0 = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{obj}] \R{} object, typically of class \code{\LinkA{lm}{lm}} or
\code{\LinkA{glm}{glm}}.
\item[\code{drop0}] logical.  If \code{TRUE}, drop all cases with
\code{\LinkA{weights}{weights} == 0}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
Weighted residuals are based on the deviance residuals, which for
a \code{\LinkA{lm}{lm}} fit are the raw residuals \eqn{R_i}{}
multiplied by \eqn{\sqrt{w_i}}{}, where \eqn{w_i}{} are the
\code{weights} as specified in \code{\LinkA{lm}{lm}}'s call.

Dropping cases with weights zero is compatible with
\code{\LinkA{influence}{influence}} and related functions.
\end{Details}
%
\begin{Value}
Numeric vector of length \eqn{n'}{}, where \eqn{n'}{} is the number of
of non-0 weights (\code{drop0 = TRUE}) or the number of
observations, otherwise.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{residuals}{residuals}}, \code{\LinkA{lm.influence}{lm.influence}}, etc.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## following on from example(lm)

all.equal(weighted.residuals(lm.D9),
          residuals(lm.D9))
x <- 1:10
w <- 0:9
y <- rnorm(x)
weighted.residuals(lmxy <- lm(y ~ x, weights = w))
weighted.residuals(lmxy, drop0 = FALSE)
\end{ExampleCode}
\end{Examples}
\HeaderA{weights}{Extract Model Weights}{weights}
\methaliasA{weights.default}{weights}{weights.default}
\keyword{models}{weights}
%
\begin{Description}\relax
\code{weights} is a generic function which extracts fitting weights from
objects returned by modeling functions.

Methods can make use of \code{\LinkA{napredict}{napredict}} methods to compensate
for the omission of missing values.  The default methods does so.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
weights(object, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] an object for which the extraction of model weights is
meaningful.
\item[\code{...}] other arguments passed to methods.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Weights extracted from the object \code{object}: the default method
looks for component \code{"weights"} and if not \code{NULL} calls
\code{\LinkA{napredict}{napredict}} on it.
\end{Value}
%
\begin{References}\relax
Chambers, J. M. and Hastie, T. J. (1992)
\emph{Statistical Models in S}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{weights.glm}{weights.glm}}
\end{SeeAlso}
\HeaderA{wilcox.test}{Wilcoxon Rank Sum and Signed Rank Tests}{wilcox.test}
\methaliasA{wilcox.test.default}{wilcox.test}{wilcox.test.default}
\methaliasA{wilcox.test.formula}{wilcox.test}{wilcox.test.formula}
\keyword{htest}{wilcox.test}
%
\begin{Description}\relax
Performs one- and two-sample Wilcoxon tests on vectors of data; the
latter is also known as `Mann-Whitney' test.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
wilcox.test(x, ...)

## Default S3 method:
wilcox.test(x, y = NULL,
            alternative = c("two.sided", "less", "greater"),
            mu = 0, paired = FALSE, exact = NULL, correct = TRUE,
            conf.int = FALSE, conf.level = 0.95, ...)

## S3 method for class 'formula'
wilcox.test(formula, data, subset, na.action, ...)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] numeric vector of data values.  Non-finite (e.g. infinite or
missing) values will be omitted.
\item[\code{y}] an optional numeric vector of data values: as with \code{x}
non-finite values will be omitted.
\item[\code{alternative}] a character string specifying the alternative
hypothesis, must be one of \code{"two.sided"} (default),
\code{"greater"} or \code{"less"}.  You can specify just the initial
letter.
\item[\code{mu}] a number specifying an optional parameter used to form the
null hypothesis.  See `Details'.
\item[\code{paired}] a logical indicating whether you want a paired test.
\item[\code{exact}] a logical indicating whether an exact p-value
should be computed.
\item[\code{correct}] a logical indicating whether to apply continuity
correction in the normal approximation for the p-value.
\item[\code{conf.int}] a logical indicating whether a confidence interval
should be computed.
\item[\code{conf.level}] confidence level of the interval.
\item[\code{formula}] a formula of the form \code{lhs \textasciitilde{} rhs} where \code{lhs}
is a numeric variable giving the data values and \code{rhs} a factor
with two levels giving the corresponding groups.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.  Defaults to
\code{getOption("na.action")}.
\item[\code{...}] further arguments to be passed to or from methods.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The formula interface is only applicable for the 2-sample tests.

If only \code{x} is given, or if both \code{x} and \code{y} are given
and \code{paired} is \code{TRUE}, a Wilcoxon signed rank test of the
null that the distribution of \code{x} (in the one sample case) or of
\code{x - y} (in the paired two sample case) is symmetric about
\code{mu} is performed.

Otherwise, if both \code{x} and \code{y} are given and \code{paired}
is \code{FALSE}, a Wilcoxon rank sum test (equivalent to the
Mann-Whitney test: see the Note) is carried out.  In this case, the
null hypothesis is that the distributions of \code{x} and \code{y}
differ by a location shift of \code{mu} and the alternative is that
they differ by some other location shift (and the one-sided
alternative \code{"greater"} is that \code{x} is shifted to the right
of \code{y}).

By default (if \code{exact} is not specified), an exact p-value
is computed if the samples contain less than 50 finite values and
there are no ties.  Otherwise, a normal approximation is used.

Optionally (if argument \code{conf.int} is true), a nonparametric
confidence interval and an estimator for the pseudomedian (one-sample
case) or for the difference of the location parameters \code{x-y} is
computed.  (The pseudomedian of a distribution \eqn{F}{} is the median
of the distribution of \eqn{(u+v)/2}{}, where \eqn{u}{} and \eqn{v}{} are
independent, each with distribution \eqn{F}{}.  If \eqn{F}{} is symmetric,
then the pseudomedian and median coincide.  See Hollander \& Wolfe
(1973), page 34.)  Note that in the two-sample case the estimator for
the difference in location parameters does \bold{not} estimate the
difference in medians (a common misconception) but rather the median
of the difference between a sample from \code{x} and a sample from
\code{y}.

If exact p-values are available, an exact confidence interval is
obtained by the algorithm described in Bauer (1972), and the
Hodges-Lehmann estimator is employed.  Otherwise, the returned
confidence interval and point estimate are based on normal
approximations.  These are continuity-corrected for the interval but
\emph{not} the estimate (as the correction depends on the
\code{alternative}).

With small samples it may not be possible to achieve very high
confidence interval coverages. If this happens a warning will be given
and an interval with lower coverage will be substituted.
\end{Details}
%
\begin{Value}
A list with class \code{"htest"} containing the following components:
\begin{ldescription}
\item[\code{statistic}] the value of the test statistic with a name
describing it.
\item[\code{parameter}] the parameter(s) for the exact distribution of the
test statistic.
\item[\code{p.value}] the p-value for the test.
\item[\code{null.value}] the location parameter \code{mu}.
\item[\code{alternative}] a character string describing the alternative
hypothesis.
\item[\code{method}] the type of test applied.
\item[\code{data.name}] a character string giving the names of the data.
\item[\code{conf.int}] a confidence interval for the location parameter.
(Only present if argument \code{conf.int = TRUE}.)
\item[\code{estimate}] an estimate of the location parameter.
(Only present if argument \code{conf.int = TRUE}.)
\end{ldescription}
\end{Value}
%
\begin{Section}{Warning}
This function can use large amounts of memory and stack (and even
crash \R{} if the stack limit is exceeded) if \code{exact = TRUE} and
one sample is large (several thousands or more).
\end{Section}
%
\begin{Note}\relax
The literature is not unanimous about the definitions of the Wilcoxon
rank sum and Mann-Whitney tests.  The two most common definitions
correspond to the sum of the ranks of the first sample with the
minimum value subtracted or not: \R{} subtracts and S-PLUS does not,
giving a value which is larger by \eqn{m(m+1)/2}{} for a first sample
of size \eqn{m}{}.  (It seems Wilcoxon's original paper used the
unadjusted sum of the ranks but subsequent tables subtracted the
minimum.)

\R{}'s value can also be computed as the number of all pairs
\code{(x[i], y[j])} for which \code{y[j]} is not greater than
\code{x[i]}, the most common definition of the Mann-Whitney test.
\end{Note}
%
\begin{References}\relax
David F. Bauer (1972),
Constructing confidence sets using rank statistics.
\emph{Journal of the American Statistical Association}
\bold{67}, 687--690.

Myles Hollander and Douglas A. Wolfe (1973),
\emph{Nonparametric Statistical Methods.}
New York: John Wiley \& Sons.
Pages 27--33 (one-sample), 68--75 (two-sample).\\{}
Or second edition (1999).
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{psignrank}{psignrank}}, \code{\LinkA{pwilcox}{pwilcox}}.

\code{\LinkA{wilcox\_test}{wilcox.Rul.test}} in package
\Rhref{http://CRAN.R-project.org/package=coin}{\pkg{coin}} for exact, asymptotic and Monte Carlo
\emph{conditional} p-values, including in the presence of ties.

\code{\LinkA{kruskal.test}{kruskal.test}} for testing homogeneity in location
parameters in the case of two or more samples;
\code{\LinkA{t.test}{t.test}} for an alternative under normality
assumptions [or large samples]
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)
## One-sample test.
## Hollander & Wolfe (1973), 29f.
## Hamilton depression scale factor measurements in 9 patients with
##  mixed anxiety and depression, taken at the first (x) and second
##  (y) visit after initiation of a therapy (administration of a
##  tranquilizer).
x <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)
y <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)
wilcox.test(x, y, paired = TRUE, alternative = "greater")
wilcox.test(y - x, alternative = "less")    # The same.
wilcox.test(y - x, alternative = "less",
            exact = FALSE, correct = FALSE) # H&W large sample
                                            # approximation

## Two-sample test.
## Hollander & Wolfe (1973), 69f.
## Permeability constants of the human chorioamnion (a placental
##  membrane) at term (x) and between 12 to 26 weeks gestational
##  age (y).  The alternative of interest is greater permeability
##  of the human chorioamnion for the term pregnancy.
x <- c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46)
y <- c(1.15, 0.88, 0.90, 0.74, 1.21)
wilcox.test(x, y, alternative = "g")        # greater
wilcox.test(x, y, alternative = "greater",
            exact = FALSE, correct = FALSE) # H&W large sample
                                            # approximation

wilcox.test(rnorm(10), rnorm(10, 2), conf.int = TRUE)

## Formula interface.
boxplot(Ozone ~ Month, data = airquality)
wilcox.test(Ozone ~ Month, data = airquality,
            subset = Month %in% c(5, 8))
\end{ExampleCode}
\end{Examples}
\HeaderA{Wilcoxon}{Distribution of the Wilcoxon Rank Sum Statistic}{Wilcoxon}
\aliasA{dwilcox}{Wilcoxon}{dwilcox}
\aliasA{pwilcox}{Wilcoxon}{pwilcox}
\aliasA{qwilcox}{Wilcoxon}{qwilcox}
\aliasA{rwilcox}{Wilcoxon}{rwilcox}
\keyword{distribution}{Wilcoxon}
%
\begin{Description}\relax
Density, distribution function, quantile function and random
generation for the distribution of the Wilcoxon rank sum statistic
obtained from samples with size \code{m} and \code{n}, respectively.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
dwilcox(x, m, n, log = FALSE)
pwilcox(q, m, n, lower.tail = TRUE, log.p = FALSE)
qwilcox(p, m, n, lower.tail = TRUE, log.p = FALSE)
rwilcox(nn, m, n)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x, q}] vector of quantiles.
\item[\code{p}] vector of probabilities.
\item[\code{nn}] number of observations. If \code{length(nn) > 1}, the length
is taken to be the number required.
\item[\code{m, n}] numbers of observations in the first and second sample,
respectively.  Can be vectors of positive integers.
\item[\code{log, log.p}] logical; if TRUE, probabilities p are given as log(p).
\item[\code{lower.tail}] logical; if TRUE (default), probabilities are
\eqn{P[X \le x]}{}, otherwise, \eqn{P[X > x]}{}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
This distribution is obtained as follows.  Let \code{x} and \code{y}
be two random, independent samples of size \code{m} and \code{n}.
Then the Wilcoxon rank sum statistic is the number of all pairs
\code{(x[i], y[j])} for which \code{y[j]} is not greater than
\code{x[i]}.  This statistic takes values between \code{0} and
\code{m * n}, and its mean and variance are \code{m * n / 2} and
\code{m * n * (m + n + 1) / 12}, respectively.

If any of the first three arguments are vectors, the recycling rule is
used to do the calculations for all combinations of the three up to
the length of the longest vector.
\end{Details}
%
\begin{Value}
\code{dwilcox} gives the density,
\code{pwilcox} gives the distribution function,
\code{qwilcox} gives the quantile function, and
\code{rwilcox} generates random deviates.
\end{Value}
%
\begin{Section}{Warning}
These functions can use large amounts of memory and stack (and even crash
\R{} if the stack limit is exceeded and stack-checking is not in place)
if one sample is large (several thousands or more).
\end{Section}
%
\begin{Note}\relax
S-PLUS uses a different (but equivalent) definition of the Wilcoxon
statistic: see \code{\LinkA{wilcox.test}{wilcox.test}} for details.
\end{Note}
%
\begin{Author}\relax
Kurt Hornik
\end{Author}
%
\begin{Source}\relax
These are calculated via recursion, based on \code{cwilcox(k, m, n)},
the number of choices with statistic \code{k} from samples of size
\code{m} and \code{n}, which is itself calculated recursively and the
results cached.  Then \code{dwilcox} and \code{pwilcox} sum
appropriate values of \code{cwilcox}, and \code{qwilcox} is based on
inversion.

\code{rwilcox} generates a random permutation of ranks and evaluates
the statistic.
\end{Source}
%
\begin{SeeAlso}\relax
\code{\LinkA{wilcox.test}{wilcox.test}} to calculate the statistic from data, find p
values and so on.

\LinkA{Distributions}{Distributions} for standard distributions, including
\code{\LinkA{dsignrank}{dsignrank}} for the distribution of the
\emph{one-sample} Wilcoxon signed rank statistic.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
require(graphics)

x <- -1:(4*6 + 1)
fx <- dwilcox(x, 4, 6)
Fx <- pwilcox(x, 4, 6)

layout(rbind(1,2), widths=1, heights=c(3,2))
plot(x, fx,type='h', col="violet",
     main= "Probabilities (density) of Wilcoxon-Statist.(n=6,m=4)")
plot(x, Fx,type="s", col="blue",
     main= "Distribution of Wilcoxon-Statist.(n=6,m=4)")
abline(h=0:1, col="gray20",lty=2)
layout(1)# set back

N <- 200
hist(U <- rwilcox(N, m=4,n=6), breaks=0:25 - 1/2,
     border="red", col="pink", sub = paste("N =",N))
mtext("N * f(x),  f() = true \"density\"", side=3, col="blue")
 lines(x, N*fx, type='h', col='blue', lwd=2)
points(x, N*fx, cex=2)

## Better is a Quantile-Quantile Plot
qqplot(U, qw <- qwilcox((1:N - 1/2)/N, m=4,n=6),
       main = paste("Q-Q-Plot of empirical and theoretical quantiles",
                     "Wilcoxon Statistic,  (m=4, n=6)",sep="\n"))
n <- as.numeric(names(print(tU <- table(U))))
text(n+.2, n+.5, labels=tU, col="red")
\end{ExampleCode}
\end{Examples}
\HeaderA{window}{Time Windows}{window}
\methaliasA{window.default}{window}{window.default}
\methaliasA{window.ts}{window}{window.ts}
\aliasA{window<\Rdash}{window}{window<.Rdash.}
\methaliasA{window<\Rdash.ts}{window}{window<.Rdash..ts}
\keyword{ts}{window}
%
\begin{Description}\relax
\code{window} is a generic function which
extracts the subset of the object \code{x}
observed between the times \code{start} and \code{end}. If a
frequency is specified, the series is then re-sampled at the new
frequency.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
window(x, ...)
## S3 method for class 'ts'
window(x, ...)
## Default S3 method:
window(x, start = NULL, end = NULL,
      frequency = NULL, deltat = NULL, extend = FALSE, ...)

window(x, ...) <- value
## S3 replacement method for class 'ts'
window(x, start, end, frequency, deltat, ...) <- value
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] a time-series (or other object if not replacing values).
\item[\code{start}] the start time of the period of interest.
\item[\code{end}] the end time of the period of interest.
\item[\code{frequency, deltat}] the new frequency can be specified by either
(or both if they are consistent).
\item[\code{extend}] logical.  If true, the \code{start} and \code{end} values
are allowed to extend the series.  If false, attempts to extend the
series give a warning and are ignored.
\item[\code{...}] further arguments passed to or from other methods.
\item[\code{value}] replacement values.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
The start and end times can be specified as for \code{\LinkA{ts}{ts}}. If
there is no observation at the new \code{start} or \code{end},
the immediately following (\code{start}) or preceding (\code{end})
observation time is used.

The replacement function has a method for \code{ts} objects, and
is allowed to extend the series (with a warning).  There is no default
method.
\end{Details}
%
\begin{Value}
The value depends on the method. \code{window.default} will return a
vector or matrix with an appropriate \code{\LinkA{tsp}{tsp}} attribute.

\code{window.ts} differs from \code{window.default} only in
ensuring the result is a \code{ts} object.

If \code{extend = TRUE} the series will be padded with \code{NA}s if
needed.
\end{Value}
%
\begin{References}\relax
Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
\emph{The New S Language}.
Wadsworth \& Brooks/Cole.
\end{References}
%
\begin{SeeAlso}\relax
\code{\LinkA{time}{time}}, \code{\LinkA{ts}{ts}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
window(presidents, 1960, c(1969,4)) # values in the 1960's
window(presidents, deltat=1)  # All Qtr1s
window(presidents, start=c(1945,3), deltat=1)  # All Qtr3s
window(presidents, 1944, c(1979,2), extend=TRUE)

pres <- window(presidents, 1945, c(1949,4)) # values in the 1940's
window(pres, 1945.25, 1945.50) <- c(60, 70)
window(pres, 1944, 1944.75) <- 0 # will generate a warning
window(pres, c(1945,4), c(1949,4), frequency=1) <- 85:89
pres
\end{ExampleCode}
\end{Examples}
\HeaderA{xtabs}{Cross Tabulation}{xtabs}
\aliasA{print.xtabs}{xtabs}{print.xtabs}
\keyword{category}{xtabs}
%
\begin{Description}\relax
Create a contingency table (optionally a sparse matrix) from
cross-classifying factors, usually contained in a data frame,
using a formula interface.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
xtabs(formula = ~., data = parent.frame(), subset, sparse = FALSE,
      na.action, exclude = c(NA, NaN), drop.unused.levels = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{formula}] a \LinkA{formula}{formula} object with the cross-classifying variables
(separated by \code{+}) on the right hand side (or an object which
can be coerced to a formula).  Interactions are not allowed.  On the
left hand side, one may optionally give a vector or a matrix of
counts; in the latter case, the columns are interpreted as
corresponding to the levels of a variable.  This is useful if the
data have already been tabulated, see the examples below.
\item[\code{data}] an optional matrix or data frame (or similar: see
\code{\LinkA{model.frame}{model.frame}}) containing the variables in the
formula \code{formula}.  By default the variables are taken from
\code{environment(formula)}.
\item[\code{subset}] an optional vector specifying a subset of observations
to be used.
\item[\code{sparse}] logical specifying if the result should be a
\emph{sparse} matrix, i.e., inheriting from
\code{\LinkA{sparseMatrix}{sparseMatrix}}
Only works for two factors (since there
are no higher-order sparse array classes yet).

\item[\code{na.action}] a function which indicates what should happen when
the data contain \code{NA}s.
\item[\code{exclude}] a vector of values to be excluded when forming the
set of levels of the classifying factors.
\item[\code{drop.unused.levels}] a logical indicating whether to drop unused
levels in the classifying factors.  If this is \code{FALSE} and
there are unused levels, the table will contain zero marginals, and
a subsequent chi-squared test for independence of the factors will
not work.
\end{ldescription}
\end{Arguments}
%
\begin{Details}\relax
There is a \code{summary} method for contingency table objects created
by \code{table} or \code{xtabs(*, sparse=FALSE)}, which gives basic
information and performs a chi-squared test for independence of
factors (note that the function \code{\LinkA{chisq.test}{chisq.test}} currently
only handles 2-d tables).

If a left hand side is given in \code{formula}, its entries are simply
summed over the cells corresponding to the right hand side; this also
works if the lhs does not give counts.
\end{Details}
%
\begin{Value}
By default, when \code{sparse=FALSE},
a contingency table in array representation of S3 class \code{c("xtabs",
    "table")}, with a \code{"call"} attribute storing the matched call.

When \code{sparse=TRUE}, a sparse numeric matrix, specifically an
object of S4 class 
\code{\LinkA{dgTMatrix}{dgTMatrix}} from package
\Rhref{http://CRAN.R-project.org/package=Matrix}{\pkg{Matrix}}.
\end{Value}
%
\begin{SeeAlso}\relax
\code{\LinkA{table}{table}} for traditional cross-tabulation, and
\code{\LinkA{as.data.frame.table}{as.data.frame.table}} which is the inverse operation of
\code{xtabs} (see the \code{DF} example below).

\code{\LinkA{sparseMatrix}{sparseMatrix}} on sparse
matrices in package \Rhref{http://CRAN.R-project.org/package=Matrix}{\pkg{Matrix}}.
\end{SeeAlso}
%
\begin{Examples}
\begin{ExampleCode}
## 'esoph' has the frequencies of cases and controls for all levels of
## the variables 'agegp', 'alcgp', and 'tobgp'.
xtabs(cbind(ncases, ncontrols) ~ ., data = esoph)
## Output is not really helpful ... flat tables are better:
ftable(xtabs(cbind(ncases, ncontrols) ~ ., data = esoph))
## In particular if we have fewer factors ...
ftable(xtabs(cbind(ncases, ncontrols) ~ agegp, data = esoph))

## This is already a contingency table in array form.
DF <- as.data.frame(UCBAdmissions)
## Now 'DF' is a data frame with a grid of the factors and the counts
## in variable 'Freq'.
DF
## Nice for taking margins ...
xtabs(Freq ~ Gender + Admit, DF)
## And for testing independence ...
summary(xtabs(Freq ~ ., DF))

## Create a nice display for the warp break data.
warpbreaks$replicate <- rep(1:9, len = 54)
ftable(xtabs(breaks ~ wool + tension + replicate, data = warpbreaks))

### ---- Sparse Examples ----

if(require("Matrix")) {
 ## similar to "nlme"s  'ergoStool' :
 d.ergo <- data.frame(Type = paste0("T", rep(1:4, 9*4)),
                      Subj = gl(9,4, 36*4))
 print(xtabs(~ Type + Subj, data=d.ergo)) # 4 replicates each
 set.seed(15) # a subset of cases:
 print(xtabs(~ Type + Subj, data=d.ergo[sample(36, 10),], sparse=TRUE))

 ## Hypothetical two level setup:
 inner <- factor(sample(letters[1:25], 100, replace = TRUE))
 inout <- factor(sample(LETTERS[1:5], 25, replace = TRUE))
 fr <- data.frame(inner = inner, outer = inout[as.integer(inner)])
 print(xtabs(~ inner + outer, fr, sparse = TRUE))
}
\end{ExampleCode}
\end{Examples}
\clearpage
